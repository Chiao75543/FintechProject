{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "定義資料集\n",
    "\"\"\"\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, data, lookback):\n",
    "        self.data = data\n",
    "        self.lookback = lookback\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.Tensor(self.data[index:index+self.lookback, 1:-1])\n",
    "        y = torch.Tensor([self.data[index+self.lookback-1, -1]])\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] - self.lookback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>開盤價</th>\n",
       "      <th>收盤價</th>\n",
       "      <th>最高價</th>\n",
       "      <th>最低價</th>\n",
       "      <th>成交張數</th>\n",
       "      <th>成交筆數</th>\n",
       "      <th>成交金額</th>\n",
       "      <th>成交均張</th>\n",
       "      <th>漲跌幅</th>\n",
       "      <th>買進_外陸資買進股數(不含外資自營商)</th>\n",
       "      <th>...</th>\n",
       "      <th>20,001-30,000(股)</th>\n",
       "      <th>30,001-40,000(股)</th>\n",
       "      <th>40,001-50,000(股)</th>\n",
       "      <th>50,001-100,000(股)</th>\n",
       "      <th>100,001-200,000(股)</th>\n",
       "      <th>200,001-400,000(股)</th>\n",
       "      <th>400,001-600,000(股)</th>\n",
       "      <th>600,001-800,000(股)</th>\n",
       "      <th>800,001-1,000,000(股)</th>\n",
       "      <th>1,000,001(股)以上</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>240.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>23721.0</td>\n",
       "      <td>8659.0</td>\n",
       "      <td>5.681935e+09</td>\n",
       "      <td>2.74</td>\n",
       "      <td>0.5</td>\n",
       "      <td>19104797.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.69</td>\n",
       "      <td>91.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>242.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>242.5</td>\n",
       "      <td>240.5</td>\n",
       "      <td>21846.0</td>\n",
       "      <td>10251.0</td>\n",
       "      <td>5.281823e+09</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17528008.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.69</td>\n",
       "      <td>91.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>242.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>239.5</td>\n",
       "      <td>19043.0</td>\n",
       "      <td>7124.0</td>\n",
       "      <td>4.588314e+09</td>\n",
       "      <td>2.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13841045.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.69</td>\n",
       "      <td>91.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>241.5</td>\n",
       "      <td>236.5</td>\n",
       "      <td>242.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>25716.0</td>\n",
       "      <td>10534.0</td>\n",
       "      <td>6.118683e+09</td>\n",
       "      <td>2.44</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>13967976.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.69</td>\n",
       "      <td>91.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-11</th>\n",
       "      <td>235.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>232.5</td>\n",
       "      <td>32070.0</td>\n",
       "      <td>9199.0</td>\n",
       "      <td>7.500674e+09</td>\n",
       "      <td>3.49</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>19103515.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.69</td>\n",
       "      <td>91.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-16</th>\n",
       "      <td>506.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>508.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>38781.0</td>\n",
       "      <td>48051.0</td>\n",
       "      <td>1.957397e+10</td>\n",
       "      <td>0.81</td>\n",
       "      <td>5.0</td>\n",
       "      <td>31958707.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.77</td>\n",
       "      <td>85.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-17</th>\n",
       "      <td>504.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>499.5</td>\n",
       "      <td>29443.0</td>\n",
       "      <td>27930.0</td>\n",
       "      <td>1.478466e+10</td>\n",
       "      <td>1.05</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>21110197.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.77</td>\n",
       "      <td>85.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-19</th>\n",
       "      <td>504.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>499.5</td>\n",
       "      <td>29443.0</td>\n",
       "      <td>27930.0</td>\n",
       "      <td>1.478466e+10</td>\n",
       "      <td>1.05</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>21110197.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.74</td>\n",
       "      <td>86.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-30</th>\n",
       "      <td>542.0</td>\n",
       "      <td>543.0</td>\n",
       "      <td>543.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>148413.0</td>\n",
       "      <td>153125.0</td>\n",
       "      <td>8.005716e+10</td>\n",
       "      <td>0.97</td>\n",
       "      <td>40.0</td>\n",
       "      <td>133236588.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.74</td>\n",
       "      <td>86.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-31</th>\n",
       "      <td>537.0</td>\n",
       "      <td>522.0</td>\n",
       "      <td>538.0</td>\n",
       "      <td>521.0</td>\n",
       "      <td>79212.0</td>\n",
       "      <td>74279.0</td>\n",
       "      <td>4.173694e+10</td>\n",
       "      <td>1.07</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>49636797.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.74</td>\n",
       "      <td>86.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1250 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              開盤價    收盤價    最高價    最低價      成交張數      成交筆數          成交金額   \n",
       "date                                                                       \n",
       "2018-01-05  240.0  240.0  240.0  238.0   23721.0    8659.0  5.681935e+09  \\\n",
       "2018-01-08  242.0  242.0  242.5  240.5   21846.0   10251.0  5.281823e+09   \n",
       "2018-01-09  242.0  242.0  242.0  239.5   19043.0    7124.0  4.588314e+09   \n",
       "2018-01-10  241.5  236.5  242.0  236.0   25716.0   10534.0  6.118683e+09   \n",
       "2018-01-11  235.0  235.0  236.0  232.5   32070.0    9199.0  7.500674e+09   \n",
       "...           ...    ...    ...    ...       ...       ...           ...   \n",
       "2023-01-16  506.0  505.0  508.0  503.0   38781.0   48051.0  1.957397e+10   \n",
       "2023-01-17  504.0  503.0  504.0  499.5   29443.0   27930.0  1.478466e+10   \n",
       "2023-01-19  504.0  503.0  504.0  499.5   29443.0   27930.0  1.478466e+10   \n",
       "2023-01-30  542.0  543.0  543.0  534.0  148413.0  153125.0  8.005716e+10   \n",
       "2023-01-31  537.0  522.0  538.0  521.0   79212.0   74279.0  4.173694e+10   \n",
       "\n",
       "            成交均張   漲跌幅  買進_外陸資買進股數(不含外資自營商)  ...  20,001-30,000(股)   \n",
       "date                                         ...                     \n",
       "2018-01-05  2.74   0.5           19104797.0  ...              0.40  \\\n",
       "2018-01-08  2.13   2.0           17528008.0  ...              0.40   \n",
       "2018-01-09  2.67   0.0           13841045.0  ...              0.40   \n",
       "2018-01-10  2.44  -5.5           13967976.0  ...              0.40   \n",
       "2018-01-11  3.49  -1.5           19103515.0  ...              0.40   \n",
       "...          ...   ...                  ...  ...               ...   \n",
       "2023-01-16  0.81   5.0           31958707.0  ...              0.70   \n",
       "2023-01-17  1.05  -2.0           21110197.0  ...              0.70   \n",
       "2023-01-19  1.05  -2.0           21110197.0  ...              0.68   \n",
       "2023-01-30  0.97  40.0          133236588.0  ...              0.68   \n",
       "2023-01-31  1.07 -21.0           49636797.0  ...              0.68   \n",
       "\n",
       "            30,001-40,000(股)  40,001-50,000(股)  50,001-100,000(股)   \n",
       "date                                                                \n",
       "2018-01-05              0.28              0.23               0.70  \\\n",
       "2018-01-08              0.28              0.23               0.70   \n",
       "2018-01-09              0.28              0.23               0.70   \n",
       "2018-01-10              0.28              0.23               0.70   \n",
       "2018-01-11              0.28              0.23               0.70   \n",
       "...                      ...               ...                ...   \n",
       "2023-01-16              0.46              0.34               1.03   \n",
       "2023-01-17              0.46              0.34               1.03   \n",
       "2023-01-19              0.45              0.34               1.02   \n",
       "2023-01-30              0.45              0.34               1.02   \n",
       "2023-01-31              0.45              0.34               1.02   \n",
       "\n",
       "            100,001-200,000(股)  200,001-400,000(股)  400,001-600,000(股)   \n",
       "date                                                                     \n",
       "2018-01-05                0.80                1.07                0.77  \\\n",
       "2018-01-08                0.80                1.07                0.77   \n",
       "2018-01-09                0.80                1.07                0.77   \n",
       "2018-01-10                0.80                1.07                0.77   \n",
       "2018-01-11                0.80                1.07                0.77   \n",
       "...                        ...                 ...                 ...   \n",
       "2023-01-16                1.05                1.25                0.87   \n",
       "2023-01-17                1.05                1.25                0.87   \n",
       "2023-01-19                1.04                1.25                0.90   \n",
       "2023-01-30                1.04                1.25                0.90   \n",
       "2023-01-31                1.04                1.25                0.90   \n",
       "\n",
       "            600,001-800,000(股)  800,001-1,000,000(股)  1,000,001(股)以上  \n",
       "date                                                                  \n",
       "2018-01-05                0.66                  0.69           91.92  \n",
       "2018-01-08                0.66                  0.69           91.92  \n",
       "2018-01-09                0.66                  0.69           91.92  \n",
       "2018-01-10                0.66                  0.69           91.92  \n",
       "2018-01-11                0.66                  0.69           91.92  \n",
       "...                        ...                   ...             ...  \n",
       "2023-01-16                0.82                  0.77           85.78  \n",
       "2023-01-17                0.82                  0.77           85.78  \n",
       "2023-01-19                0.80                  0.74           86.05  \n",
       "2023-01-30                0.80                  0.74           86.05  \n",
       "2023-01-31                0.80                  0.74           86.05  \n",
       "\n",
       "[1250 rows x 73 columns]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_id = \"2330\"\n",
    "total_features_name = 'data/{}_Total_features.csv'.format(stock_id)\n",
    "\n",
    "#處理重複欄位跟缺失值 以及過多缺失值的欄位\n",
    "df = pd.read_csv(total_features_name, index_col='date')\n",
    "df = df.drop(columns=['股價','借券賣出_當日調整'])\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>開盤價</th>\n",
       "      <th>收盤價</th>\n",
       "      <th>最高價</th>\n",
       "      <th>最低價</th>\n",
       "      <th>成交張數</th>\n",
       "      <th>成交筆數</th>\n",
       "      <th>成交金額</th>\n",
       "      <th>成交均張</th>\n",
       "      <th>漲跌幅</th>\n",
       "      <th>買進_外陸資買進股數(不含外資自營商)</th>\n",
       "      <th>...</th>\n",
       "      <th>20,001-30,000(股)</th>\n",
       "      <th>30,001-40,000(股)</th>\n",
       "      <th>40,001-50,000(股)</th>\n",
       "      <th>50,001-100,000(股)</th>\n",
       "      <th>100,001-200,000(股)</th>\n",
       "      <th>200,001-400,000(股)</th>\n",
       "      <th>400,001-600,000(股)</th>\n",
       "      <th>600,001-800,000(股)</th>\n",
       "      <th>800,001-1,000,000(股)</th>\n",
       "      <th>1,000,001(股)以上</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>0.061181</td>\n",
       "      <td>0.067368</td>\n",
       "      <td>0.059811</td>\n",
       "      <td>0.066808</td>\n",
       "      <td>0.119356</td>\n",
       "      <td>0.021746</td>\n",
       "      <td>0.054955</td>\n",
       "      <td>0.254963</td>\n",
       "      <td>0.483660</td>\n",
       "      <td>0.142837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.978626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>0.065401</td>\n",
       "      <td>0.071579</td>\n",
       "      <td>0.065058</td>\n",
       "      <td>0.072110</td>\n",
       "      <td>0.108232</td>\n",
       "      <td>0.027130</td>\n",
       "      <td>0.050363</td>\n",
       "      <td>0.191223</td>\n",
       "      <td>0.503268</td>\n",
       "      <td>0.130995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.978626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>0.065401</td>\n",
       "      <td>0.071579</td>\n",
       "      <td>0.064008</td>\n",
       "      <td>0.069989</td>\n",
       "      <td>0.091603</td>\n",
       "      <td>0.016555</td>\n",
       "      <td>0.042404</td>\n",
       "      <td>0.247649</td>\n",
       "      <td>0.477124</td>\n",
       "      <td>0.103304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.978626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>0.064346</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.064008</td>\n",
       "      <td>0.062566</td>\n",
       "      <td>0.131192</td>\n",
       "      <td>0.028087</td>\n",
       "      <td>0.059968</td>\n",
       "      <td>0.223615</td>\n",
       "      <td>0.405229</td>\n",
       "      <td>0.104258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.978626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-11</th>\n",
       "      <td>0.050633</td>\n",
       "      <td>0.056842</td>\n",
       "      <td>0.051417</td>\n",
       "      <td>0.055143</td>\n",
       "      <td>0.168889</td>\n",
       "      <td>0.023572</td>\n",
       "      <td>0.075829</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.457516</td>\n",
       "      <td>0.142827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.978626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-16</th>\n",
       "      <td>0.622363</td>\n",
       "      <td>0.625263</td>\n",
       "      <td>0.622246</td>\n",
       "      <td>0.628844</td>\n",
       "      <td>0.208705</td>\n",
       "      <td>0.154967</td>\n",
       "      <td>0.214396</td>\n",
       "      <td>0.053292</td>\n",
       "      <td>0.542484</td>\n",
       "      <td>0.239373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.041221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-17</th>\n",
       "      <td>0.618143</td>\n",
       "      <td>0.621053</td>\n",
       "      <td>0.613851</td>\n",
       "      <td>0.621421</td>\n",
       "      <td>0.153304</td>\n",
       "      <td>0.086919</td>\n",
       "      <td>0.159428</td>\n",
       "      <td>0.078370</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.157898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.041221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-19</th>\n",
       "      <td>0.618143</td>\n",
       "      <td>0.621053</td>\n",
       "      <td>0.613851</td>\n",
       "      <td>0.621421</td>\n",
       "      <td>0.153304</td>\n",
       "      <td>0.086919</td>\n",
       "      <td>0.159428</td>\n",
       "      <td>0.078370</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.157898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.082443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-30</th>\n",
       "      <td>0.698312</td>\n",
       "      <td>0.705263</td>\n",
       "      <td>0.695698</td>\n",
       "      <td>0.694592</td>\n",
       "      <td>0.859131</td>\n",
       "      <td>0.510322</td>\n",
       "      <td>0.908567</td>\n",
       "      <td>0.070010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.082443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-31</th>\n",
       "      <td>0.687764</td>\n",
       "      <td>0.661053</td>\n",
       "      <td>0.685205</td>\n",
       "      <td>0.667020</td>\n",
       "      <td>0.448574</td>\n",
       "      <td>0.243669</td>\n",
       "      <td>0.468762</td>\n",
       "      <td>0.080460</td>\n",
       "      <td>0.202614</td>\n",
       "      <td>0.372141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.082443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1250 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 開盤價       收盤價       最高價       最低價      成交張數      成交筆數   \n",
       "date                                                                     \n",
       "2018-01-05  0.061181  0.067368  0.059811  0.066808  0.119356  0.021746  \\\n",
       "2018-01-08  0.065401  0.071579  0.065058  0.072110  0.108232  0.027130   \n",
       "2018-01-09  0.065401  0.071579  0.064008  0.069989  0.091603  0.016555   \n",
       "2018-01-10  0.064346  0.060000  0.064008  0.062566  0.131192  0.028087   \n",
       "2018-01-11  0.050633  0.056842  0.051417  0.055143  0.168889  0.023572   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2023-01-16  0.622363  0.625263  0.622246  0.628844  0.208705  0.154967   \n",
       "2023-01-17  0.618143  0.621053  0.613851  0.621421  0.153304  0.086919   \n",
       "2023-01-19  0.618143  0.621053  0.613851  0.621421  0.153304  0.086919   \n",
       "2023-01-30  0.698312  0.705263  0.695698  0.694592  0.859131  0.510322   \n",
       "2023-01-31  0.687764  0.661053  0.685205  0.667020  0.448574  0.243669   \n",
       "\n",
       "                成交金額      成交均張       漲跌幅  買進_外陸資買進股數(不含外資自營商)  ...   \n",
       "date                                                           ...   \n",
       "2018-01-05  0.054955  0.254963  0.483660             0.142837  ...  \\\n",
       "2018-01-08  0.050363  0.191223  0.503268             0.130995  ...   \n",
       "2018-01-09  0.042404  0.247649  0.477124             0.103304  ...   \n",
       "2018-01-10  0.059968  0.223615  0.405229             0.104258  ...   \n",
       "2018-01-11  0.075829  0.333333  0.457516             0.142827  ...   \n",
       "...              ...       ...       ...                  ...  ...   \n",
       "2023-01-16  0.214396  0.053292  0.542484             0.239373  ...   \n",
       "2023-01-17  0.159428  0.078370  0.450980             0.157898  ...   \n",
       "2023-01-19  0.159428  0.078370  0.450980             0.157898  ...   \n",
       "2023-01-30  0.908567  0.070010  1.000000             1.000000  ...   \n",
       "2023-01-31  0.468762  0.080460  0.202614             0.372141  ...   \n",
       "\n",
       "            20,001-30,000(股)  30,001-40,000(股)  40,001-50,000(股)   \n",
       "date                                                               \n",
       "2018-01-05          0.030303              0.05          0.076923  \\\n",
       "2018-01-08          0.030303              0.05          0.076923   \n",
       "2018-01-09          0.030303              0.05          0.076923   \n",
       "2018-01-10          0.030303              0.05          0.076923   \n",
       "2018-01-11          0.030303              0.05          0.076923   \n",
       "...                      ...               ...               ...   \n",
       "2023-01-16          0.939394              0.95          0.923077   \n",
       "2023-01-17          0.939394              0.95          0.923077   \n",
       "2023-01-19          0.878788              0.90          0.923077   \n",
       "2023-01-30          0.878788              0.90          0.923077   \n",
       "2023-01-31          0.878788              0.90          0.923077   \n",
       "\n",
       "            50,001-100,000(股)  100,001-200,000(股)  200,001-400,000(股)   \n",
       "date                                                                    \n",
       "2018-01-05           0.028571                0.00            0.107143  \\\n",
       "2018-01-08           0.028571                0.00            0.107143   \n",
       "2018-01-09           0.028571                0.00            0.107143   \n",
       "2018-01-10           0.028571                0.00            0.107143   \n",
       "2018-01-11           0.028571                0.00            0.107143   \n",
       "...                       ...                 ...                 ...   \n",
       "2023-01-16           0.971429                1.00            0.750000   \n",
       "2023-01-17           0.971429                1.00            0.750000   \n",
       "2023-01-19           0.942857                0.96            0.750000   \n",
       "2023-01-30           0.942857                0.96            0.750000   \n",
       "2023-01-31           0.942857                0.96            0.750000   \n",
       "\n",
       "            400,001-600,000(股)  600,001-800,000(股)  800,001-1,000,000(股)   \n",
       "date                                                                       \n",
       "2018-01-05                0.16                0.05              0.571429  \\\n",
       "2018-01-08                0.16                0.05              0.571429   \n",
       "2018-01-09                0.16                0.05              0.571429   \n",
       "2018-01-10                0.16                0.05              0.571429   \n",
       "2018-01-11                0.16                0.05              0.571429   \n",
       "...                        ...                 ...                   ...   \n",
       "2023-01-16                0.56                0.85              0.952381   \n",
       "2023-01-17                0.56                0.85              0.952381   \n",
       "2023-01-19                0.68                0.75              0.809524   \n",
       "2023-01-30                0.68                0.75              0.809524   \n",
       "2023-01-31                0.68                0.75              0.809524   \n",
       "\n",
       "            1,000,001(股)以上  \n",
       "date                        \n",
       "2018-01-05        0.978626  \n",
       "2018-01-08        0.978626  \n",
       "2018-01-09        0.978626  \n",
       "2018-01-10        0.978626  \n",
       "2018-01-11        0.978626  \n",
       "...                    ...  \n",
       "2023-01-16        0.041221  \n",
       "2023-01-17        0.041221  \n",
       "2023-01-19        0.082443  \n",
       "2023-01-30        0.082443  \n",
       "2023-01-31        0.082443  \n",
       "\n",
       "[1250 rows x 73 columns]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df[df.columns] = scaler.fit_transform(df[df.columns])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>開盤價</th>\n",
       "      <th>收盤價</th>\n",
       "      <th>最高價</th>\n",
       "      <th>最低價</th>\n",
       "      <th>成交張數</th>\n",
       "      <th>成交筆數</th>\n",
       "      <th>成交金額</th>\n",
       "      <th>成交均張</th>\n",
       "      <th>漲跌幅</th>\n",
       "      <th>買進_外陸資買進股數(不含外資自營商)</th>\n",
       "      <th>...</th>\n",
       "      <th>30,001-40,000(股)</th>\n",
       "      <th>40,001-50,000(股)</th>\n",
       "      <th>50,001-100,000(股)</th>\n",
       "      <th>100,001-200,000(股)</th>\n",
       "      <th>200,001-400,000(股)</th>\n",
       "      <th>400,001-600,000(股)</th>\n",
       "      <th>600,001-800,000(股)</th>\n",
       "      <th>800,001-1,000,000(股)</th>\n",
       "      <th>1,000,001(股)以上</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-25</th>\n",
       "      <td>0.099156</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.110178</td>\n",
       "      <td>0.106045</td>\n",
       "      <td>0.252803</td>\n",
       "      <td>0.045984</td>\n",
       "      <td>0.127251</td>\n",
       "      <td>0.273772</td>\n",
       "      <td>0.477124</td>\n",
       "      <td>0.249479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-26</th>\n",
       "      <td>0.095992</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.096537</td>\n",
       "      <td>0.099682</td>\n",
       "      <td>0.236785</td>\n",
       "      <td>0.035822</td>\n",
       "      <td>0.117162</td>\n",
       "      <td>0.322884</td>\n",
       "      <td>0.437908</td>\n",
       "      <td>0.196885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-29</th>\n",
       "      <td>0.101266</td>\n",
       "      <td>0.106316</td>\n",
       "      <td>0.104932</td>\n",
       "      <td>0.102863</td>\n",
       "      <td>0.164357</td>\n",
       "      <td>0.033759</td>\n",
       "      <td>0.082338</td>\n",
       "      <td>0.236155</td>\n",
       "      <td>0.522876</td>\n",
       "      <td>0.129802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-30</th>\n",
       "      <td>0.094937</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.096537</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.200571</td>\n",
       "      <td>0.036383</td>\n",
       "      <td>0.099051</td>\n",
       "      <td>0.269592</td>\n",
       "      <td>0.405229</td>\n",
       "      <td>0.151488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>0.088608</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.096537</td>\n",
       "      <td>0.096501</td>\n",
       "      <td>0.250389</td>\n",
       "      <td>0.038297</td>\n",
       "      <td>0.123688</td>\n",
       "      <td>0.321839</td>\n",
       "      <td>0.503268</td>\n",
       "      <td>0.254194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-16</th>\n",
       "      <td>0.622363</td>\n",
       "      <td>0.625263</td>\n",
       "      <td>0.622246</td>\n",
       "      <td>0.628844</td>\n",
       "      <td>0.208705</td>\n",
       "      <td>0.154967</td>\n",
       "      <td>0.214396</td>\n",
       "      <td>0.053292</td>\n",
       "      <td>0.542484</td>\n",
       "      <td>0.239373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.041221</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-17</th>\n",
       "      <td>0.618143</td>\n",
       "      <td>0.621053</td>\n",
       "      <td>0.613851</td>\n",
       "      <td>0.621421</td>\n",
       "      <td>0.153304</td>\n",
       "      <td>0.086919</td>\n",
       "      <td>0.159428</td>\n",
       "      <td>0.078370</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.157898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.041221</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-19</th>\n",
       "      <td>0.618143</td>\n",
       "      <td>0.621053</td>\n",
       "      <td>0.613851</td>\n",
       "      <td>0.621421</td>\n",
       "      <td>0.153304</td>\n",
       "      <td>0.086919</td>\n",
       "      <td>0.159428</td>\n",
       "      <td>0.078370</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.157898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.082443</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-30</th>\n",
       "      <td>0.698312</td>\n",
       "      <td>0.705263</td>\n",
       "      <td>0.695698</td>\n",
       "      <td>0.694592</td>\n",
       "      <td>0.859131</td>\n",
       "      <td>0.510322</td>\n",
       "      <td>0.908567</td>\n",
       "      <td>0.070010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.082443</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-31</th>\n",
       "      <td>0.687764</td>\n",
       "      <td>0.661053</td>\n",
       "      <td>0.685205</td>\n",
       "      <td>0.667020</td>\n",
       "      <td>0.448574</td>\n",
       "      <td>0.243669</td>\n",
       "      <td>0.468762</td>\n",
       "      <td>0.080460</td>\n",
       "      <td>0.202614</td>\n",
       "      <td>0.372141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.082443</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1236 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 開盤價       收盤價       最高價       最低價      成交張數      成交筆數   \n",
       "date                                                                     \n",
       "2018-01-25  0.099156  0.105263  0.110178  0.106045  0.252803  0.045984  \\\n",
       "2018-01-26  0.095992  0.098947  0.096537  0.099682  0.236785  0.035822   \n",
       "2018-01-29  0.101266  0.106316  0.104932  0.102863  0.164357  0.033759   \n",
       "2018-01-30  0.094937  0.094737  0.096537  0.097561  0.200571  0.036383   \n",
       "2018-01-31  0.088608  0.098947  0.096537  0.096501  0.250389  0.038297   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2023-01-16  0.622363  0.625263  0.622246  0.628844  0.208705  0.154967   \n",
       "2023-01-17  0.618143  0.621053  0.613851  0.621421  0.153304  0.086919   \n",
       "2023-01-19  0.618143  0.621053  0.613851  0.621421  0.153304  0.086919   \n",
       "2023-01-30  0.698312  0.705263  0.695698  0.694592  0.859131  0.510322   \n",
       "2023-01-31  0.687764  0.661053  0.685205  0.667020  0.448574  0.243669   \n",
       "\n",
       "                成交金額      成交均張       漲跌幅  買進_外陸資買進股數(不含外資自營商)  ...   \n",
       "date                                                           ...   \n",
       "2018-01-25  0.127251  0.273772  0.477124             0.249479  ...  \\\n",
       "2018-01-26  0.117162  0.322884  0.437908             0.196885  ...   \n",
       "2018-01-29  0.082338  0.236155  0.522876             0.129802  ...   \n",
       "2018-01-30  0.099051  0.269592  0.405229             0.151488  ...   \n",
       "2018-01-31  0.123688  0.321839  0.503268             0.254194  ...   \n",
       "...              ...       ...       ...                  ...  ...   \n",
       "2023-01-16  0.214396  0.053292  0.542484             0.239373  ...   \n",
       "2023-01-17  0.159428  0.078370  0.450980             0.157898  ...   \n",
       "2023-01-19  0.159428  0.078370  0.450980             0.157898  ...   \n",
       "2023-01-30  0.908567  0.070010  1.000000             1.000000  ...   \n",
       "2023-01-31  0.468762  0.080460  0.202614             0.372141  ...   \n",
       "\n",
       "            30,001-40,000(股)  40,001-50,000(股)  50,001-100,000(股)   \n",
       "date                                                                \n",
       "2018-01-25              0.00          0.076923           0.000000  \\\n",
       "2018-01-26              0.00          0.000000           0.000000   \n",
       "2018-01-29              0.00          0.000000           0.000000   \n",
       "2018-01-30              0.00          0.000000           0.000000   \n",
       "2018-01-31              0.00          0.000000           0.000000   \n",
       "...                      ...               ...                ...   \n",
       "2023-01-16              0.95          0.923077           0.971429   \n",
       "2023-01-17              0.95          0.923077           0.971429   \n",
       "2023-01-19              0.90          0.923077           0.942857   \n",
       "2023-01-30              0.90          0.923077           0.942857   \n",
       "2023-01-31              0.90          0.923077           0.942857   \n",
       "\n",
       "            100,001-200,000(股)  200,001-400,000(股)  400,001-600,000(股)   \n",
       "date                                                                     \n",
       "2018-01-25                0.00            0.071429                0.16  \\\n",
       "2018-01-26                0.00            0.107143                0.24   \n",
       "2018-01-29                0.00            0.107143                0.24   \n",
       "2018-01-30                0.00            0.107143                0.24   \n",
       "2018-01-31                0.00            0.107143                0.24   \n",
       "...                        ...                 ...                 ...   \n",
       "2023-01-16                1.00            0.750000                0.56   \n",
       "2023-01-17                1.00            0.750000                0.56   \n",
       "2023-01-19                0.96            0.750000                0.68   \n",
       "2023-01-30                0.96            0.750000                0.68   \n",
       "2023-01-31                0.96            0.750000                0.68   \n",
       "\n",
       "            600,001-800,000(股)  800,001-1,000,000(股)  1,000,001(股)以上  Label  \n",
       "date                                                                         \n",
       "2018-01-25                0.10              0.476190        1.000000    1.0  \n",
       "2018-01-26                0.10              0.523810        1.000000    1.0  \n",
       "2018-01-29                0.10              0.523810        1.000000    1.0  \n",
       "2018-01-30                0.10              0.523810        1.000000    1.0  \n",
       "2018-01-31                0.10              0.523810        1.000000    1.0  \n",
       "...                        ...                   ...             ...    ...  \n",
       "2023-01-16                0.85              0.952381        0.041221    1.0  \n",
       "2023-01-17                0.85              0.952381        0.041221    1.0  \n",
       "2023-01-19                0.75              0.809524        0.082443    1.0  \n",
       "2023-01-30                0.75              0.809524        0.082443    1.0  \n",
       "2023-01-31                0.75              0.809524        0.082443    1.0  \n",
       "\n",
       "[1236 rows x 74 columns]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 創建標籤\n",
    "df['Label'] = df['收盤價'].rolling(15).apply(lambda x: 1 if list(x)[-1] / list(x)[0] - 1 > 0.07 else 0, raw=True)\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料前處理：將所有的非數值資料轉換為數值型態\n",
    "df = df.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size :865 val size:371\n"
     ]
    }
   ],
   "source": [
    "#分割訓練集\n",
    "train_size = int(len(df) * 0.7)\n",
    "train_set = df[:train_size]\n",
    "val_set = df[train_size:]\n",
    "\n",
    "print(\"train size :\"+str(len(train_set))+\" \"+\"val size:\"+str(len(val_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 技術面指標\n",
    "tech_train_set = train_set[['開盤價','收盤價','最高價','最低價','成交張數','成交筆數','成交金額','成交均張','漲跌幅']]\n",
    "tech_val_set = val_set[['開盤價','收盤價','最高價','最低價','成交張數','成交筆數','成交金額','成交均張','漲跌幅']]\n",
    "# 籌碼面\n",
    "chip_train_set = train_set.drop(columns=['開盤價','收盤價','最高價','最低價','成交張數','成交筆數','成交金額','成交均張','漲跌幅'])\n",
    "chip_val_set = val_set.drop(columns=['開盤價','收盤價','最高價','最低價','成交張數','成交筆數','成交金額','成交均張','漲跌幅'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(bootstrap=False, max_features=1, n_estimators=1000,\n",
       "                       n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(bootstrap=False, max_features=1, n_estimators=1000,\n",
       "                       n_jobs=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, max_features=1, n_estimators=1000,\n",
       "                       n_jobs=-1)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RF MDI 尋找籌碼面的特徵重要性\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train = chip_train_set.drop('Label',axis=1)\n",
    "y_train = chip_train_set['Label']\n",
    "X_val = chip_val_set.drop('Label',axis=1)\n",
    "y_val = chip_val_set['Label']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000, max_features=1 ,n_jobs=-1,bootstrap=False)\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>累計_融資_增減(張)</th>\n",
       "      <td>0.026249</td>\n",
       "      <td>0.000931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>融資_融資今日餘額(張)</th>\n",
       "      <td>0.024094</td>\n",
       "      <td>0.000841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>累計_買進_自營商買進股數(自行買賣+避險)</th>\n",
       "      <td>0.022014</td>\n",
       "      <td>0.000651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>累計_三大法人賣出股數</th>\n",
       "      <td>0.021857</td>\n",
       "      <td>0.000644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>累計_賣出_自營商賣出股數(自行買賣+避險)</th>\n",
       "      <td>0.021558</td>\n",
       "      <td>0.000628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>累計_賣出_外陸資賣出股數(不含外資自營商)</th>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.000612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>累計_賣出_投信賣出股數</th>\n",
       "      <td>0.021428</td>\n",
       "      <td>0.000644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>累計_融資_融資買進(張)</th>\n",
       "      <td>0.021405</td>\n",
       "      <td>0.000630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>三大法人買賣超股數</th>\n",
       "      <td>0.021259</td>\n",
       "      <td>0.000685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>累計_借券賣出_當日賣出</th>\n",
       "      <td>0.021132</td>\n",
       "      <td>0.000616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            mean       std\n",
       "累計_融資_增減(張)             0.026249  0.000931\n",
       "融資_融資今日餘額(張)            0.024094  0.000841\n",
       "累計_買進_自營商買進股數(自行買賣+避險)  0.022014  0.000651\n",
       "累計_三大法人賣出股數             0.021857  0.000644\n",
       "累計_賣出_自營商賣出股數(自行買賣+避險)  0.021558  0.000628\n",
       "累計_賣出_外陸資賣出股數(不含外資自營商)  0.021429  0.000612\n",
       "累計_賣出_投信賣出股數            0.021428  0.000644\n",
       "累計_融資_融資買進(張)           0.021405  0.000630\n",
       "三大法人買賣超股數               0.021259  0.000685\n",
       "累計_借券賣出_當日賣出            0.021132  0.000616"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = X_train.columns\n",
    "\n",
    "# 計算特徵重要性\n",
    "importances = myMethod.featImpMDI(rf, feature_names)\n",
    "\n",
    "importances = importances.sort_values('mean',ascending=False)\n",
    "\n",
    "# 抓前10個特徵\n",
    "importances = importances.head(10)\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>開盤價</th>\n",
       "      <th>收盤價</th>\n",
       "      <th>最高價</th>\n",
       "      <th>最低價</th>\n",
       "      <th>成交張數</th>\n",
       "      <th>成交筆數</th>\n",
       "      <th>成交金額</th>\n",
       "      <th>成交均張</th>\n",
       "      <th>漲跌幅</th>\n",
       "      <th>累計_融資_增減(張)</th>\n",
       "      <th>融資_融資今日餘額(張)</th>\n",
       "      <th>累計_買進_自營商買進股數(自行買賣+避險)</th>\n",
       "      <th>累計_三大法人賣出股數</th>\n",
       "      <th>累計_賣出_自營商賣出股數(自行買賣+避險)</th>\n",
       "      <th>累計_賣出_外陸資賣出股數(不含外資自營商)</th>\n",
       "      <th>累計_賣出_投信賣出股數</th>\n",
       "      <th>累計_融資_融資買進(張)</th>\n",
       "      <th>三大法人買賣超股數</th>\n",
       "      <th>累計_借券賣出_當日賣出</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-25</th>\n",
       "      <td>0.099156</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.110178</td>\n",
       "      <td>0.106045</td>\n",
       "      <td>0.252803</td>\n",
       "      <td>0.045984</td>\n",
       "      <td>0.127251</td>\n",
       "      <td>0.273772</td>\n",
       "      <td>0.477124</td>\n",
       "      <td>0.021980</td>\n",
       "      <td>0.022159</td>\n",
       "      <td>0.008978</td>\n",
       "      <td>0.008983</td>\n",
       "      <td>0.006822</td>\n",
       "      <td>0.009012</td>\n",
       "      <td>0.013965</td>\n",
       "      <td>0.004927</td>\n",
       "      <td>0.497373</td>\n",
       "      <td>0.011256</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-26</th>\n",
       "      <td>0.095992</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.096537</td>\n",
       "      <td>0.099682</td>\n",
       "      <td>0.236785</td>\n",
       "      <td>0.035822</td>\n",
       "      <td>0.117162</td>\n",
       "      <td>0.322884</td>\n",
       "      <td>0.437908</td>\n",
       "      <td>0.042959</td>\n",
       "      <td>0.043281</td>\n",
       "      <td>0.009808</td>\n",
       "      <td>0.009840</td>\n",
       "      <td>0.007132</td>\n",
       "      <td>0.009843</td>\n",
       "      <td>0.019161</td>\n",
       "      <td>0.005675</td>\n",
       "      <td>0.472976</td>\n",
       "      <td>0.012036</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-29</th>\n",
       "      <td>0.101266</td>\n",
       "      <td>0.106316</td>\n",
       "      <td>0.104932</td>\n",
       "      <td>0.102863</td>\n",
       "      <td>0.164357</td>\n",
       "      <td>0.033759</td>\n",
       "      <td>0.082338</td>\n",
       "      <td>0.236155</td>\n",
       "      <td>0.522876</td>\n",
       "      <td>0.057650</td>\n",
       "      <td>0.058072</td>\n",
       "      <td>0.010684</td>\n",
       "      <td>0.010298</td>\n",
       "      <td>0.007399</td>\n",
       "      <td>0.010305</td>\n",
       "      <td>0.019972</td>\n",
       "      <td>0.006480</td>\n",
       "      <td>0.495446</td>\n",
       "      <td>0.013655</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-30</th>\n",
       "      <td>0.094937</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.096537</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.200571</td>\n",
       "      <td>0.036383</td>\n",
       "      <td>0.099051</td>\n",
       "      <td>0.269592</td>\n",
       "      <td>0.405229</td>\n",
       "      <td>0.073927</td>\n",
       "      <td>0.074460</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.011037</td>\n",
       "      <td>0.007711</td>\n",
       "      <td>0.011062</td>\n",
       "      <td>0.020577</td>\n",
       "      <td>0.007253</td>\n",
       "      <td>0.456444</td>\n",
       "      <td>0.013662</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>0.088608</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.096537</td>\n",
       "      <td>0.096501</td>\n",
       "      <td>0.250389</td>\n",
       "      <td>0.038297</td>\n",
       "      <td>0.123688</td>\n",
       "      <td>0.321839</td>\n",
       "      <td>0.503268</td>\n",
       "      <td>0.072925</td>\n",
       "      <td>0.073452</td>\n",
       "      <td>0.013509</td>\n",
       "      <td>0.012128</td>\n",
       "      <td>0.008126</td>\n",
       "      <td>0.012190</td>\n",
       "      <td>0.020688</td>\n",
       "      <td>0.007560</td>\n",
       "      <td>0.486697</td>\n",
       "      <td>0.013740</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-23</th>\n",
       "      <td>0.803797</td>\n",
       "      <td>0.793684</td>\n",
       "      <td>0.798531</td>\n",
       "      <td>0.798515</td>\n",
       "      <td>0.071799</td>\n",
       "      <td>0.072394</td>\n",
       "      <td>0.095404</td>\n",
       "      <td>0.037618</td>\n",
       "      <td>0.398693</td>\n",
       "      <td>0.604797</td>\n",
       "      <td>0.601199</td>\n",
       "      <td>0.684841</td>\n",
       "      <td>0.727231</td>\n",
       "      <td>0.726177</td>\n",
       "      <td>0.727284</td>\n",
       "      <td>0.726153</td>\n",
       "      <td>0.719883</td>\n",
       "      <td>0.430449</td>\n",
       "      <td>0.714828</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-26</th>\n",
       "      <td>0.801688</td>\n",
       "      <td>0.783158</td>\n",
       "      <td>0.796432</td>\n",
       "      <td>0.792153</td>\n",
       "      <td>0.111768</td>\n",
       "      <td>0.120387</td>\n",
       "      <td>0.139974</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.615147</td>\n",
       "      <td>0.611620</td>\n",
       "      <td>0.685292</td>\n",
       "      <td>0.727747</td>\n",
       "      <td>0.726577</td>\n",
       "      <td>0.727799</td>\n",
       "      <td>0.727114</td>\n",
       "      <td>0.720408</td>\n",
       "      <td>0.431916</td>\n",
       "      <td>0.715130</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-27</th>\n",
       "      <td>0.780591</td>\n",
       "      <td>0.783158</td>\n",
       "      <td>0.781742</td>\n",
       "      <td>0.792153</td>\n",
       "      <td>0.090903</td>\n",
       "      <td>0.060767</td>\n",
       "      <td>0.115968</td>\n",
       "      <td>0.066876</td>\n",
       "      <td>0.477124</td>\n",
       "      <td>0.614424</td>\n",
       "      <td>0.610892</td>\n",
       "      <td>0.685864</td>\n",
       "      <td>0.728159</td>\n",
       "      <td>0.726628</td>\n",
       "      <td>0.728219</td>\n",
       "      <td>0.728152</td>\n",
       "      <td>0.720692</td>\n",
       "      <td>0.462666</td>\n",
       "      <td>0.715143</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-28</th>\n",
       "      <td>0.770042</td>\n",
       "      <td>0.781053</td>\n",
       "      <td>0.771249</td>\n",
       "      <td>0.777306</td>\n",
       "      <td>0.214418</td>\n",
       "      <td>0.246371</td>\n",
       "      <td>0.252392</td>\n",
       "      <td>0.024033</td>\n",
       "      <td>0.464052</td>\n",
       "      <td>0.607607</td>\n",
       "      <td>0.604028</td>\n",
       "      <td>0.688639</td>\n",
       "      <td>0.729160</td>\n",
       "      <td>0.727738</td>\n",
       "      <td>0.729226</td>\n",
       "      <td>0.728209</td>\n",
       "      <td>0.721174</td>\n",
       "      <td>0.415421</td>\n",
       "      <td>0.716490</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-29</th>\n",
       "      <td>0.789030</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.783841</td>\n",
       "      <td>0.785790</td>\n",
       "      <td>0.127028</td>\n",
       "      <td>0.083328</td>\n",
       "      <td>0.156456</td>\n",
       "      <td>0.065831</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.608024</td>\n",
       "      <td>0.604449</td>\n",
       "      <td>0.688934</td>\n",
       "      <td>0.729818</td>\n",
       "      <td>0.727944</td>\n",
       "      <td>0.729903</td>\n",
       "      <td>0.728719</td>\n",
       "      <td>0.721341</td>\n",
       "      <td>0.461213</td>\n",
       "      <td>0.716700</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>865 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 開盤價       收盤價       最高價       最低價      成交張數      成交筆數   \n",
       "date                                                                     \n",
       "2018-01-25  0.099156  0.105263  0.110178  0.106045  0.252803  0.045984  \\\n",
       "2018-01-26  0.095992  0.098947  0.096537  0.099682  0.236785  0.035822   \n",
       "2018-01-29  0.101266  0.106316  0.104932  0.102863  0.164357  0.033759   \n",
       "2018-01-30  0.094937  0.094737  0.096537  0.097561  0.200571  0.036383   \n",
       "2018-01-31  0.088608  0.098947  0.096537  0.096501  0.250389  0.038297   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2021-07-23  0.803797  0.793684  0.798531  0.798515  0.071799  0.072394   \n",
       "2021-07-26  0.801688  0.783158  0.796432  0.792153  0.111768  0.120387   \n",
       "2021-07-27  0.780591  0.783158  0.781742  0.792153  0.090903  0.060767   \n",
       "2021-07-28  0.770042  0.781053  0.771249  0.777306  0.214418  0.246371   \n",
       "2021-07-29  0.789030  0.789474  0.783841  0.785790  0.127028  0.083328   \n",
       "\n",
       "                成交金額      成交均張       漲跌幅  累計_融資_增減(張)  融資_融資今日餘額(張)   \n",
       "date                                                                  \n",
       "2018-01-25  0.127251  0.273772  0.477124     0.021980      0.022159  \\\n",
       "2018-01-26  0.117162  0.322884  0.437908     0.042959      0.043281   \n",
       "2018-01-29  0.082338  0.236155  0.522876     0.057650      0.058072   \n",
       "2018-01-30  0.099051  0.269592  0.405229     0.073927      0.074460   \n",
       "2018-01-31  0.123688  0.321839  0.503268     0.072925      0.073452   \n",
       "...              ...       ...       ...          ...           ...   \n",
       "2021-07-23  0.095404  0.037618  0.398693     0.604797      0.601199   \n",
       "2021-07-26  0.139974  0.030303  0.411765     0.615147      0.611620   \n",
       "2021-07-27  0.115968  0.066876  0.477124     0.614424      0.610892   \n",
       "2021-07-28  0.252392  0.024033  0.464052     0.607607      0.604028   \n",
       "2021-07-29  0.156456  0.065831  0.529412     0.608024      0.604449   \n",
       "\n",
       "            累計_買進_自營商買進股數(自行買賣+避險)  累計_三大法人賣出股數  累計_賣出_自營商賣出股數(自行買賣+避險)   \n",
       "date                                                                      \n",
       "2018-01-25                0.008978     0.008983                0.006822  \\\n",
       "2018-01-26                0.009808     0.009840                0.007132   \n",
       "2018-01-29                0.010684     0.010298                0.007399   \n",
       "2018-01-30                0.011428     0.011037                0.007711   \n",
       "2018-01-31                0.013509     0.012128                0.008126   \n",
       "...                            ...          ...                     ...   \n",
       "2021-07-23                0.684841     0.727231                0.726177   \n",
       "2021-07-26                0.685292     0.727747                0.726577   \n",
       "2021-07-27                0.685864     0.728159                0.726628   \n",
       "2021-07-28                0.688639     0.729160                0.727738   \n",
       "2021-07-29                0.688934     0.729818                0.727944   \n",
       "\n",
       "            累計_賣出_外陸資賣出股數(不含外資自營商)  累計_賣出_投信賣出股數  累計_融資_融資買進(張)  三大法人買賣超股數   \n",
       "date                                                                         \n",
       "2018-01-25                0.009012      0.013965       0.004927   0.497373  \\\n",
       "2018-01-26                0.009843      0.019161       0.005675   0.472976   \n",
       "2018-01-29                0.010305      0.019972       0.006480   0.495446   \n",
       "2018-01-30                0.011062      0.020577       0.007253   0.456444   \n",
       "2018-01-31                0.012190      0.020688       0.007560   0.486697   \n",
       "...                            ...           ...            ...        ...   \n",
       "2021-07-23                0.727284      0.726153       0.719883   0.430449   \n",
       "2021-07-26                0.727799      0.727114       0.720408   0.431916   \n",
       "2021-07-27                0.728219      0.728152       0.720692   0.462666   \n",
       "2021-07-28                0.729226      0.728209       0.721174   0.415421   \n",
       "2021-07-29                0.729903      0.728719       0.721341   0.461213   \n",
       "\n",
       "            累計_借券賣出_當日賣出  Label  \n",
       "date                             \n",
       "2018-01-25      0.011256    1.0  \n",
       "2018-01-26      0.012036    1.0  \n",
       "2018-01-29      0.013655    1.0  \n",
       "2018-01-30      0.013662    1.0  \n",
       "2018-01-31      0.013740    1.0  \n",
       "...                  ...    ...  \n",
       "2021-07-23      0.714828    0.0  \n",
       "2021-07-26      0.715130    0.0  \n",
       "2021-07-27      0.715143    0.0  \n",
       "2021-07-28      0.716490    0.0  \n",
       "2021-07-29      0.716700    0.0  \n",
       "\n",
       "[865 rows x 20 columns]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_set = pd.concat([tech_train_set, chip_train_set[importances.index]], axis=1)\n",
    "\n",
    "new_val_set = pd.concat([tech_val_set,chip_val_set[importances.index]],axis=1)\n",
    "\n",
    "new_train_set['Label'] = train_set['Label']\n",
    "new_val_set['Label'] = val_set['Label']\n",
    "\n",
    "new_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 轉換成numpy array\n",
    "train_set_np = new_train_set.values\n",
    "test_set_np = new_val_set.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建 DataLoader\n",
    "lookback = 15\n",
    "train_data = StockDataset(train_set_np, lookback)\n",
    "test_data = StockDataset(test_set_np, lookback)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Step [10/22], Loss: 0.7810\n",
      "Epoch [1/500], Step [20/22], Loss: 0.6485\n",
      "Epoch [2/500], Step [10/22], Loss: 0.7672\n",
      "Epoch [2/500], Step [20/22], Loss: 0.6098\n",
      "Epoch [3/500], Step [10/22], Loss: 0.7762\n",
      "Epoch [3/500], Step [20/22], Loss: 0.5441\n",
      "Epoch [4/500], Step [10/22], Loss: 0.7777\n",
      "Epoch [4/500], Step [20/22], Loss: 0.4082\n",
      "Epoch [5/500], Step [10/22], Loss: 0.6569\n",
      "Epoch [5/500], Step [20/22], Loss: 0.3124\n",
      "Epoch [6/500], Step [10/22], Loss: 0.5998\n",
      "Epoch [6/500], Step [20/22], Loss: 0.2628\n",
      "Epoch [7/500], Step [10/22], Loss: 0.5893\n",
      "Epoch [7/500], Step [20/22], Loss: 0.3462\n",
      "Epoch [8/500], Step [10/22], Loss: 0.6056\n",
      "Epoch [8/500], Step [20/22], Loss: 0.2443\n",
      "Epoch [9/500], Step [10/22], Loss: 0.5570\n",
      "Epoch [9/500], Step [20/22], Loss: 0.6227\n",
      "Epoch [10/500], Step [10/22], Loss: 0.6667\n",
      "Epoch [10/500], Step [20/22], Loss: 0.5048\n",
      "Epoch [11/500], Step [10/22], Loss: 0.6591\n",
      "Epoch [11/500], Step [20/22], Loss: 0.4046\n",
      "Epoch [12/500], Step [10/22], Loss: 0.5626\n",
      "Epoch [12/500], Step [20/22], Loss: 0.3047\n",
      "Epoch [13/500], Step [10/22], Loss: 0.4934\n",
      "Epoch [13/500], Step [20/22], Loss: 0.2814\n",
      "Epoch [14/500], Step [10/22], Loss: 0.4992\n",
      "Epoch [14/500], Step [20/22], Loss: 0.2559\n",
      "Epoch [15/500], Step [10/22], Loss: 0.4167\n",
      "Epoch [15/500], Step [20/22], Loss: 0.2452\n",
      "Epoch [16/500], Step [10/22], Loss: 0.3781\n",
      "Epoch [16/500], Step [20/22], Loss: 0.2436\n",
      "Epoch [17/500], Step [10/22], Loss: 0.3127\n",
      "Epoch [17/500], Step [20/22], Loss: 0.3621\n",
      "Epoch [18/500], Step [10/22], Loss: 0.8306\n",
      "Epoch [18/500], Step [20/22], Loss: 0.2696\n",
      "Epoch [19/500], Step [10/22], Loss: 0.4311\n",
      "Epoch [19/500], Step [20/22], Loss: 0.2446\n",
      "Epoch [20/500], Step [10/22], Loss: 0.3457\n",
      "Epoch [20/500], Step [20/22], Loss: 0.2403\n",
      "Epoch [21/500], Step [10/22], Loss: 0.3307\n",
      "Epoch [21/500], Step [20/22], Loss: 0.2385\n",
      "Epoch [22/500], Step [10/22], Loss: 0.2978\n",
      "Epoch [22/500], Step [20/22], Loss: 0.2364\n",
      "Epoch [23/500], Step [10/22], Loss: 0.2547\n",
      "Epoch [23/500], Step [20/22], Loss: 0.2362\n",
      "Epoch [24/500], Step [10/22], Loss: 0.3264\n",
      "Epoch [24/500], Step [20/22], Loss: 0.2360\n",
      "Epoch [25/500], Step [10/22], Loss: 0.6396\n",
      "Epoch [25/500], Step [20/22], Loss: 0.2460\n",
      "Epoch [26/500], Step [10/22], Loss: 0.2665\n",
      "Epoch [26/500], Step [20/22], Loss: 0.2344\n",
      "Epoch [27/500], Step [10/22], Loss: 0.3172\n",
      "Epoch [27/500], Step [20/22], Loss: 0.2619\n",
      "Epoch [28/500], Step [10/22], Loss: 0.2647\n",
      "Epoch [28/500], Step [20/22], Loss: 0.5096\n",
      "Epoch [29/500], Step [10/22], Loss: 0.4384\n",
      "Epoch [29/500], Step [20/22], Loss: 0.4070\n",
      "Epoch [30/500], Step [10/22], Loss: 0.2035\n",
      "Epoch [30/500], Step [20/22], Loss: 0.3419\n",
      "Epoch [31/500], Step [10/22], Loss: 0.5348\n",
      "Epoch [31/500], Step [20/22], Loss: 0.2922\n",
      "Epoch [32/500], Step [10/22], Loss: 0.1951\n",
      "Epoch [32/500], Step [20/22], Loss: 0.2543\n",
      "Epoch [33/500], Step [10/22], Loss: 0.6105\n",
      "Epoch [33/500], Step [20/22], Loss: 0.2451\n",
      "Epoch [34/500], Step [10/22], Loss: 0.2549\n",
      "Epoch [34/500], Step [20/22], Loss: 0.2352\n",
      "Epoch [35/500], Step [10/22], Loss: 0.3926\n",
      "Epoch [35/500], Step [20/22], Loss: 0.2386\n",
      "Epoch [36/500], Step [10/22], Loss: 0.5289\n",
      "Epoch [36/500], Step [20/22], Loss: 0.2363\n",
      "Epoch [37/500], Step [10/22], Loss: 0.2066\n",
      "Epoch [37/500], Step [20/22], Loss: 0.2375\n",
      "Epoch [38/500], Step [10/22], Loss: 0.6922\n",
      "Epoch [38/500], Step [20/22], Loss: 0.2391\n",
      "Epoch [39/500], Step [10/22], Loss: 0.4189\n",
      "Epoch [39/500], Step [20/22], Loss: 0.2460\n",
      "Epoch [40/500], Step [10/22], Loss: 0.3099\n",
      "Epoch [40/500], Step [20/22], Loss: 0.2386\n",
      "Epoch [41/500], Step [10/22], Loss: 0.4379\n",
      "Epoch [41/500], Step [20/22], Loss: 0.2362\n",
      "Epoch [42/500], Step [10/22], Loss: 0.2655\n",
      "Epoch [42/500], Step [20/22], Loss: 0.2359\n",
      "Epoch [43/500], Step [10/22], Loss: 0.3231\n",
      "Epoch [43/500], Step [20/22], Loss: 0.2339\n",
      "Epoch [44/500], Step [10/22], Loss: 0.3506\n",
      "Epoch [44/500], Step [20/22], Loss: 0.4630\n",
      "Epoch [45/500], Step [10/22], Loss: 0.7059\n",
      "Epoch [45/500], Step [20/22], Loss: 0.3505\n",
      "Epoch [46/500], Step [10/22], Loss: 0.4367\n",
      "Epoch [46/500], Step [20/22], Loss: 0.2749\n",
      "Epoch [47/500], Step [10/22], Loss: 0.3393\n",
      "Epoch [47/500], Step [20/22], Loss: 0.2448\n",
      "Epoch [48/500], Step [10/22], Loss: 0.1824\n",
      "Epoch [48/500], Step [20/22], Loss: 0.2414\n",
      "Epoch [49/500], Step [10/22], Loss: 0.6153\n",
      "Epoch [49/500], Step [20/22], Loss: 0.2363\n",
      "Epoch [50/500], Step [10/22], Loss: 0.3596\n",
      "Epoch [50/500], Step [20/22], Loss: 0.2324\n",
      "Epoch [51/500], Step [10/22], Loss: 0.1626\n",
      "Epoch [51/500], Step [20/22], Loss: 0.2297\n",
      "Epoch [52/500], Step [10/22], Loss: 0.7201\n",
      "Epoch [52/500], Step [20/22], Loss: 0.2322\n",
      "Epoch [53/500], Step [10/22], Loss: 0.4825\n",
      "Epoch [53/500], Step [20/22], Loss: 0.2358\n",
      "Epoch [54/500], Step [10/22], Loss: 0.2255\n",
      "Epoch [54/500], Step [20/22], Loss: 0.2310\n",
      "Epoch [55/500], Step [10/22], Loss: 0.7390\n",
      "Epoch [55/500], Step [20/22], Loss: 0.2178\n",
      "Epoch [56/500], Step [10/22], Loss: 0.3469\n",
      "Epoch [56/500], Step [20/22], Loss: 0.2372\n",
      "Epoch [57/500], Step [10/22], Loss: 0.3232\n",
      "Epoch [57/500], Step [20/22], Loss: 0.2144\n",
      "Epoch [58/500], Step [10/22], Loss: 0.3141\n",
      "Epoch [58/500], Step [20/22], Loss: 0.2395\n",
      "Epoch [59/500], Step [10/22], Loss: 0.3350\n",
      "Epoch [59/500], Step [20/22], Loss: 0.2288\n",
      "Epoch [60/500], Step [10/22], Loss: 0.1877\n",
      "Epoch [60/500], Step [20/22], Loss: 0.2256\n",
      "Epoch [61/500], Step [10/22], Loss: 0.7606\n",
      "Epoch [61/500], Step [20/22], Loss: 0.2389\n",
      "Epoch [62/500], Step [10/22], Loss: 0.3738\n",
      "Epoch [62/500], Step [20/22], Loss: 0.2313\n",
      "Epoch [63/500], Step [10/22], Loss: 0.3584\n",
      "Epoch [63/500], Step [20/22], Loss: 0.2324\n",
      "Epoch [64/500], Step [10/22], Loss: 0.2184\n",
      "Epoch [64/500], Step [20/22], Loss: 0.2236\n",
      "Epoch [65/500], Step [10/22], Loss: 0.5774\n",
      "Epoch [65/500], Step [20/22], Loss: 0.2360\n",
      "Epoch [66/500], Step [10/22], Loss: 0.1739\n",
      "Epoch [66/500], Step [20/22], Loss: 0.2278\n",
      "Epoch [67/500], Step [10/22], Loss: 0.3706\n",
      "Epoch [67/500], Step [20/22], Loss: 0.2528\n",
      "Epoch [68/500], Step [10/22], Loss: 0.4812\n",
      "Epoch [68/500], Step [20/22], Loss: 0.2325\n",
      "Epoch [69/500], Step [10/22], Loss: 0.1880\n",
      "Epoch [69/500], Step [20/22], Loss: 0.1800\n",
      "Epoch [70/500], Step [10/22], Loss: 0.3996\n",
      "Epoch [70/500], Step [20/22], Loss: 0.2600\n",
      "Epoch [71/500], Step [10/22], Loss: 0.2015\n",
      "Epoch [71/500], Step [20/22], Loss: 0.2265\n",
      "Epoch [72/500], Step [10/22], Loss: 0.2412\n",
      "Epoch [72/500], Step [20/22], Loss: 0.2368\n",
      "Epoch [73/500], Step [10/22], Loss: 0.4313\n",
      "Epoch [73/500], Step [20/22], Loss: 0.2053\n",
      "Epoch [74/500], Step [10/22], Loss: 1.0328\n",
      "Epoch [74/500], Step [20/22], Loss: 0.1808\n",
      "Epoch [75/500], Step [10/22], Loss: 0.3062\n",
      "Epoch [75/500], Step [20/22], Loss: 0.2171\n",
      "Epoch [76/500], Step [10/22], Loss: 0.3122\n",
      "Epoch [76/500], Step [20/22], Loss: 0.2246\n",
      "Epoch [77/500], Step [10/22], Loss: 0.1879\n",
      "Epoch [77/500], Step [20/22], Loss: 0.2297\n",
      "Epoch [78/500], Step [10/22], Loss: 0.3832\n",
      "Epoch [78/500], Step [20/22], Loss: 0.2004\n",
      "Epoch [79/500], Step [10/22], Loss: 0.2325\n",
      "Epoch [79/500], Step [20/22], Loss: 0.2332\n",
      "Epoch [80/500], Step [10/22], Loss: 0.1560\n",
      "Epoch [80/500], Step [20/22], Loss: 0.2468\n",
      "Epoch [81/500], Step [10/22], Loss: 0.6521\n",
      "Epoch [81/500], Step [20/22], Loss: 0.1500\n",
      "Epoch [82/500], Step [10/22], Loss: 0.4047\n",
      "Epoch [82/500], Step [20/22], Loss: 0.1856\n",
      "Epoch [83/500], Step [10/22], Loss: 0.1500\n",
      "Epoch [83/500], Step [20/22], Loss: 0.2188\n",
      "Epoch [84/500], Step [10/22], Loss: 0.4625\n",
      "Epoch [84/500], Step [20/22], Loss: 0.1563\n",
      "Epoch [85/500], Step [10/22], Loss: 0.8483\n",
      "Epoch [85/500], Step [20/22], Loss: 0.1857\n",
      "Epoch [86/500], Step [10/22], Loss: 0.3953\n",
      "Epoch [86/500], Step [20/22], Loss: 0.1605\n",
      "Epoch [87/500], Step [10/22], Loss: 0.4306\n",
      "Epoch [87/500], Step [20/22], Loss: 0.1991\n",
      "Epoch [88/500], Step [10/22], Loss: 0.1972\n",
      "Epoch [88/500], Step [20/22], Loss: 0.1882\n",
      "Epoch [89/500], Step [10/22], Loss: 0.5232\n",
      "Epoch [89/500], Step [20/22], Loss: 0.1914\n",
      "Epoch [90/500], Step [10/22], Loss: 0.1842\n",
      "Epoch [90/500], Step [20/22], Loss: 0.2221\n",
      "Epoch [91/500], Step [10/22], Loss: 0.3074\n",
      "Epoch [91/500], Step [20/22], Loss: 0.1440\n",
      "Epoch [92/500], Step [10/22], Loss: 0.6488\n",
      "Epoch [92/500], Step [20/22], Loss: 0.2015\n",
      "Epoch [93/500], Step [10/22], Loss: 0.1377\n",
      "Epoch [93/500], Step [20/22], Loss: 0.1926\n",
      "Epoch [94/500], Step [10/22], Loss: 0.5159\n",
      "Epoch [94/500], Step [20/22], Loss: 0.1547\n",
      "Epoch [95/500], Step [10/22], Loss: 0.4036\n",
      "Epoch [95/500], Step [20/22], Loss: 0.1828\n",
      "Epoch [96/500], Step [10/22], Loss: 0.1666\n",
      "Epoch [96/500], Step [20/22], Loss: 0.1749\n",
      "Epoch [97/500], Step [10/22], Loss: 0.3231\n",
      "Epoch [97/500], Step [20/22], Loss: 0.2050\n",
      "Epoch [98/500], Step [10/22], Loss: 0.6849\n",
      "Epoch [98/500], Step [20/22], Loss: 0.1302\n",
      "Epoch [99/500], Step [10/22], Loss: 0.2852\n",
      "Epoch [99/500], Step [20/22], Loss: 0.1099\n",
      "Epoch [100/500], Step [10/22], Loss: 0.1911\n",
      "Epoch [100/500], Step [20/22], Loss: 0.1132\n",
      "Epoch [101/500], Step [10/22], Loss: 0.6635\n",
      "Epoch [101/500], Step [20/22], Loss: 0.1117\n",
      "Epoch [102/500], Step [10/22], Loss: 0.1521\n",
      "Epoch [102/500], Step [20/22], Loss: 0.2080\n",
      "Epoch [103/500], Step [10/22], Loss: 0.4807\n",
      "Epoch [103/500], Step [20/22], Loss: 0.1284\n",
      "Epoch [104/500], Step [10/22], Loss: 0.2019\n",
      "Epoch [104/500], Step [20/22], Loss: 0.1380\n",
      "Epoch [105/500], Step [10/22], Loss: 0.1736\n",
      "Epoch [105/500], Step [20/22], Loss: 0.1187\n",
      "Epoch [106/500], Step [10/22], Loss: 0.4083\n",
      "Epoch [106/500], Step [20/22], Loss: 0.1329\n",
      "Epoch [107/500], Step [10/22], Loss: 0.5224\n",
      "Epoch [107/500], Step [20/22], Loss: 0.0770\n",
      "Epoch [108/500], Step [10/22], Loss: 0.1859\n",
      "Epoch [108/500], Step [20/22], Loss: 0.1052\n",
      "Epoch [109/500], Step [10/22], Loss: 0.2216\n",
      "Epoch [109/500], Step [20/22], Loss: 0.0923\n",
      "Epoch [110/500], Step [10/22], Loss: 0.6870\n",
      "Epoch [110/500], Step [20/22], Loss: 0.1414\n",
      "Epoch [111/500], Step [10/22], Loss: 0.2111\n",
      "Epoch [111/500], Step [20/22], Loss: 0.1753\n",
      "Epoch [112/500], Step [10/22], Loss: 0.3739\n",
      "Epoch [112/500], Step [20/22], Loss: 0.1431\n",
      "Epoch [113/500], Step [10/22], Loss: 0.1482\n",
      "Epoch [113/500], Step [20/22], Loss: 0.1508\n",
      "Epoch [114/500], Step [10/22], Loss: 0.1695\n",
      "Epoch [114/500], Step [20/22], Loss: 0.1862\n",
      "Epoch [115/500], Step [10/22], Loss: 0.5265\n",
      "Epoch [115/500], Step [20/22], Loss: 0.1025\n",
      "Epoch [116/500], Step [10/22], Loss: 0.2555\n",
      "Epoch [116/500], Step [20/22], Loss: 0.0960\n",
      "Epoch [117/500], Step [10/22], Loss: 0.1303\n",
      "Epoch [117/500], Step [20/22], Loss: 0.0921\n",
      "Epoch [118/500], Step [10/22], Loss: 0.7591\n",
      "Epoch [118/500], Step [20/22], Loss: 0.0604\n",
      "Epoch [119/500], Step [10/22], Loss: 0.2362\n",
      "Epoch [119/500], Step [20/22], Loss: 0.1709\n",
      "Epoch [120/500], Step [10/22], Loss: 0.3215\n",
      "Epoch [120/500], Step [20/22], Loss: 0.1515\n",
      "Epoch [121/500], Step [10/22], Loss: 0.1738\n",
      "Epoch [121/500], Step [20/22], Loss: 0.1035\n",
      "Epoch [122/500], Step [10/22], Loss: 0.5660\n",
      "Epoch [122/500], Step [20/22], Loss: 0.1145\n",
      "Epoch [123/500], Step [10/22], Loss: 0.1585\n",
      "Epoch [123/500], Step [20/22], Loss: 0.1588\n",
      "Epoch [124/500], Step [10/22], Loss: 0.5749\n",
      "Epoch [124/500], Step [20/22], Loss: 0.0974\n",
      "Epoch [125/500], Step [10/22], Loss: 0.1644\n",
      "Epoch [125/500], Step [20/22], Loss: 0.1671\n",
      "Epoch [126/500], Step [10/22], Loss: 0.2397\n",
      "Epoch [126/500], Step [20/22], Loss: 0.1457\n",
      "Epoch [127/500], Step [10/22], Loss: 0.3134\n",
      "Epoch [127/500], Step [20/22], Loss: 0.0995\n",
      "Epoch [128/500], Step [10/22], Loss: 0.3437\n",
      "Epoch [128/500], Step [20/22], Loss: 0.0823\n",
      "Epoch [129/500], Step [10/22], Loss: 0.0983\n",
      "Epoch [129/500], Step [20/22], Loss: 0.0756\n",
      "Epoch [130/500], Step [10/22], Loss: 0.4532\n",
      "Epoch [130/500], Step [20/22], Loss: 0.0573\n",
      "Epoch [131/500], Step [10/22], Loss: 0.5833\n",
      "Epoch [131/500], Step [20/22], Loss: 0.1002\n",
      "Epoch [132/500], Step [10/22], Loss: 0.1843\n",
      "Epoch [132/500], Step [20/22], Loss: 0.1922\n",
      "Epoch [133/500], Step [10/22], Loss: 0.4997\n",
      "Epoch [133/500], Step [20/22], Loss: 0.0815\n",
      "Epoch [134/500], Step [10/22], Loss: 0.1678\n",
      "Epoch [134/500], Step [20/22], Loss: 0.1377\n",
      "Epoch [135/500], Step [10/22], Loss: 0.3026\n",
      "Epoch [135/500], Step [20/22], Loss: 0.1362\n",
      "Epoch [136/500], Step [10/22], Loss: 0.3055\n",
      "Epoch [136/500], Step [20/22], Loss: 0.0886\n",
      "Epoch [137/500], Step [10/22], Loss: 0.4473\n",
      "Epoch [137/500], Step [20/22], Loss: 0.1111\n",
      "Epoch [138/500], Step [10/22], Loss: 0.1071\n",
      "Epoch [138/500], Step [20/22], Loss: 0.1178\n",
      "Epoch [139/500], Step [10/22], Loss: 0.7143\n",
      "Epoch [139/500], Step [20/22], Loss: 0.0777\n",
      "Epoch [140/500], Step [10/22], Loss: 0.2432\n",
      "Epoch [140/500], Step [20/22], Loss: 0.0710\n",
      "Epoch [141/500], Step [10/22], Loss: 0.3244\n",
      "Epoch [141/500], Step [20/22], Loss: 0.1133\n",
      "Epoch [142/500], Step [10/22], Loss: 0.1425\n",
      "Epoch [142/500], Step [20/22], Loss: 0.0919\n",
      "Epoch [143/500], Step [10/22], Loss: 0.3773\n",
      "Epoch [143/500], Step [20/22], Loss: 0.0540\n",
      "Epoch [144/500], Step [10/22], Loss: 0.1634\n",
      "Epoch [144/500], Step [20/22], Loss: 0.0493\n",
      "Epoch [145/500], Step [10/22], Loss: 0.2841\n",
      "Epoch [145/500], Step [20/22], Loss: 0.0659\n",
      "Epoch [146/500], Step [10/22], Loss: 0.1707\n",
      "Epoch [146/500], Step [20/22], Loss: 0.0601\n",
      "Epoch [147/500], Step [10/22], Loss: 0.1827\n",
      "Epoch [147/500], Step [20/22], Loss: 0.0383\n",
      "Epoch [148/500], Step [10/22], Loss: 0.5130\n",
      "Epoch [148/500], Step [20/22], Loss: 0.0953\n",
      "Epoch [149/500], Step [10/22], Loss: 0.1748\n",
      "Epoch [149/500], Step [20/22], Loss: 0.2197\n",
      "Epoch [150/500], Step [10/22], Loss: 0.1377\n",
      "Epoch [150/500], Step [20/22], Loss: 0.1449\n",
      "Epoch [151/500], Step [10/22], Loss: 0.4238\n",
      "Epoch [151/500], Step [20/22], Loss: 0.0922\n",
      "Epoch [152/500], Step [10/22], Loss: 0.5274\n",
      "Epoch [152/500], Step [20/22], Loss: 0.1962\n",
      "Epoch [153/500], Step [10/22], Loss: 0.2601\n",
      "Epoch [153/500], Step [20/22], Loss: 0.2018\n",
      "Epoch [154/500], Step [10/22], Loss: 0.1024\n",
      "Epoch [154/500], Step [20/22], Loss: 0.2136\n",
      "Epoch [155/500], Step [10/22], Loss: 0.8369\n",
      "Epoch [155/500], Step [20/22], Loss: 0.1715\n",
      "Epoch [156/500], Step [10/22], Loss: 0.2875\n",
      "Epoch [156/500], Step [20/22], Loss: 0.1142\n",
      "Epoch [157/500], Step [10/22], Loss: 0.2662\n",
      "Epoch [157/500], Step [20/22], Loss: 0.0914\n",
      "Epoch [158/500], Step [10/22], Loss: 0.2638\n",
      "Epoch [158/500], Step [20/22], Loss: 0.0843\n",
      "Epoch [159/500], Step [10/22], Loss: 0.1944\n",
      "Epoch [159/500], Step [20/22], Loss: 0.0739\n",
      "Epoch [160/500], Step [10/22], Loss: 0.1990\n",
      "Epoch [160/500], Step [20/22], Loss: 0.0665\n",
      "Epoch [161/500], Step [10/22], Loss: 0.2530\n",
      "Epoch [161/500], Step [20/22], Loss: 0.0612\n",
      "Epoch [162/500], Step [10/22], Loss: 0.1681\n",
      "Epoch [162/500], Step [20/22], Loss: 0.0377\n",
      "Epoch [163/500], Step [10/22], Loss: 0.1917\n",
      "Epoch [163/500], Step [20/22], Loss: 0.0458\n",
      "Epoch [164/500], Step [10/22], Loss: 0.2071\n",
      "Epoch [164/500], Step [20/22], Loss: 0.0379\n",
      "Epoch [165/500], Step [10/22], Loss: 0.3795\n",
      "Epoch [165/500], Step [20/22], Loss: 0.0769\n",
      "Epoch [166/500], Step [10/22], Loss: 0.1851\n",
      "Epoch [166/500], Step [20/22], Loss: 0.0187\n",
      "Epoch [167/500], Step [10/22], Loss: 0.1789\n",
      "Epoch [167/500], Step [20/22], Loss: 0.0569\n",
      "Epoch [168/500], Step [10/22], Loss: 0.1458\n",
      "Epoch [168/500], Step [20/22], Loss: 0.0812\n",
      "Epoch [169/500], Step [10/22], Loss: 0.1597\n",
      "Epoch [169/500], Step [20/22], Loss: 0.0297\n",
      "Epoch [170/500], Step [10/22], Loss: 0.3336\n",
      "Epoch [170/500], Step [20/22], Loss: 0.3369\n",
      "Epoch [171/500], Step [10/22], Loss: 0.5452\n",
      "Epoch [171/500], Step [20/22], Loss: 0.1732\n",
      "Epoch [172/500], Step [10/22], Loss: 0.2272\n",
      "Epoch [172/500], Step [20/22], Loss: 0.1292\n",
      "Epoch [173/500], Step [10/22], Loss: 0.2968\n",
      "Epoch [173/500], Step [20/22], Loss: 0.1576\n",
      "Epoch [174/500], Step [10/22], Loss: 0.2082\n",
      "Epoch [174/500], Step [20/22], Loss: 0.0868\n",
      "Epoch [175/500], Step [10/22], Loss: 0.1088\n",
      "Epoch [175/500], Step [20/22], Loss: 0.0796\n",
      "Epoch [176/500], Step [10/22], Loss: 0.4558\n",
      "Epoch [176/500], Step [20/22], Loss: 0.0939\n",
      "Epoch [177/500], Step [10/22], Loss: 0.1679\n",
      "Epoch [177/500], Step [20/22], Loss: 0.0752\n",
      "Epoch [178/500], Step [10/22], Loss: 0.3993\n",
      "Epoch [178/500], Step [20/22], Loss: 0.0828\n",
      "Epoch [179/500], Step [10/22], Loss: 0.1822\n",
      "Epoch [179/500], Step [20/22], Loss: 0.1052\n",
      "Epoch [180/500], Step [10/22], Loss: 0.2075\n",
      "Epoch [180/500], Step [20/22], Loss: 0.0852\n",
      "Epoch [181/500], Step [10/22], Loss: 0.1471\n",
      "Epoch [181/500], Step [20/22], Loss: 0.0962\n",
      "Epoch [182/500], Step [10/22], Loss: 0.5990\n",
      "Epoch [182/500], Step [20/22], Loss: 0.1160\n",
      "Epoch [183/500], Step [10/22], Loss: 0.3043\n",
      "Epoch [183/500], Step [20/22], Loss: 0.1210\n",
      "Epoch [184/500], Step [10/22], Loss: 0.1993\n",
      "Epoch [184/500], Step [20/22], Loss: 0.0836\n",
      "Epoch [185/500], Step [10/22], Loss: 0.1028\n",
      "Epoch [185/500], Step [20/22], Loss: 0.1106\n",
      "Epoch [186/500], Step [10/22], Loss: 0.5541\n",
      "Epoch [186/500], Step [20/22], Loss: 0.0567\n",
      "Epoch [187/500], Step [10/22], Loss: 0.1336\n",
      "Epoch [187/500], Step [20/22], Loss: 0.0737\n",
      "Epoch [188/500], Step [10/22], Loss: 0.2339\n",
      "Epoch [188/500], Step [20/22], Loss: 0.0784\n",
      "Epoch [189/500], Step [10/22], Loss: 0.3208\n",
      "Epoch [189/500], Step [20/22], Loss: 0.0454\n",
      "Epoch [190/500], Step [10/22], Loss: 0.1248\n",
      "Epoch [190/500], Step [20/22], Loss: 0.0894\n",
      "Epoch [191/500], Step [10/22], Loss: 0.3218\n",
      "Epoch [191/500], Step [20/22], Loss: 0.1347\n",
      "Epoch [192/500], Step [10/22], Loss: 0.2412\n",
      "Epoch [192/500], Step [20/22], Loss: 0.1268\n",
      "Epoch [193/500], Step [10/22], Loss: 0.3935\n",
      "Epoch [193/500], Step [20/22], Loss: 0.1091\n",
      "Epoch [194/500], Step [10/22], Loss: 0.1505\n",
      "Epoch [194/500], Step [20/22], Loss: 0.1248\n",
      "Epoch [195/500], Step [10/22], Loss: 0.2156\n",
      "Epoch [195/500], Step [20/22], Loss: 0.1022\n",
      "Epoch [196/500], Step [10/22], Loss: 0.4155\n",
      "Epoch [196/500], Step [20/22], Loss: 0.0672\n",
      "Epoch [197/500], Step [10/22], Loss: 0.1343\n",
      "Epoch [197/500], Step [20/22], Loss: 0.0598\n",
      "Epoch [198/500], Step [10/22], Loss: 0.0748\n",
      "Epoch [198/500], Step [20/22], Loss: 0.0580\n",
      "Epoch [199/500], Step [10/22], Loss: 0.6497\n",
      "Epoch [199/500], Step [20/22], Loss: 0.0844\n",
      "Epoch [200/500], Step [10/22], Loss: 0.1592\n",
      "Epoch [200/500], Step [20/22], Loss: 0.0566\n",
      "Epoch [201/500], Step [10/22], Loss: 0.3510\n",
      "Epoch [201/500], Step [20/22], Loss: 0.0484\n",
      "Epoch [202/500], Step [10/22], Loss: 0.1204\n",
      "Epoch [202/500], Step [20/22], Loss: 0.0604\n",
      "Epoch [203/500], Step [10/22], Loss: 0.4327\n",
      "Epoch [203/500], Step [20/22], Loss: 0.0678\n",
      "Epoch [204/500], Step [10/22], Loss: 0.1990\n",
      "Epoch [204/500], Step [20/22], Loss: 0.0792\n",
      "Epoch [205/500], Step [10/22], Loss: 0.3300\n",
      "Epoch [205/500], Step [20/22], Loss: 0.1251\n",
      "Epoch [206/500], Step [10/22], Loss: 0.1203\n",
      "Epoch [206/500], Step [20/22], Loss: 0.0692\n",
      "Epoch [207/500], Step [10/22], Loss: 0.3343\n",
      "Epoch [207/500], Step [20/22], Loss: 0.1122\n",
      "Epoch [208/500], Step [10/22], Loss: 0.3299\n",
      "Epoch [208/500], Step [20/22], Loss: 0.1145\n",
      "Epoch [209/500], Step [10/22], Loss: 0.1634\n",
      "Epoch [209/500], Step [20/22], Loss: 0.1346\n",
      "Epoch [210/500], Step [10/22], Loss: 0.1168\n",
      "Epoch [210/500], Step [20/22], Loss: 0.0772\n",
      "Epoch [211/500], Step [10/22], Loss: 0.3146\n",
      "Epoch [211/500], Step [20/22], Loss: 0.0731\n",
      "Epoch [212/500], Step [10/22], Loss: 0.1356\n",
      "Epoch [212/500], Step [20/22], Loss: 0.0675\n",
      "Epoch [213/500], Step [10/22], Loss: 0.1250\n",
      "Epoch [213/500], Step [20/22], Loss: 0.0673\n",
      "Epoch [214/500], Step [10/22], Loss: 0.2129\n",
      "Epoch [214/500], Step [20/22], Loss: 0.0454\n",
      "Epoch [215/500], Step [10/22], Loss: 0.2339\n",
      "Epoch [215/500], Step [20/22], Loss: 0.0322\n",
      "Epoch [216/500], Step [10/22], Loss: 0.1453\n",
      "Epoch [216/500], Step [20/22], Loss: 0.0492\n",
      "Epoch [217/500], Step [10/22], Loss: 0.1636\n",
      "Epoch [217/500], Step [20/22], Loss: 0.0788\n",
      "Epoch [218/500], Step [10/22], Loss: 0.8045\n",
      "Epoch [218/500], Step [20/22], Loss: 0.1303\n",
      "Epoch [219/500], Step [10/22], Loss: 0.3568\n",
      "Epoch [219/500], Step [20/22], Loss: 0.0939\n",
      "Epoch [220/500], Step [10/22], Loss: 0.1626\n",
      "Epoch [220/500], Step [20/22], Loss: 0.0794\n",
      "Epoch [221/500], Step [10/22], Loss: 0.1773\n",
      "Epoch [221/500], Step [20/22], Loss: 0.0556\n",
      "Epoch [222/500], Step [10/22], Loss: 0.0966\n",
      "Epoch [222/500], Step [20/22], Loss: 0.0891\n",
      "Epoch [223/500], Step [10/22], Loss: 0.2242\n",
      "Epoch [223/500], Step [20/22], Loss: 0.0279\n",
      "Epoch [224/500], Step [10/22], Loss: 0.1354\n",
      "Epoch [224/500], Step [20/22], Loss: 0.0285\n",
      "Epoch [225/500], Step [10/22], Loss: 0.1908\n",
      "Epoch [225/500], Step [20/22], Loss: 0.0205\n",
      "Epoch [226/500], Step [10/22], Loss: 0.3532\n",
      "Epoch [226/500], Step [20/22], Loss: 0.0413\n",
      "Epoch [227/500], Step [10/22], Loss: 0.1278\n",
      "Epoch [227/500], Step [20/22], Loss: 0.1291\n",
      "Epoch [228/500], Step [10/22], Loss: 0.2598\n",
      "Epoch [228/500], Step [20/22], Loss: 0.1087\n",
      "Epoch [229/500], Step [10/22], Loss: 0.1795\n",
      "Epoch [229/500], Step [20/22], Loss: 0.0345\n",
      "Epoch [230/500], Step [10/22], Loss: 0.3564\n",
      "Epoch [230/500], Step [20/22], Loss: 0.1047\n",
      "Epoch [231/500], Step [10/22], Loss: 0.1779\n",
      "Epoch [231/500], Step [20/22], Loss: 0.0868\n",
      "Epoch [232/500], Step [10/22], Loss: 0.1999\n",
      "Epoch [232/500], Step [20/22], Loss: 0.0202\n",
      "Epoch [233/500], Step [10/22], Loss: 0.1532\n",
      "Epoch [233/500], Step [20/22], Loss: 0.0414\n",
      "Epoch [234/500], Step [10/22], Loss: 0.1192\n",
      "Epoch [234/500], Step [20/22], Loss: 0.1499\n",
      "Epoch [235/500], Step [10/22], Loss: 0.5032\n",
      "Epoch [235/500], Step [20/22], Loss: 0.0966\n",
      "Epoch [236/500], Step [10/22], Loss: 0.1361\n",
      "Epoch [236/500], Step [20/22], Loss: 0.0521\n",
      "Epoch [237/500], Step [10/22], Loss: 0.1496\n",
      "Epoch [237/500], Step [20/22], Loss: 0.0832\n",
      "Epoch [238/500], Step [10/22], Loss: 0.3585\n",
      "Epoch [238/500], Step [20/22], Loss: 0.0233\n",
      "Epoch [239/500], Step [10/22], Loss: 0.0745\n",
      "Epoch [239/500], Step [20/22], Loss: 0.0673\n",
      "Epoch [240/500], Step [10/22], Loss: 0.2820\n",
      "Epoch [240/500], Step [20/22], Loss: 0.0297\n",
      "Epoch [241/500], Step [10/22], Loss: 0.2101\n",
      "Epoch [241/500], Step [20/22], Loss: 0.0806\n",
      "Epoch [242/500], Step [10/22], Loss: 0.2350\n",
      "Epoch [242/500], Step [20/22], Loss: 0.1300\n",
      "Epoch [243/500], Step [10/22], Loss: 0.0794\n",
      "Epoch [243/500], Step [20/22], Loss: 0.0724\n",
      "Epoch [244/500], Step [10/22], Loss: 0.1483\n",
      "Epoch [244/500], Step [20/22], Loss: 0.0505\n",
      "Epoch [245/500], Step [10/22], Loss: 0.3812\n",
      "Epoch [245/500], Step [20/22], Loss: 0.0336\n",
      "Epoch [246/500], Step [10/22], Loss: 0.0979\n",
      "Epoch [246/500], Step [20/22], Loss: 0.0318\n",
      "Epoch [247/500], Step [10/22], Loss: 0.0972\n",
      "Epoch [247/500], Step [20/22], Loss: 0.0710\n",
      "Epoch [248/500], Step [10/22], Loss: 0.6232\n",
      "Epoch [248/500], Step [20/22], Loss: 0.0686\n",
      "Epoch [249/500], Step [10/22], Loss: 0.4867\n",
      "Epoch [249/500], Step [20/22], Loss: 0.1131\n",
      "Epoch [250/500], Step [10/22], Loss: 0.1928\n",
      "Epoch [250/500], Step [20/22], Loss: 0.1046\n",
      "Epoch [251/500], Step [10/22], Loss: 0.0731\n",
      "Epoch [251/500], Step [20/22], Loss: 0.0543\n",
      "Epoch [252/500], Step [10/22], Loss: 0.2708\n",
      "Epoch [252/500], Step [20/22], Loss: 0.0324\n",
      "Epoch [253/500], Step [10/22], Loss: 0.3612\n",
      "Epoch [253/500], Step [20/22], Loss: 0.0359\n",
      "Epoch [254/500], Step [10/22], Loss: 0.1165\n",
      "Epoch [254/500], Step [20/22], Loss: 0.0416\n",
      "Epoch [255/500], Step [10/22], Loss: 0.4322\n",
      "Epoch [255/500], Step [20/22], Loss: 0.0212\n",
      "Epoch [256/500], Step [10/22], Loss: 0.1185\n",
      "Epoch [256/500], Step [20/22], Loss: 0.0429\n",
      "Epoch [257/500], Step [10/22], Loss: 0.2270\n",
      "Epoch [257/500], Step [20/22], Loss: 0.0293\n",
      "Epoch [258/500], Step [10/22], Loss: 0.3293\n",
      "Epoch [258/500], Step [20/22], Loss: 0.0191\n",
      "Epoch [259/500], Step [10/22], Loss: 0.0764\n",
      "Epoch [259/500], Step [20/22], Loss: 0.0334\n",
      "Epoch [260/500], Step [10/22], Loss: 0.4063\n",
      "Epoch [260/500], Step [20/22], Loss: 0.0708\n",
      "Epoch [261/500], Step [10/22], Loss: 0.1496\n",
      "Epoch [261/500], Step [20/22], Loss: 0.1163\n",
      "Epoch [262/500], Step [10/22], Loss: 0.1281\n",
      "Epoch [262/500], Step [20/22], Loss: 0.0192\n",
      "Epoch [263/500], Step [10/22], Loss: 0.2122\n",
      "Epoch [263/500], Step [20/22], Loss: 0.0339\n",
      "Epoch [264/500], Step [10/22], Loss: 0.1685\n",
      "Epoch [264/500], Step [20/22], Loss: 0.0135\n",
      "Epoch [265/500], Step [10/22], Loss: 0.1944\n",
      "Epoch [265/500], Step [20/22], Loss: 0.0124\n",
      "Epoch [266/500], Step [10/22], Loss: 0.1844\n",
      "Epoch [266/500], Step [20/22], Loss: 0.0139\n",
      "Epoch [267/500], Step [10/22], Loss: 0.2843\n",
      "Epoch [267/500], Step [20/22], Loss: 0.0135\n",
      "Epoch [268/500], Step [10/22], Loss: 0.2668\n",
      "Epoch [268/500], Step [20/22], Loss: 0.1261\n",
      "Epoch [269/500], Step [10/22], Loss: 0.1380\n",
      "Epoch [269/500], Step [20/22], Loss: 0.1376\n",
      "Epoch [270/500], Step [10/22], Loss: 0.0697\n",
      "Epoch [270/500], Step [20/22], Loss: 0.1353\n",
      "Epoch [271/500], Step [10/22], Loss: 0.6565\n",
      "Epoch [271/500], Step [20/22], Loss: 0.0464\n",
      "Epoch [272/500], Step [10/22], Loss: 0.3201\n",
      "Epoch [272/500], Step [20/22], Loss: 0.0537\n",
      "Epoch [273/500], Step [10/22], Loss: 0.3097\n",
      "Epoch [273/500], Step [20/22], Loss: 0.0990\n",
      "Epoch [274/500], Step [10/22], Loss: 0.1083\n",
      "Epoch [274/500], Step [20/22], Loss: 0.0857\n",
      "Epoch [275/500], Step [10/22], Loss: 0.0909\n",
      "Epoch [275/500], Step [20/22], Loss: 0.0591\n",
      "Epoch [276/500], Step [10/22], Loss: 0.2065\n",
      "Epoch [276/500], Step [20/22], Loss: 0.0793\n",
      "Epoch [277/500], Step [10/22], Loss: 0.4565\n",
      "Epoch [277/500], Step [20/22], Loss: 0.0372\n",
      "Epoch [278/500], Step [10/22], Loss: 0.0842\n",
      "Epoch [278/500], Step [20/22], Loss: 0.0622\n",
      "Epoch [279/500], Step [10/22], Loss: 0.2842\n",
      "Epoch [279/500], Step [20/22], Loss: 0.0345\n",
      "Epoch [280/500], Step [10/22], Loss: 0.1586\n",
      "Epoch [280/500], Step [20/22], Loss: 0.0260\n",
      "Epoch [281/500], Step [10/22], Loss: 0.0983\n",
      "Epoch [281/500], Step [20/22], Loss: 0.0337\n",
      "Epoch [282/500], Step [10/22], Loss: 0.7294\n",
      "Epoch [282/500], Step [20/22], Loss: 0.0598\n",
      "Epoch [283/500], Step [10/22], Loss: 0.1263\n",
      "Epoch [283/500], Step [20/22], Loss: 0.0599\n",
      "Epoch [284/500], Step [10/22], Loss: 0.1403\n",
      "Epoch [284/500], Step [20/22], Loss: 0.0261\n",
      "Epoch [285/500], Step [10/22], Loss: 0.1107\n",
      "Epoch [285/500], Step [20/22], Loss: 0.0306\n",
      "Epoch [286/500], Step [10/22], Loss: 0.1066\n",
      "Epoch [286/500], Step [20/22], Loss: 0.0191\n",
      "Epoch [287/500], Step [10/22], Loss: 0.1702\n",
      "Epoch [287/500], Step [20/22], Loss: 0.0225\n",
      "Epoch [288/500], Step [10/22], Loss: 0.1583\n",
      "Epoch [288/500], Step [20/22], Loss: 0.0384\n",
      "Epoch [289/500], Step [10/22], Loss: 0.1779\n",
      "Epoch [289/500], Step [20/22], Loss: 0.0369\n",
      "Epoch [290/500], Step [10/22], Loss: 0.1874\n",
      "Epoch [290/500], Step [20/22], Loss: 0.0253\n",
      "Epoch [291/500], Step [10/22], Loss: 0.1030\n",
      "Epoch [291/500], Step [20/22], Loss: 0.0486\n",
      "Epoch [292/500], Step [10/22], Loss: 0.4357\n",
      "Epoch [292/500], Step [20/22], Loss: 0.0573\n",
      "Epoch [293/500], Step [10/22], Loss: 0.1546\n",
      "Epoch [293/500], Step [20/22], Loss: 0.0838\n",
      "Epoch [294/500], Step [10/22], Loss: 0.1115\n",
      "Epoch [294/500], Step [20/22], Loss: 0.0482\n",
      "Epoch [295/500], Step [10/22], Loss: 0.0883\n",
      "Epoch [295/500], Step [20/22], Loss: 0.1751\n",
      "Epoch [296/500], Step [10/22], Loss: 0.3687\n",
      "Epoch [296/500], Step [20/22], Loss: 0.0996\n",
      "Epoch [297/500], Step [10/22], Loss: 0.0823\n",
      "Epoch [297/500], Step [20/22], Loss: 0.1061\n",
      "Epoch [298/500], Step [10/22], Loss: 0.1307\n",
      "Epoch [298/500], Step [20/22], Loss: 0.0339\n",
      "Epoch [299/500], Step [10/22], Loss: 0.3521\n",
      "Epoch [299/500], Step [20/22], Loss: 0.1104\n",
      "Epoch [300/500], Step [10/22], Loss: 0.0883\n",
      "Epoch [300/500], Step [20/22], Loss: 0.0639\n",
      "Epoch [301/500], Step [10/22], Loss: 0.1506\n",
      "Epoch [301/500], Step [20/22], Loss: 0.0687\n",
      "Epoch [302/500], Step [10/22], Loss: 0.4159\n",
      "Epoch [302/500], Step [20/22], Loss: 0.0860\n",
      "Epoch [303/500], Step [10/22], Loss: 0.0865\n",
      "Epoch [303/500], Step [20/22], Loss: 0.0689\n",
      "Epoch [304/500], Step [10/22], Loss: 0.1909\n",
      "Epoch [304/500], Step [20/22], Loss: 0.0346\n",
      "Epoch [305/500], Step [10/22], Loss: 0.0632\n",
      "Epoch [305/500], Step [20/22], Loss: 0.0362\n",
      "Epoch [306/500], Step [10/22], Loss: 0.1323\n",
      "Epoch [306/500], Step [20/22], Loss: 0.0265\n",
      "Epoch [307/500], Step [10/22], Loss: 0.3077\n",
      "Epoch [307/500], Step [20/22], Loss: 0.0399\n",
      "Epoch [308/500], Step [10/22], Loss: 0.1067\n",
      "Epoch [308/500], Step [20/22], Loss: 0.0681\n",
      "Epoch [309/500], Step [10/22], Loss: 0.5370\n",
      "Epoch [309/500], Step [20/22], Loss: 0.0511\n",
      "Epoch [310/500], Step [10/22], Loss: 0.1620\n",
      "Epoch [310/500], Step [20/22], Loss: 0.1193\n",
      "Epoch [311/500], Step [10/22], Loss: 0.1075\n",
      "Epoch [311/500], Step [20/22], Loss: 0.0759\n",
      "Epoch [312/500], Step [10/22], Loss: 0.1862\n",
      "Epoch [312/500], Step [20/22], Loss: 0.0655\n",
      "Epoch [313/500], Step [10/22], Loss: 0.1102\n",
      "Epoch [313/500], Step [20/22], Loss: 0.0471\n",
      "Epoch [314/500], Step [10/22], Loss: 0.0834\n",
      "Epoch [314/500], Step [20/22], Loss: 0.0485\n",
      "Epoch [315/500], Step [10/22], Loss: 0.4327\n",
      "Epoch [315/500], Step [20/22], Loss: 0.0919\n",
      "Epoch [316/500], Step [10/22], Loss: 0.0849\n",
      "Epoch [316/500], Step [20/22], Loss: 0.0622\n",
      "Epoch [317/500], Step [10/22], Loss: 0.1065\n",
      "Epoch [317/500], Step [20/22], Loss: 0.0587\n",
      "Epoch [318/500], Step [10/22], Loss: 0.1949\n",
      "Epoch [318/500], Step [20/22], Loss: 0.0386\n",
      "Epoch [319/500], Step [10/22], Loss: 0.1714\n",
      "Epoch [319/500], Step [20/22], Loss: 0.0219\n",
      "Epoch [320/500], Step [10/22], Loss: 0.1448\n",
      "Epoch [320/500], Step [20/22], Loss: 0.0297\n",
      "Epoch [321/500], Step [10/22], Loss: 0.0981\n",
      "Epoch [321/500], Step [20/22], Loss: 0.0461\n",
      "Epoch [322/500], Step [10/22], Loss: 0.4613\n",
      "Epoch [322/500], Step [20/22], Loss: 0.0294\n",
      "Epoch [323/500], Step [10/22], Loss: 0.0740\n",
      "Epoch [323/500], Step [20/22], Loss: 0.0368\n",
      "Epoch [324/500], Step [10/22], Loss: 0.1178\n",
      "Epoch [324/500], Step [20/22], Loss: 0.0585\n",
      "Epoch [325/500], Step [10/22], Loss: 0.2987\n",
      "Epoch [325/500], Step [20/22], Loss: 0.0742\n",
      "Epoch [326/500], Step [10/22], Loss: 0.0935\n",
      "Epoch [326/500], Step [20/22], Loss: 0.0947\n",
      "Epoch [327/500], Step [10/22], Loss: 0.1109\n",
      "Epoch [327/500], Step [20/22], Loss: 0.0908\n",
      "Epoch [328/500], Step [10/22], Loss: 0.1758\n",
      "Epoch [328/500], Step [20/22], Loss: 0.0424\n",
      "Epoch [329/500], Step [10/22], Loss: 0.0594\n",
      "Epoch [329/500], Step [20/22], Loss: 0.0475\n",
      "Epoch [330/500], Step [10/22], Loss: 0.3231\n",
      "Epoch [330/500], Step [20/22], Loss: 0.0358\n",
      "Epoch [331/500], Step [10/22], Loss: 0.1082\n",
      "Epoch [331/500], Step [20/22], Loss: 0.0221\n",
      "Epoch [332/500], Step [10/22], Loss: 0.0861\n",
      "Epoch [332/500], Step [20/22], Loss: 0.0325\n",
      "Epoch [333/500], Step [10/22], Loss: 0.4394\n",
      "Epoch [333/500], Step [20/22], Loss: 0.0440\n",
      "Epoch [334/500], Step [10/22], Loss: 0.0931\n",
      "Epoch [334/500], Step [20/22], Loss: 0.1141\n",
      "Epoch [335/500], Step [10/22], Loss: 0.1026\n",
      "Epoch [335/500], Step [20/22], Loss: 0.0495\n",
      "Epoch [336/500], Step [10/22], Loss: 0.1660\n",
      "Epoch [336/500], Step [20/22], Loss: 0.0310\n",
      "Epoch [337/500], Step [10/22], Loss: 0.1692\n",
      "Epoch [337/500], Step [20/22], Loss: 0.0712\n",
      "Epoch [338/500], Step [10/22], Loss: 0.2511\n",
      "Epoch [338/500], Step [20/22], Loss: 0.0214\n",
      "Epoch [339/500], Step [10/22], Loss: 0.1336\n",
      "Epoch [339/500], Step [20/22], Loss: 0.0366\n",
      "Epoch [340/500], Step [10/22], Loss: 0.0677\n",
      "Epoch [340/500], Step [20/22], Loss: 0.0391\n",
      "Epoch [341/500], Step [10/22], Loss: 0.2359\n",
      "Epoch [341/500], Step [20/22], Loss: 0.0317\n",
      "Epoch [342/500], Step [10/22], Loss: 0.0866\n",
      "Epoch [342/500], Step [20/22], Loss: 0.0450\n",
      "Epoch [343/500], Step [10/22], Loss: 0.0830\n",
      "Epoch [343/500], Step [20/22], Loss: 0.0943\n",
      "Epoch [344/500], Step [10/22], Loss: 0.4335\n",
      "Epoch [344/500], Step [20/22], Loss: 0.1143\n",
      "Epoch [345/500], Step [10/22], Loss: 0.3502\n",
      "Epoch [345/500], Step [20/22], Loss: 0.1751\n",
      "Epoch [346/500], Step [10/22], Loss: 0.0981\n",
      "Epoch [346/500], Step [20/22], Loss: 0.0775\n",
      "Epoch [347/500], Step [10/22], Loss: 0.0992\n",
      "Epoch [347/500], Step [20/22], Loss: 0.1294\n",
      "Epoch [348/500], Step [10/22], Loss: 0.2075\n",
      "Epoch [348/500], Step [20/22], Loss: 0.0525\n",
      "Epoch [349/500], Step [10/22], Loss: 0.0592\n",
      "Epoch [349/500], Step [20/22], Loss: 0.0550\n",
      "Epoch [350/500], Step [10/22], Loss: 0.2774\n",
      "Epoch [350/500], Step [20/22], Loss: 0.0388\n",
      "Epoch [351/500], Step [10/22], Loss: 0.0774\n",
      "Epoch [351/500], Step [20/22], Loss: 0.0388\n",
      "Epoch [352/500], Step [10/22], Loss: 0.0933\n",
      "Epoch [352/500], Step [20/22], Loss: 0.0280\n",
      "Epoch [353/500], Step [10/22], Loss: 0.1852\n",
      "Epoch [353/500], Step [20/22], Loss: 0.0265\n",
      "Epoch [354/500], Step [10/22], Loss: 0.1010\n",
      "Epoch [354/500], Step [20/22], Loss: 0.0418\n",
      "Epoch [355/500], Step [10/22], Loss: 0.1752\n",
      "Epoch [355/500], Step [20/22], Loss: 0.0417\n",
      "Epoch [356/500], Step [10/22], Loss: 0.6074\n",
      "Epoch [356/500], Step [20/22], Loss: 0.0251\n",
      "Epoch [357/500], Step [10/22], Loss: 0.0822\n",
      "Epoch [357/500], Step [20/22], Loss: 0.0438\n",
      "Epoch [358/500], Step [10/22], Loss: 0.1065\n",
      "Epoch [358/500], Step [20/22], Loss: 0.0580\n",
      "Epoch [359/500], Step [10/22], Loss: 0.1392\n",
      "Epoch [359/500], Step [20/22], Loss: 0.0477\n",
      "Epoch [360/500], Step [10/22], Loss: 0.0696\n",
      "Epoch [360/500], Step [20/22], Loss: 0.0283\n",
      "Epoch [361/500], Step [10/22], Loss: 0.0709\n",
      "Epoch [361/500], Step [20/22], Loss: 0.0342\n",
      "Epoch [362/500], Step [10/22], Loss: 0.4554\n",
      "Epoch [362/500], Step [20/22], Loss: 0.0357\n",
      "Epoch [363/500], Step [10/22], Loss: 0.0824\n",
      "Epoch [363/500], Step [20/22], Loss: 0.0392\n",
      "Epoch [364/500], Step [10/22], Loss: 0.1001\n",
      "Epoch [364/500], Step [20/22], Loss: 0.0738\n",
      "Epoch [365/500], Step [10/22], Loss: 0.1342\n",
      "Epoch [365/500], Step [20/22], Loss: 0.0585\n",
      "Epoch [366/500], Step [10/22], Loss: 0.0691\n",
      "Epoch [366/500], Step [20/22], Loss: 0.0329\n",
      "Epoch [367/500], Step [10/22], Loss: 0.0696\n",
      "Epoch [367/500], Step [20/22], Loss: 0.0690\n",
      "Epoch [368/500], Step [10/22], Loss: 0.4027\n",
      "Epoch [368/500], Step [20/22], Loss: 0.0335\n",
      "Epoch [369/500], Step [10/22], Loss: 0.0785\n",
      "Epoch [369/500], Step [20/22], Loss: 0.0619\n",
      "Epoch [370/500], Step [10/22], Loss: 0.1306\n",
      "Epoch [370/500], Step [20/22], Loss: 0.0757\n",
      "Epoch [371/500], Step [10/22], Loss: 0.2823\n",
      "Epoch [371/500], Step [20/22], Loss: 0.0329\n",
      "Epoch [372/500], Step [10/22], Loss: 0.0712\n",
      "Epoch [372/500], Step [20/22], Loss: 0.0321\n",
      "Epoch [373/500], Step [10/22], Loss: 0.2006\n",
      "Epoch [373/500], Step [20/22], Loss: 0.0232\n",
      "Epoch [374/500], Step [10/22], Loss: 0.0726\n",
      "Epoch [374/500], Step [20/22], Loss: 0.0222\n",
      "Epoch [375/500], Step [10/22], Loss: 0.0626\n",
      "Epoch [375/500], Step [20/22], Loss: 0.0180\n",
      "Epoch [376/500], Step [10/22], Loss: 0.1911\n",
      "Epoch [376/500], Step [20/22], Loss: 0.0233\n",
      "Epoch [377/500], Step [10/22], Loss: 0.0491\n",
      "Epoch [377/500], Step [20/22], Loss: 0.0285\n",
      "Epoch [378/500], Step [10/22], Loss: 0.1089\n",
      "Epoch [378/500], Step [20/22], Loss: 0.0319\n",
      "Epoch [379/500], Step [10/22], Loss: 0.4982\n",
      "Epoch [379/500], Step [20/22], Loss: 0.0162\n",
      "Epoch [380/500], Step [10/22], Loss: 0.0783\n",
      "Epoch [380/500], Step [20/22], Loss: 0.0409\n",
      "Epoch [381/500], Step [10/22], Loss: 0.1369\n",
      "Epoch [381/500], Step [20/22], Loss: 0.0493\n",
      "Epoch [382/500], Step [10/22], Loss: 0.1099\n",
      "Epoch [382/500], Step [20/22], Loss: 0.0285\n",
      "Epoch [383/500], Step [10/22], Loss: 0.0680\n",
      "Epoch [383/500], Step [20/22], Loss: 0.0245\n",
      "Epoch [384/500], Step [10/22], Loss: 0.0667\n",
      "Epoch [384/500], Step [20/22], Loss: 0.0305\n",
      "Epoch [385/500], Step [10/22], Loss: 0.2123\n",
      "Epoch [385/500], Step [20/22], Loss: 0.0330\n",
      "Epoch [386/500], Step [10/22], Loss: 0.0811\n",
      "Epoch [386/500], Step [20/22], Loss: 0.0282\n",
      "Epoch [387/500], Step [10/22], Loss: 0.0675\n",
      "Epoch [387/500], Step [20/22], Loss: 0.0368\n",
      "Epoch [388/500], Step [10/22], Loss: 0.2780\n",
      "Epoch [388/500], Step [20/22], Loss: 0.0237\n",
      "Epoch [389/500], Step [10/22], Loss: 0.0467\n",
      "Epoch [389/500], Step [20/22], Loss: 0.0186\n",
      "Epoch [390/500], Step [10/22], Loss: 0.1060\n",
      "Epoch [390/500], Step [20/22], Loss: 0.0433\n",
      "Epoch [391/500], Step [10/22], Loss: 0.1329\n",
      "Epoch [391/500], Step [20/22], Loss: 0.0210\n",
      "Epoch [392/500], Step [10/22], Loss: 0.0974\n",
      "Epoch [392/500], Step [20/22], Loss: 0.0216\n",
      "Epoch [393/500], Step [10/22], Loss: 0.0578\n",
      "Epoch [393/500], Step [20/22], Loss: 0.0424\n",
      "Epoch [394/500], Step [10/22], Loss: 0.0964\n",
      "Epoch [394/500], Step [20/22], Loss: 0.0179\n",
      "Epoch [395/500], Step [10/22], Loss: 0.0600\n",
      "Epoch [395/500], Step [20/22], Loss: 0.0084\n",
      "Epoch [396/500], Step [10/22], Loss: 0.0659\n",
      "Epoch [396/500], Step [20/22], Loss: 0.0135\n",
      "Epoch [397/500], Step [10/22], Loss: 0.1885\n",
      "Epoch [397/500], Step [20/22], Loss: 0.0346\n",
      "Epoch [398/500], Step [10/22], Loss: 0.0457\n",
      "Epoch [398/500], Step [20/22], Loss: 0.0664\n",
      "Epoch [399/500], Step [10/22], Loss: 0.1629\n",
      "Epoch [399/500], Step [20/22], Loss: 0.0661\n",
      "Epoch [400/500], Step [10/22], Loss: 0.3117\n",
      "Epoch [400/500], Step [20/22], Loss: 0.0420\n",
      "Epoch [401/500], Step [10/22], Loss: 0.0901\n",
      "Epoch [401/500], Step [20/22], Loss: 0.0307\n",
      "Epoch [402/500], Step [10/22], Loss: 0.1009\n",
      "Epoch [402/500], Step [20/22], Loss: 0.0449\n",
      "Epoch [403/500], Step [10/22], Loss: 0.2807\n",
      "Epoch [403/500], Step [20/22], Loss: 0.0225\n",
      "Epoch [404/500], Step [10/22], Loss: 0.1047\n",
      "Epoch [404/500], Step [20/22], Loss: 0.0407\n",
      "Epoch [405/500], Step [10/22], Loss: 0.1868\n",
      "Epoch [405/500], Step [20/22], Loss: 0.0423\n",
      "Epoch [406/500], Step [10/22], Loss: 0.1777\n",
      "Epoch [406/500], Step [20/22], Loss: 0.0271\n",
      "Epoch [407/500], Step [10/22], Loss: 0.0801\n",
      "Epoch [407/500], Step [20/22], Loss: 0.0183\n",
      "Epoch [408/500], Step [10/22], Loss: 0.0660\n",
      "Epoch [408/500], Step [20/22], Loss: 0.0145\n",
      "Epoch [409/500], Step [10/22], Loss: 0.1658\n",
      "Epoch [409/500], Step [20/22], Loss: 0.0135\n",
      "Epoch [410/500], Step [10/22], Loss: 0.0933\n",
      "Epoch [410/500], Step [20/22], Loss: 0.0092\n",
      "Epoch [411/500], Step [10/22], Loss: 0.0788\n",
      "Epoch [411/500], Step [20/22], Loss: 0.0156\n",
      "Epoch [412/500], Step [10/22], Loss: 0.2894\n",
      "Epoch [412/500], Step [20/22], Loss: 0.0188\n",
      "Epoch [413/500], Step [10/22], Loss: 0.0840\n",
      "Epoch [413/500], Step [20/22], Loss: 0.0136\n",
      "Epoch [414/500], Step [10/22], Loss: 0.0462\n",
      "Epoch [414/500], Step [20/22], Loss: 0.0393\n",
      "Epoch [415/500], Step [10/22], Loss: 0.2869\n",
      "Epoch [415/500], Step [20/22], Loss: 0.0589\n",
      "Epoch [416/500], Step [10/22], Loss: 0.1222\n",
      "Epoch [416/500], Step [20/22], Loss: 0.0098\n",
      "Epoch [417/500], Step [10/22], Loss: 0.0531\n",
      "Epoch [417/500], Step [20/22], Loss: 0.0254\n",
      "Epoch [418/500], Step [10/22], Loss: 0.1316\n",
      "Epoch [418/500], Step [20/22], Loss: 0.0139\n",
      "Epoch [419/500], Step [10/22], Loss: 0.0669\n",
      "Epoch [419/500], Step [20/22], Loss: 0.0089\n",
      "Epoch [420/500], Step [10/22], Loss: 0.0804\n",
      "Epoch [420/500], Step [20/22], Loss: 0.0123\n",
      "Epoch [421/500], Step [10/22], Loss: 0.2080\n",
      "Epoch [421/500], Step [20/22], Loss: 0.0067\n",
      "Epoch [422/500], Step [10/22], Loss: 0.0546\n",
      "Epoch [422/500], Step [20/22], Loss: 0.0217\n",
      "Epoch [423/500], Step [10/22], Loss: 0.1786\n",
      "Epoch [423/500], Step [20/22], Loss: 0.0375\n",
      "Epoch [424/500], Step [10/22], Loss: 0.1849\n",
      "Epoch [424/500], Step [20/22], Loss: 0.0410\n",
      "Epoch [425/500], Step [10/22], Loss: 0.0473\n",
      "Epoch [425/500], Step [20/22], Loss: 0.0805\n",
      "Epoch [426/500], Step [10/22], Loss: 0.3291\n",
      "Epoch [426/500], Step [20/22], Loss: 0.1739\n",
      "Epoch [427/500], Step [10/22], Loss: 0.0785\n",
      "Epoch [427/500], Step [20/22], Loss: 0.0428\n",
      "Epoch [428/500], Step [10/22], Loss: 0.0690\n",
      "Epoch [428/500], Step [20/22], Loss: 0.0174\n",
      "Epoch [429/500], Step [10/22], Loss: 0.1248\n",
      "Epoch [429/500], Step [20/22], Loss: 0.0124\n",
      "Epoch [430/500], Step [10/22], Loss: 0.0448\n",
      "Epoch [430/500], Step [20/22], Loss: 0.0140\n",
      "Epoch [431/500], Step [10/22], Loss: 0.1400\n",
      "Epoch [431/500], Step [20/22], Loss: 0.0121\n",
      "Epoch [432/500], Step [10/22], Loss: 0.0542\n",
      "Epoch [432/500], Step [20/22], Loss: 0.0124\n",
      "Epoch [433/500], Step [10/22], Loss: 0.0500\n",
      "Epoch [433/500], Step [20/22], Loss: 0.0143\n",
      "Epoch [434/500], Step [10/22], Loss: 0.2131\n",
      "Epoch [434/500], Step [20/22], Loss: 0.0041\n",
      "Epoch [435/500], Step [10/22], Loss: 0.0404\n",
      "Epoch [435/500], Step [20/22], Loss: 0.0241\n",
      "Epoch [436/500], Step [10/22], Loss: 0.0861\n",
      "Epoch [436/500], Step [20/22], Loss: 0.0298\n",
      "Epoch [437/500], Step [10/22], Loss: 0.3495\n",
      "Epoch [437/500], Step [20/22], Loss: 0.0119\n",
      "Epoch [438/500], Step [10/22], Loss: 0.0406\n",
      "Epoch [438/500], Step [20/22], Loss: 0.0411\n",
      "Epoch [439/500], Step [10/22], Loss: 0.1687\n",
      "Epoch [439/500], Step [20/22], Loss: 0.0386\n",
      "Epoch [440/500], Step [10/22], Loss: 0.0709\n",
      "Epoch [440/500], Step [20/22], Loss: 0.0093\n",
      "Epoch [441/500], Step [10/22], Loss: 0.0550\n",
      "Epoch [441/500], Step [20/22], Loss: 0.0217\n",
      "Epoch [442/500], Step [10/22], Loss: 0.0566\n",
      "Epoch [442/500], Step [20/22], Loss: 0.1443\n",
      "Epoch [443/500], Step [10/22], Loss: 0.1607\n",
      "Epoch [443/500], Step [20/22], Loss: 0.0485\n",
      "Epoch [444/500], Step [10/22], Loss: 0.0881\n",
      "Epoch [444/500], Step [20/22], Loss: 0.0116\n",
      "Epoch [445/500], Step [10/22], Loss: 0.0472\n",
      "Epoch [445/500], Step [20/22], Loss: 0.0118\n",
      "Epoch [446/500], Step [10/22], Loss: 0.2088\n",
      "Epoch [446/500], Step [20/22], Loss: 0.0165\n",
      "Epoch [447/500], Step [10/22], Loss: 0.0516\n",
      "Epoch [447/500], Step [20/22], Loss: 0.0053\n",
      "Epoch [448/500], Step [10/22], Loss: 0.0699\n",
      "Epoch [448/500], Step [20/22], Loss: 0.0098\n",
      "Epoch [449/500], Step [10/22], Loss: 0.1558\n",
      "Epoch [449/500], Step [20/22], Loss: 0.0048\n",
      "Epoch [450/500], Step [10/22], Loss: 0.0719\n",
      "Epoch [450/500], Step [20/22], Loss: 0.0869\n",
      "Epoch [451/500], Step [10/22], Loss: 0.0985\n",
      "Epoch [451/500], Step [20/22], Loss: 0.0687\n",
      "Epoch [452/500], Step [10/22], Loss: 0.2345\n",
      "Epoch [452/500], Step [20/22], Loss: 0.0189\n",
      "Epoch [453/500], Step [10/22], Loss: 0.0630\n",
      "Epoch [453/500], Step [20/22], Loss: 0.0414\n",
      "Epoch [454/500], Step [10/22], Loss: 0.1107\n",
      "Epoch [454/500], Step [20/22], Loss: 0.0072\n",
      "Epoch [455/500], Step [10/22], Loss: 0.0961\n",
      "Epoch [455/500], Step [20/22], Loss: 0.0052\n",
      "Epoch [456/500], Step [10/22], Loss: 0.0688\n",
      "Epoch [456/500], Step [20/22], Loss: 0.0049\n",
      "Epoch [457/500], Step [10/22], Loss: 0.1014\n",
      "Epoch [457/500], Step [20/22], Loss: 0.0068\n",
      "Epoch [458/500], Step [10/22], Loss: 0.1167\n",
      "Epoch [458/500], Step [20/22], Loss: 0.0058\n",
      "Epoch [459/500], Step [10/22], Loss: 0.0433\n",
      "Epoch [459/500], Step [20/22], Loss: 0.0058\n",
      "Epoch [460/500], Step [10/22], Loss: 0.0954\n",
      "Epoch [460/500], Step [20/22], Loss: 0.0089\n",
      "Epoch [461/500], Step [10/22], Loss: 0.0438\n",
      "Epoch [461/500], Step [20/22], Loss: 0.0933\n",
      "Epoch [462/500], Step [10/22], Loss: 0.1892\n",
      "Epoch [462/500], Step [20/22], Loss: 0.0387\n",
      "Epoch [463/500], Step [10/22], Loss: 0.0355\n",
      "Epoch [463/500], Step [20/22], Loss: 0.0362\n",
      "Epoch [464/500], Step [10/22], Loss: 0.0518\n",
      "Epoch [464/500], Step [20/22], Loss: 0.0144\n",
      "Epoch [465/500], Step [10/22], Loss: 0.0342\n",
      "Epoch [465/500], Step [20/22], Loss: 0.0119\n",
      "Epoch [466/500], Step [10/22], Loss: 0.0367\n",
      "Epoch [466/500], Step [20/22], Loss: 0.0096\n",
      "Epoch [467/500], Step [10/22], Loss: 0.0460\n",
      "Epoch [467/500], Step [20/22], Loss: 0.0162\n",
      "Epoch [468/500], Step [10/22], Loss: 0.0272\n",
      "Epoch [468/500], Step [20/22], Loss: 0.0075\n",
      "Epoch [469/500], Step [10/22], Loss: 0.1197\n",
      "Epoch [469/500], Step [20/22], Loss: 0.0150\n",
      "Epoch [470/500], Step [10/22], Loss: 0.2258\n",
      "Epoch [470/500], Step [20/22], Loss: 0.0113\n",
      "Epoch [471/500], Step [10/22], Loss: 0.1244\n",
      "Epoch [471/500], Step [20/22], Loss: 0.0803\n",
      "Epoch [472/500], Step [10/22], Loss: 0.1098\n",
      "Epoch [472/500], Step [20/22], Loss: 0.0214\n",
      "Epoch [473/500], Step [10/22], Loss: 0.2335\n",
      "Epoch [473/500], Step [20/22], Loss: 0.0521\n",
      "Epoch [474/500], Step [10/22], Loss: 0.2316\n",
      "Epoch [474/500], Step [20/22], Loss: 0.2125\n",
      "Epoch [475/500], Step [10/22], Loss: 0.0598\n",
      "Epoch [475/500], Step [20/22], Loss: 0.0933\n",
      "Epoch [476/500], Step [10/22], Loss: 0.7822\n",
      "Epoch [476/500], Step [20/22], Loss: 0.0437\n",
      "Epoch [477/500], Step [10/22], Loss: 0.1315\n",
      "Epoch [477/500], Step [20/22], Loss: 0.0262\n",
      "Epoch [478/500], Step [10/22], Loss: 0.1577\n",
      "Epoch [478/500], Step [20/22], Loss: 0.0282\n",
      "Epoch [479/500], Step [10/22], Loss: 0.1148\n",
      "Epoch [479/500], Step [20/22], Loss: 0.0081\n",
      "Epoch [480/500], Step [10/22], Loss: 0.0720\n",
      "Epoch [480/500], Step [20/22], Loss: 0.0106\n",
      "Epoch [481/500], Step [10/22], Loss: 0.0974\n",
      "Epoch [481/500], Step [20/22], Loss: 0.0074\n",
      "Epoch [482/500], Step [10/22], Loss: 0.0534\n",
      "Epoch [482/500], Step [20/22], Loss: 0.0055\n",
      "Epoch [483/500], Step [10/22], Loss: 0.0578\n",
      "Epoch [483/500], Step [20/22], Loss: 0.0058\n",
      "Epoch [484/500], Step [10/22], Loss: 0.0537\n",
      "Epoch [484/500], Step [20/22], Loss: 0.0039\n",
      "Epoch [485/500], Step [10/22], Loss: 0.0546\n",
      "Epoch [485/500], Step [20/22], Loss: 0.0043\n",
      "Epoch [486/500], Step [10/22], Loss: 0.0491\n",
      "Epoch [486/500], Step [20/22], Loss: 0.0053\n",
      "Epoch [487/500], Step [10/22], Loss: 0.0422\n",
      "Epoch [487/500], Step [20/22], Loss: 0.0038\n",
      "Epoch [488/500], Step [10/22], Loss: 0.0559\n",
      "Epoch [488/500], Step [20/22], Loss: 0.0095\n",
      "Epoch [489/500], Step [10/22], Loss: 0.0384\n",
      "Epoch [489/500], Step [20/22], Loss: 0.0134\n",
      "Epoch [490/500], Step [10/22], Loss: 0.0572\n",
      "Epoch [490/500], Step [20/22], Loss: 0.0044\n",
      "Epoch [491/500], Step [10/22], Loss: 0.0493\n",
      "Epoch [491/500], Step [20/22], Loss: 0.0236\n",
      "Epoch [492/500], Step [10/22], Loss: 0.2757\n",
      "Epoch [492/500], Step [20/22], Loss: 0.0335\n",
      "Epoch [493/500], Step [10/22], Loss: 0.1662\n",
      "Epoch [493/500], Step [20/22], Loss: 0.0102\n",
      "Epoch [494/500], Step [10/22], Loss: 0.0782\n",
      "Epoch [494/500], Step [20/22], Loss: 0.0159\n",
      "Epoch [495/500], Step [10/22], Loss: 0.2623\n",
      "Epoch [495/500], Step [20/22], Loss: 0.0085\n",
      "Epoch [496/500], Step [10/22], Loss: 0.0597\n",
      "Epoch [496/500], Step [20/22], Loss: 0.0032\n",
      "Epoch [497/500], Step [10/22], Loss: 0.0905\n",
      "Epoch [497/500], Step [20/22], Loss: 0.0047\n",
      "Epoch [498/500], Step [10/22], Loss: 0.0496\n",
      "Epoch [498/500], Step [20/22], Loss: 0.0022\n",
      "Epoch [499/500], Step [10/22], Loss: 0.0479\n",
      "Epoch [499/500], Step [20/22], Loss: 0.0038\n",
      "Epoch [500/500], Step [10/22], Loss: 0.0446\n",
      "Epoch [500/500], Step [20/22], Loss: 0.0019\n",
      "Epoch [1/500], Step [10/22], Loss: 0.6891\n",
      "Epoch [1/500], Step [20/22], Loss: 0.7025\n",
      "Epoch [2/500], Step [10/22], Loss: 0.7036\n",
      "Epoch [2/500], Step [20/22], Loss: 0.6106\n",
      "Epoch [3/500], Step [10/22], Loss: 0.7346\n",
      "Epoch [3/500], Step [20/22], Loss: 0.4883\n",
      "Epoch [4/500], Step [10/22], Loss: 0.7402\n",
      "Epoch [4/500], Step [20/22], Loss: 0.3841\n",
      "Epoch [5/500], Step [10/22], Loss: 0.7036\n",
      "Epoch [5/500], Step [20/22], Loss: 0.3043\n",
      "Epoch [6/500], Step [10/22], Loss: 0.6878\n",
      "Epoch [6/500], Step [20/22], Loss: 0.2329\n",
      "Epoch [7/500], Step [10/22], Loss: 0.6753\n",
      "Epoch [7/500], Step [20/22], Loss: 0.2504\n",
      "Epoch [8/500], Step [10/22], Loss: 0.6756\n",
      "Epoch [8/500], Step [20/22], Loss: 0.1774\n",
      "Epoch [9/500], Step [10/22], Loss: 0.6667\n",
      "Epoch [9/500], Step [20/22], Loss: 0.4668\n",
      "Epoch [10/500], Step [10/22], Loss: 0.6763\n",
      "Epoch [10/500], Step [20/22], Loss: 0.2364\n",
      "Epoch [11/500], Step [10/22], Loss: 0.6748\n",
      "Epoch [11/500], Step [20/22], Loss: 0.2246\n",
      "Epoch [12/500], Step [10/22], Loss: 0.6670\n",
      "Epoch [12/500], Step [20/22], Loss: 0.2045\n",
      "Epoch [13/500], Step [10/22], Loss: 0.6579\n",
      "Epoch [13/500], Step [20/22], Loss: 0.1752\n",
      "Epoch [14/500], Step [10/22], Loss: 0.6441\n",
      "Epoch [14/500], Step [20/22], Loss: 0.1662\n",
      "Epoch [15/500], Step [10/22], Loss: 0.6323\n",
      "Epoch [15/500], Step [20/22], Loss: 0.1550\n",
      "Epoch [16/500], Step [10/22], Loss: 0.6387\n",
      "Epoch [16/500], Step [20/22], Loss: 0.1594\n",
      "Epoch [17/500], Step [10/22], Loss: 0.6268\n",
      "Epoch [17/500], Step [20/22], Loss: 0.2679\n",
      "Epoch [18/500], Step [10/22], Loss: 0.6198\n",
      "Epoch [18/500], Step [20/22], Loss: 0.1541\n",
      "Epoch [19/500], Step [10/22], Loss: 0.6332\n",
      "Epoch [19/500], Step [20/22], Loss: 0.2686\n",
      "Epoch [20/500], Step [10/22], Loss: 0.7174\n",
      "Epoch [20/500], Step [20/22], Loss: 0.1623\n",
      "Epoch [21/500], Step [10/22], Loss: 0.6576\n",
      "Epoch [21/500], Step [20/22], Loss: 0.1757\n",
      "Epoch [22/500], Step [10/22], Loss: 0.6412\n",
      "Epoch [22/500], Step [20/22], Loss: 0.1444\n",
      "Epoch [23/500], Step [10/22], Loss: 0.6274\n",
      "Epoch [23/500], Step [20/22], Loss: 0.1772\n",
      "Epoch [24/500], Step [10/22], Loss: 0.6646\n",
      "Epoch [24/500], Step [20/22], Loss: 0.1478\n",
      "Epoch [25/500], Step [10/22], Loss: 0.6299\n",
      "Epoch [25/500], Step [20/22], Loss: 0.4056\n",
      "Epoch [26/500], Step [10/22], Loss: 0.6267\n",
      "Epoch [26/500], Step [20/22], Loss: 0.2324\n",
      "Epoch [27/500], Step [10/22], Loss: 0.6233\n",
      "Epoch [27/500], Step [20/22], Loss: 0.1814\n",
      "Epoch [28/500], Step [10/22], Loss: 0.6397\n",
      "Epoch [28/500], Step [20/22], Loss: 0.1747\n",
      "Epoch [29/500], Step [10/22], Loss: 0.6279\n",
      "Epoch [29/500], Step [20/22], Loss: 0.1589\n",
      "Epoch [30/500], Step [10/22], Loss: 0.6580\n",
      "Epoch [30/500], Step [20/22], Loss: 0.1498\n",
      "Epoch [31/500], Step [10/22], Loss: 0.6108\n",
      "Epoch [31/500], Step [20/22], Loss: 0.1448\n",
      "Epoch [32/500], Step [10/22], Loss: 0.6503\n",
      "Epoch [32/500], Step [20/22], Loss: 0.1417\n",
      "Epoch [33/500], Step [10/22], Loss: 0.6089\n",
      "Epoch [33/500], Step [20/22], Loss: 0.1424\n",
      "Epoch [34/500], Step [10/22], Loss: 0.6021\n",
      "Epoch [34/500], Step [20/22], Loss: 0.1398\n",
      "Epoch [35/500], Step [10/22], Loss: 0.7070\n",
      "Epoch [35/500], Step [20/22], Loss: 0.1437\n",
      "Epoch [36/500], Step [10/22], Loss: 0.6488\n",
      "Epoch [36/500], Step [20/22], Loss: 0.1401\n",
      "Epoch [37/500], Step [10/22], Loss: 0.6166\n",
      "Epoch [37/500], Step [20/22], Loss: 0.1401\n",
      "Epoch [38/500], Step [10/22], Loss: 0.6154\n",
      "Epoch [38/500], Step [20/22], Loss: 0.1402\n",
      "Epoch [39/500], Step [10/22], Loss: 0.6116\n",
      "Epoch [39/500], Step [20/22], Loss: 0.1379\n",
      "Epoch [40/500], Step [10/22], Loss: 0.5951\n",
      "Epoch [40/500], Step [20/22], Loss: 0.1395\n",
      "Epoch [41/500], Step [10/22], Loss: 0.6282\n",
      "Epoch [41/500], Step [20/22], Loss: 0.1359\n",
      "Epoch [42/500], Step [10/22], Loss: 0.5661\n",
      "Epoch [42/500], Step [20/22], Loss: 0.1432\n",
      "Epoch [43/500], Step [10/22], Loss: 0.6630\n",
      "Epoch [43/500], Step [20/22], Loss: 0.1392\n",
      "Epoch [44/500], Step [10/22], Loss: 0.5959\n",
      "Epoch [44/500], Step [20/22], Loss: 0.1292\n",
      "Epoch [45/500], Step [10/22], Loss: 0.5618\n",
      "Epoch [45/500], Step [20/22], Loss: 0.1290\n",
      "Epoch [46/500], Step [10/22], Loss: 0.6502\n",
      "Epoch [46/500], Step [20/22], Loss: 0.1349\n",
      "Epoch [47/500], Step [10/22], Loss: 0.5561\n",
      "Epoch [47/500], Step [20/22], Loss: 0.1568\n",
      "Epoch [48/500], Step [10/22], Loss: 0.6491\n",
      "Epoch [48/500], Step [20/22], Loss: 0.1449\n",
      "Epoch [49/500], Step [10/22], Loss: 0.5506\n",
      "Epoch [49/500], Step [20/22], Loss: 0.1393\n",
      "Epoch [50/500], Step [10/22], Loss: 0.4929\n",
      "Epoch [50/500], Step [20/22], Loss: 0.1157\n",
      "Epoch [51/500], Step [10/22], Loss: 0.5590\n",
      "Epoch [51/500], Step [20/22], Loss: 0.1347\n",
      "Epoch [52/500], Step [10/22], Loss: 0.5716\n",
      "Epoch [52/500], Step [20/22], Loss: 0.3403\n",
      "Epoch [53/500], Step [10/22], Loss: 0.5408\n",
      "Epoch [53/500], Step [20/22], Loss: 0.1593\n",
      "Epoch [54/500], Step [10/22], Loss: 0.7163\n",
      "Epoch [54/500], Step [20/22], Loss: 0.1508\n",
      "Epoch [55/500], Step [10/22], Loss: 0.5190\n",
      "Epoch [55/500], Step [20/22], Loss: 0.1370\n",
      "Epoch [56/500], Step [10/22], Loss: 0.5579\n",
      "Epoch [56/500], Step [20/22], Loss: 0.1345\n",
      "Epoch [57/500], Step [10/22], Loss: 0.5479\n",
      "Epoch [57/500], Step [20/22], Loss: 0.1332\n",
      "Epoch [58/500], Step [10/22], Loss: 0.4878\n",
      "Epoch [58/500], Step [20/22], Loss: 0.1285\n",
      "Epoch [59/500], Step [10/22], Loss: 0.5458\n",
      "Epoch [59/500], Step [20/22], Loss: 0.1330\n",
      "Epoch [60/500], Step [10/22], Loss: 0.6044\n",
      "Epoch [60/500], Step [20/22], Loss: 0.1321\n",
      "Epoch [61/500], Step [10/22], Loss: 0.6036\n",
      "Epoch [61/500], Step [20/22], Loss: 0.1484\n",
      "Epoch [62/500], Step [10/22], Loss: 0.5054\n",
      "Epoch [62/500], Step [20/22], Loss: 0.1365\n",
      "Epoch [63/500], Step [10/22], Loss: 0.4997\n",
      "Epoch [63/500], Step [20/22], Loss: 0.1385\n",
      "Epoch [64/500], Step [10/22], Loss: 0.4867\n",
      "Epoch [64/500], Step [20/22], Loss: 0.1461\n",
      "Epoch [65/500], Step [10/22], Loss: 0.5147\n",
      "Epoch [65/500], Step [20/22], Loss: 0.1369\n",
      "Epoch [66/500], Step [10/22], Loss: 0.5213\n",
      "Epoch [66/500], Step [20/22], Loss: 0.1334\n",
      "Epoch [67/500], Step [10/22], Loss: 0.4303\n",
      "Epoch [67/500], Step [20/22], Loss: 0.1378\n",
      "Epoch [68/500], Step [10/22], Loss: 0.4012\n",
      "Epoch [68/500], Step [20/22], Loss: 0.1419\n",
      "Epoch [69/500], Step [10/22], Loss: 0.6757\n",
      "Epoch [69/500], Step [20/22], Loss: 0.1272\n",
      "Epoch [70/500], Step [10/22], Loss: 0.5850\n",
      "Epoch [70/500], Step [20/22], Loss: 0.1238\n",
      "Epoch [71/500], Step [10/22], Loss: 0.4732\n",
      "Epoch [71/500], Step [20/22], Loss: 0.1411\n",
      "Epoch [72/500], Step [10/22], Loss: 0.3984\n",
      "Epoch [72/500], Step [20/22], Loss: 0.1353\n",
      "Epoch [73/500], Step [10/22], Loss: 0.4974\n",
      "Epoch [73/500], Step [20/22], Loss: 0.1399\n",
      "Epoch [74/500], Step [10/22], Loss: 0.4002\n",
      "Epoch [74/500], Step [20/22], Loss: 0.1415\n",
      "Epoch [75/500], Step [10/22], Loss: 0.4008\n",
      "Epoch [75/500], Step [20/22], Loss: 0.1412\n",
      "Epoch [76/500], Step [10/22], Loss: 0.5021\n",
      "Epoch [76/500], Step [20/22], Loss: 0.1283\n",
      "Epoch [77/500], Step [10/22], Loss: 0.4497\n",
      "Epoch [77/500], Step [20/22], Loss: 0.1280\n",
      "Epoch [78/500], Step [10/22], Loss: 0.3542\n",
      "Epoch [78/500], Step [20/22], Loss: 0.1373\n",
      "Epoch [79/500], Step [10/22], Loss: 0.3101\n",
      "Epoch [79/500], Step [20/22], Loss: 0.1430\n",
      "Epoch [80/500], Step [10/22], Loss: 0.3190\n",
      "Epoch [80/500], Step [20/22], Loss: 0.1442\n",
      "Epoch [81/500], Step [10/22], Loss: 0.5476\n",
      "Epoch [81/500], Step [20/22], Loss: 0.1158\n",
      "Epoch [82/500], Step [10/22], Loss: 0.4755\n",
      "Epoch [82/500], Step [20/22], Loss: 0.1301\n",
      "Epoch [83/500], Step [10/22], Loss: 0.4424\n",
      "Epoch [83/500], Step [20/22], Loss: 0.1347\n",
      "Epoch [84/500], Step [10/22], Loss: 0.4101\n",
      "Epoch [84/500], Step [20/22], Loss: 0.1393\n",
      "Epoch [85/500], Step [10/22], Loss: 0.3622\n",
      "Epoch [85/500], Step [20/22], Loss: 0.1481\n",
      "Epoch [86/500], Step [10/22], Loss: 0.3191\n",
      "Epoch [86/500], Step [20/22], Loss: 0.1494\n",
      "Epoch [87/500], Step [10/22], Loss: 0.2785\n",
      "Epoch [87/500], Step [20/22], Loss: 0.1450\n",
      "Epoch [88/500], Step [10/22], Loss: 0.2636\n",
      "Epoch [88/500], Step [20/22], Loss: 0.1293\n",
      "Epoch [89/500], Step [10/22], Loss: 0.2486\n",
      "Epoch [89/500], Step [20/22], Loss: 0.1317\n",
      "Epoch [90/500], Step [10/22], Loss: 0.3310\n",
      "Epoch [90/500], Step [20/22], Loss: 0.1290\n",
      "Epoch [91/500], Step [10/22], Loss: 0.2851\n",
      "Epoch [91/500], Step [20/22], Loss: 0.1236\n",
      "Epoch [92/500], Step [10/22], Loss: 0.3228\n",
      "Epoch [92/500], Step [20/22], Loss: 0.1226\n",
      "Epoch [93/500], Step [10/22], Loss: 0.2252\n",
      "Epoch [93/500], Step [20/22], Loss: 0.1219\n",
      "Epoch [94/500], Step [10/22], Loss: 0.3376\n",
      "Epoch [94/500], Step [20/22], Loss: 0.1329\n",
      "Epoch [95/500], Step [10/22], Loss: 0.3421\n",
      "Epoch [95/500], Step [20/22], Loss: 0.1183\n",
      "Epoch [96/500], Step [10/22], Loss: 0.4764\n",
      "Epoch [96/500], Step [20/22], Loss: 0.1226\n",
      "Epoch [97/500], Step [10/22], Loss: 0.4157\n",
      "Epoch [97/500], Step [20/22], Loss: 0.1700\n",
      "Epoch [98/500], Step [10/22], Loss: 0.3819\n",
      "Epoch [98/500], Step [20/22], Loss: 0.1482\n",
      "Epoch [99/500], Step [10/22], Loss: 0.3148\n",
      "Epoch [99/500], Step [20/22], Loss: 0.1227\n",
      "Epoch [100/500], Step [10/22], Loss: 0.2723\n",
      "Epoch [100/500], Step [20/22], Loss: 0.1229\n",
      "Epoch [101/500], Step [10/22], Loss: 0.2621\n",
      "Epoch [101/500], Step [20/22], Loss: 0.1241\n",
      "Epoch [102/500], Step [10/22], Loss: 0.2587\n",
      "Epoch [102/500], Step [20/22], Loss: 0.1221\n",
      "Epoch [103/500], Step [10/22], Loss: 0.3087\n",
      "Epoch [103/500], Step [20/22], Loss: 0.1304\n",
      "Epoch [104/500], Step [10/22], Loss: 0.3244\n",
      "Epoch [104/500], Step [20/22], Loss: 0.1231\n",
      "Epoch [105/500], Step [10/22], Loss: 0.2892\n",
      "Epoch [105/500], Step [20/22], Loss: 0.1281\n",
      "Epoch [106/500], Step [10/22], Loss: 0.2455\n",
      "Epoch [106/500], Step [20/22], Loss: 0.1224\n",
      "Epoch [107/500], Step [10/22], Loss: 0.3106\n",
      "Epoch [107/500], Step [20/22], Loss: 0.1288\n",
      "Epoch [108/500], Step [10/22], Loss: 0.3367\n",
      "Epoch [108/500], Step [20/22], Loss: 0.1349\n",
      "Epoch [109/500], Step [10/22], Loss: 0.3735\n",
      "Epoch [109/500], Step [20/22], Loss: 0.1302\n",
      "Epoch [110/500], Step [10/22], Loss: 0.3074\n",
      "Epoch [110/500], Step [20/22], Loss: 0.1343\n",
      "Epoch [111/500], Step [10/22], Loss: 0.2521\n",
      "Epoch [111/500], Step [20/22], Loss: 0.1234\n",
      "Epoch [112/500], Step [10/22], Loss: 0.2704\n",
      "Epoch [112/500], Step [20/22], Loss: 0.1374\n",
      "Epoch [113/500], Step [10/22], Loss: 0.3570\n",
      "Epoch [113/500], Step [20/22], Loss: 0.1120\n",
      "Epoch [114/500], Step [10/22], Loss: 0.3248\n",
      "Epoch [114/500], Step [20/22], Loss: 0.1145\n",
      "Epoch [115/500], Step [10/22], Loss: 0.3462\n",
      "Epoch [115/500], Step [20/22], Loss: 0.1199\n",
      "Epoch [116/500], Step [10/22], Loss: 0.3191\n",
      "Epoch [116/500], Step [20/22], Loss: 0.1286\n",
      "Epoch [117/500], Step [10/22], Loss: 0.2553\n",
      "Epoch [117/500], Step [20/22], Loss: 0.1294\n",
      "Epoch [118/500], Step [10/22], Loss: 0.2211\n",
      "Epoch [118/500], Step [20/22], Loss: 0.1321\n",
      "Epoch [119/500], Step [10/22], Loss: 0.2608\n",
      "Epoch [119/500], Step [20/22], Loss: 0.1226\n",
      "Epoch [120/500], Step [10/22], Loss: 0.3949\n",
      "Epoch [120/500], Step [20/22], Loss: 0.1188\n",
      "Epoch [121/500], Step [10/22], Loss: 0.2933\n",
      "Epoch [121/500], Step [20/22], Loss: 0.1166\n",
      "Epoch [122/500], Step [10/22], Loss: 0.2639\n",
      "Epoch [122/500], Step [20/22], Loss: 0.1223\n",
      "Epoch [123/500], Step [10/22], Loss: 0.2472\n",
      "Epoch [123/500], Step [20/22], Loss: 0.1205\n",
      "Epoch [124/500], Step [10/22], Loss: 0.3220\n",
      "Epoch [124/500], Step [20/22], Loss: 0.1458\n",
      "Epoch [125/500], Step [10/22], Loss: 0.2610\n",
      "Epoch [125/500], Step [20/22], Loss: 0.1311\n",
      "Epoch [126/500], Step [10/22], Loss: 0.2438\n",
      "Epoch [126/500], Step [20/22], Loss: 0.1256\n",
      "Epoch [127/500], Step [10/22], Loss: 0.2345\n",
      "Epoch [127/500], Step [20/22], Loss: 0.1225\n",
      "Epoch [128/500], Step [10/22], Loss: 0.2104\n",
      "Epoch [128/500], Step [20/22], Loss: 0.1233\n",
      "Epoch [129/500], Step [10/22], Loss: 0.2236\n",
      "Epoch [129/500], Step [20/22], Loss: 0.1135\n",
      "Epoch [130/500], Step [10/22], Loss: 0.2151\n",
      "Epoch [130/500], Step [20/22], Loss: 0.1134\n",
      "Epoch [131/500], Step [10/22], Loss: 0.2485\n",
      "Epoch [131/500], Step [20/22], Loss: 0.1113\n",
      "Epoch [132/500], Step [10/22], Loss: 0.2228\n",
      "Epoch [132/500], Step [20/22], Loss: 0.1248\n",
      "Epoch [133/500], Step [10/22], Loss: 0.3286\n",
      "Epoch [133/500], Step [20/22], Loss: 0.1440\n",
      "Epoch [134/500], Step [10/22], Loss: 0.5515\n",
      "Epoch [134/500], Step [20/22], Loss: 0.1487\n",
      "Epoch [135/500], Step [10/22], Loss: 0.2503\n",
      "Epoch [135/500], Step [20/22], Loss: 0.1251\n",
      "Epoch [136/500], Step [10/22], Loss: 0.2269\n",
      "Epoch [136/500], Step [20/22], Loss: 0.1060\n",
      "Epoch [137/500], Step [10/22], Loss: 0.2169\n",
      "Epoch [137/500], Step [20/22], Loss: 0.1098\n",
      "Epoch [138/500], Step [10/22], Loss: 0.1962\n",
      "Epoch [138/500], Step [20/22], Loss: 0.0984\n",
      "Epoch [139/500], Step [10/22], Loss: 0.2726\n",
      "Epoch [139/500], Step [20/22], Loss: 0.0913\n",
      "Epoch [140/500], Step [10/22], Loss: 0.2463\n",
      "Epoch [140/500], Step [20/22], Loss: 0.1057\n",
      "Epoch [141/500], Step [10/22], Loss: 0.2194\n",
      "Epoch [141/500], Step [20/22], Loss: 0.1077\n",
      "Epoch [142/500], Step [10/22], Loss: 0.2425\n",
      "Epoch [142/500], Step [20/22], Loss: 0.1177\n",
      "Epoch [143/500], Step [10/22], Loss: 0.2878\n",
      "Epoch [143/500], Step [20/22], Loss: 0.1300\n",
      "Epoch [144/500], Step [10/22], Loss: 0.4047\n",
      "Epoch [144/500], Step [20/22], Loss: 0.1286\n",
      "Epoch [145/500], Step [10/22], Loss: 0.2633\n",
      "Epoch [145/500], Step [20/22], Loss: 0.1217\n",
      "Epoch [146/500], Step [10/22], Loss: 0.2073\n",
      "Epoch [146/500], Step [20/22], Loss: 0.1109\n",
      "Epoch [147/500], Step [10/22], Loss: 0.2021\n",
      "Epoch [147/500], Step [20/22], Loss: 0.1108\n",
      "Epoch [148/500], Step [10/22], Loss: 0.1820\n",
      "Epoch [148/500], Step [20/22], Loss: 0.1117\n",
      "Epoch [149/500], Step [10/22], Loss: 0.2264\n",
      "Epoch [149/500], Step [20/22], Loss: 0.1337\n",
      "Epoch [150/500], Step [10/22], Loss: 0.2300\n",
      "Epoch [150/500], Step [20/22], Loss: 0.1198\n",
      "Epoch [151/500], Step [10/22], Loss: 0.2455\n",
      "Epoch [151/500], Step [20/22], Loss: 0.1134\n",
      "Epoch [152/500], Step [10/22], Loss: 0.2822\n",
      "Epoch [152/500], Step [20/22], Loss: 0.1078\n",
      "Epoch [153/500], Step [10/22], Loss: 0.1878\n",
      "Epoch [153/500], Step [20/22], Loss: 0.0959\n",
      "Epoch [154/500], Step [10/22], Loss: 0.2782\n",
      "Epoch [154/500], Step [20/22], Loss: 0.1089\n",
      "Epoch [155/500], Step [10/22], Loss: 0.2912\n",
      "Epoch [155/500], Step [20/22], Loss: 0.1283\n",
      "Epoch [156/500], Step [10/22], Loss: 0.2713\n",
      "Epoch [156/500], Step [20/22], Loss: 0.1244\n",
      "Epoch [157/500], Step [10/22], Loss: 0.4176\n",
      "Epoch [157/500], Step [20/22], Loss: 0.1967\n",
      "Epoch [158/500], Step [10/22], Loss: 0.2850\n",
      "Epoch [158/500], Step [20/22], Loss: 0.1426\n",
      "Epoch [159/500], Step [10/22], Loss: 0.2591\n",
      "Epoch [159/500], Step [20/22], Loss: 0.1252\n",
      "Epoch [160/500], Step [10/22], Loss: 0.2309\n",
      "Epoch [160/500], Step [20/22], Loss: 0.1428\n",
      "Epoch [161/500], Step [10/22], Loss: 0.3404\n",
      "Epoch [161/500], Step [20/22], Loss: 0.1204\n",
      "Epoch [162/500], Step [10/22], Loss: 0.2913\n",
      "Epoch [162/500], Step [20/22], Loss: 0.1162\n",
      "Epoch [163/500], Step [10/22], Loss: 0.5028\n",
      "Epoch [163/500], Step [20/22], Loss: 0.1160\n",
      "Epoch [164/500], Step [10/22], Loss: 0.3645\n",
      "Epoch [164/500], Step [20/22], Loss: 0.1233\n",
      "Epoch [165/500], Step [10/22], Loss: 0.2664\n",
      "Epoch [165/500], Step [20/22], Loss: 0.1208\n",
      "Epoch [166/500], Step [10/22], Loss: 0.2682\n",
      "Epoch [166/500], Step [20/22], Loss: 0.1163\n",
      "Epoch [167/500], Step [10/22], Loss: 0.2408\n",
      "Epoch [167/500], Step [20/22], Loss: 0.1283\n",
      "Epoch [168/500], Step [10/22], Loss: 0.2115\n",
      "Epoch [168/500], Step [20/22], Loss: 0.1033\n",
      "Epoch [169/500], Step [10/22], Loss: 0.2695\n",
      "Epoch [169/500], Step [20/22], Loss: 0.1117\n",
      "Epoch [170/500], Step [10/22], Loss: 0.2889\n",
      "Epoch [170/500], Step [20/22], Loss: 0.1101\n",
      "Epoch [171/500], Step [10/22], Loss: 0.2320\n",
      "Epoch [171/500], Step [20/22], Loss: 0.1243\n",
      "Epoch [172/500], Step [10/22], Loss: 0.2704\n",
      "Epoch [172/500], Step [20/22], Loss: 0.1202\n",
      "Epoch [173/500], Step [10/22], Loss: 0.1966\n",
      "Epoch [173/500], Step [20/22], Loss: 0.1257\n",
      "Epoch [174/500], Step [10/22], Loss: 0.2235\n",
      "Epoch [174/500], Step [20/22], Loss: 0.1164\n",
      "Epoch [175/500], Step [10/22], Loss: 0.2091\n",
      "Epoch [175/500], Step [20/22], Loss: 0.1355\n",
      "Epoch [176/500], Step [10/22], Loss: 0.2004\n",
      "Epoch [176/500], Step [20/22], Loss: 0.1252\n",
      "Epoch [177/500], Step [10/22], Loss: 0.2304\n",
      "Epoch [177/500], Step [20/22], Loss: 0.1130\n",
      "Epoch [178/500], Step [10/22], Loss: 0.3429\n",
      "Epoch [178/500], Step [20/22], Loss: 0.1128\n",
      "Epoch [179/500], Step [10/22], Loss: 0.2206\n",
      "Epoch [179/500], Step [20/22], Loss: 0.1054\n",
      "Epoch [180/500], Step [10/22], Loss: 0.2277\n",
      "Epoch [180/500], Step [20/22], Loss: 0.1115\n",
      "Epoch [181/500], Step [10/22], Loss: 0.3391\n",
      "Epoch [181/500], Step [20/22], Loss: 0.1016\n",
      "Epoch [182/500], Step [10/22], Loss: 0.4403\n",
      "Epoch [182/500], Step [20/22], Loss: 0.0970\n",
      "Epoch [183/500], Step [10/22], Loss: 0.2287\n",
      "Epoch [183/500], Step [20/22], Loss: 0.1340\n",
      "Epoch [184/500], Step [10/22], Loss: 0.2168\n",
      "Epoch [184/500], Step [20/22], Loss: 0.0898\n",
      "Epoch [185/500], Step [10/22], Loss: 0.2339\n",
      "Epoch [185/500], Step [20/22], Loss: 0.1120\n",
      "Epoch [186/500], Step [10/22], Loss: 0.1983\n",
      "Epoch [186/500], Step [20/22], Loss: 0.1076\n",
      "Epoch [187/500], Step [10/22], Loss: 0.1818\n",
      "Epoch [187/500], Step [20/22], Loss: 0.1259\n",
      "Epoch [188/500], Step [10/22], Loss: 0.3325\n",
      "Epoch [188/500], Step [20/22], Loss: 0.1103\n",
      "Epoch [189/500], Step [10/22], Loss: 0.3219\n",
      "Epoch [189/500], Step [20/22], Loss: 0.1242\n",
      "Epoch [190/500], Step [10/22], Loss: 0.2721\n",
      "Epoch [190/500], Step [20/22], Loss: 0.1210\n",
      "Epoch [191/500], Step [10/22], Loss: 0.2582\n",
      "Epoch [191/500], Step [20/22], Loss: 0.1014\n",
      "Epoch [192/500], Step [10/22], Loss: 0.2199\n",
      "Epoch [192/500], Step [20/22], Loss: 0.1278\n",
      "Epoch [193/500], Step [10/22], Loss: 0.1999\n",
      "Epoch [193/500], Step [20/22], Loss: 0.0946\n",
      "Epoch [194/500], Step [10/22], Loss: 0.1980\n",
      "Epoch [194/500], Step [20/22], Loss: 0.1072\n",
      "Epoch [195/500], Step [10/22], Loss: 0.1953\n",
      "Epoch [195/500], Step [20/22], Loss: 0.0936\n",
      "Epoch [196/500], Step [10/22], Loss: 0.1861\n",
      "Epoch [196/500], Step [20/22], Loss: 0.1024\n",
      "Epoch [197/500], Step [10/22], Loss: 0.2242\n",
      "Epoch [197/500], Step [20/22], Loss: 0.1014\n",
      "Epoch [198/500], Step [10/22], Loss: 0.2116\n",
      "Epoch [198/500], Step [20/22], Loss: 0.0999\n",
      "Epoch [199/500], Step [10/22], Loss: 0.2712\n",
      "Epoch [199/500], Step [20/22], Loss: 0.1102\n",
      "Epoch [200/500], Step [10/22], Loss: 0.2353\n",
      "Epoch [200/500], Step [20/22], Loss: 0.1130\n",
      "Epoch [201/500], Step [10/22], Loss: 0.2161\n",
      "Epoch [201/500], Step [20/22], Loss: 0.0912\n",
      "Epoch [202/500], Step [10/22], Loss: 0.1808\n",
      "Epoch [202/500], Step [20/22], Loss: 0.0864\n",
      "Epoch [203/500], Step [10/22], Loss: 0.1782\n",
      "Epoch [203/500], Step [20/22], Loss: 0.0840\n",
      "Epoch [204/500], Step [10/22], Loss: 0.1641\n",
      "Epoch [204/500], Step [20/22], Loss: 0.1029\n",
      "Epoch [205/500], Step [10/22], Loss: 0.3037\n",
      "Epoch [205/500], Step [20/22], Loss: 0.1013\n",
      "Epoch [206/500], Step [10/22], Loss: 0.3298\n",
      "Epoch [206/500], Step [20/22], Loss: 0.1395\n",
      "Epoch [207/500], Step [10/22], Loss: 0.3815\n",
      "Epoch [207/500], Step [20/22], Loss: 0.1406\n",
      "Epoch [208/500], Step [10/22], Loss: 0.3629\n",
      "Epoch [208/500], Step [20/22], Loss: 0.1596\n",
      "Epoch [209/500], Step [10/22], Loss: 0.1988\n",
      "Epoch [209/500], Step [20/22], Loss: 0.1157\n",
      "Epoch [210/500], Step [10/22], Loss: 0.1834\n",
      "Epoch [210/500], Step [20/22], Loss: 0.1150\n",
      "Epoch [211/500], Step [10/22], Loss: 0.2387\n",
      "Epoch [211/500], Step [20/22], Loss: 0.1064\n",
      "Epoch [212/500], Step [10/22], Loss: 0.1945\n",
      "Epoch [212/500], Step [20/22], Loss: 0.1057\n",
      "Epoch [213/500], Step [10/22], Loss: 0.1600\n",
      "Epoch [213/500], Step [20/22], Loss: 0.1142\n",
      "Epoch [214/500], Step [10/22], Loss: 0.2862\n",
      "Epoch [214/500], Step [20/22], Loss: 0.0991\n",
      "Epoch [215/500], Step [10/22], Loss: 0.2969\n",
      "Epoch [215/500], Step [20/22], Loss: 0.0868\n",
      "Epoch [216/500], Step [10/22], Loss: 0.1626\n",
      "Epoch [216/500], Step [20/22], Loss: 0.0950\n",
      "Epoch [217/500], Step [10/22], Loss: 0.2093\n",
      "Epoch [217/500], Step [20/22], Loss: 0.0902\n",
      "Epoch [218/500], Step [10/22], Loss: 0.2011\n",
      "Epoch [218/500], Step [20/22], Loss: 0.0781\n",
      "Epoch [219/500], Step [10/22], Loss: 0.1986\n",
      "Epoch [219/500], Step [20/22], Loss: 0.0827\n",
      "Epoch [220/500], Step [10/22], Loss: 0.1552\n",
      "Epoch [220/500], Step [20/22], Loss: 0.0906\n",
      "Epoch [221/500], Step [10/22], Loss: 0.1590\n",
      "Epoch [221/500], Step [20/22], Loss: 0.1181\n",
      "Epoch [222/500], Step [10/22], Loss: 0.4158\n",
      "Epoch [222/500], Step [20/22], Loss: 0.0921\n",
      "Epoch [223/500], Step [10/22], Loss: 0.4831\n",
      "Epoch [223/500], Step [20/22], Loss: 0.0859\n",
      "Epoch [224/500], Step [10/22], Loss: 0.2358\n",
      "Epoch [224/500], Step [20/22], Loss: 0.0688\n",
      "Epoch [225/500], Step [10/22], Loss: 0.2139\n",
      "Epoch [225/500], Step [20/22], Loss: 0.0682\n",
      "Epoch [226/500], Step [10/22], Loss: 0.1747\n",
      "Epoch [226/500], Step [20/22], Loss: 0.0645\n",
      "Epoch [227/500], Step [10/22], Loss: 0.2122\n",
      "Epoch [227/500], Step [20/22], Loss: 0.0612\n",
      "Epoch [228/500], Step [10/22], Loss: 0.2011\n",
      "Epoch [228/500], Step [20/22], Loss: 0.1056\n",
      "Epoch [229/500], Step [10/22], Loss: 0.2490\n",
      "Epoch [229/500], Step [20/22], Loss: 0.1063\n",
      "Epoch [230/500], Step [10/22], Loss: 0.2201\n",
      "Epoch [230/500], Step [20/22], Loss: 0.0773\n",
      "Epoch [231/500], Step [10/22], Loss: 0.2272\n",
      "Epoch [231/500], Step [20/22], Loss: 0.0877\n",
      "Epoch [232/500], Step [10/22], Loss: 0.1951\n",
      "Epoch [232/500], Step [20/22], Loss: 0.0708\n",
      "Epoch [233/500], Step [10/22], Loss: 0.1838\n",
      "Epoch [233/500], Step [20/22], Loss: 0.0615\n",
      "Epoch [234/500], Step [10/22], Loss: 0.1960\n",
      "Epoch [234/500], Step [20/22], Loss: 0.1058\n",
      "Epoch [235/500], Step [10/22], Loss: 0.1678\n",
      "Epoch [235/500], Step [20/22], Loss: 0.0705\n",
      "Epoch [236/500], Step [10/22], Loss: 0.1545\n",
      "Epoch [236/500], Step [20/22], Loss: 0.0577\n",
      "Epoch [237/500], Step [10/22], Loss: 0.1703\n",
      "Epoch [237/500], Step [20/22], Loss: 0.0772\n",
      "Epoch [238/500], Step [10/22], Loss: 0.2070\n",
      "Epoch [238/500], Step [20/22], Loss: 0.0715\n",
      "Epoch [239/500], Step [10/22], Loss: 0.1502\n",
      "Epoch [239/500], Step [20/22], Loss: 0.0776\n",
      "Epoch [240/500], Step [10/22], Loss: 0.1588\n",
      "Epoch [240/500], Step [20/22], Loss: 0.0609\n",
      "Epoch [241/500], Step [10/22], Loss: 0.2946\n",
      "Epoch [241/500], Step [20/22], Loss: 0.0690\n",
      "Epoch [242/500], Step [10/22], Loss: 0.1876\n",
      "Epoch [242/500], Step [20/22], Loss: 0.0724\n",
      "Epoch [243/500], Step [10/22], Loss: 0.1882\n",
      "Epoch [243/500], Step [20/22], Loss: 0.1158\n",
      "Epoch [244/500], Step [10/22], Loss: 0.2321\n",
      "Epoch [244/500], Step [20/22], Loss: 0.1197\n",
      "Epoch [245/500], Step [10/22], Loss: 0.1863\n",
      "Epoch [245/500], Step [20/22], Loss: 0.1015\n",
      "Epoch [246/500], Step [10/22], Loss: 0.2572\n",
      "Epoch [246/500], Step [20/22], Loss: 0.1222\n",
      "Epoch [247/500], Step [10/22], Loss: 0.2553\n",
      "Epoch [247/500], Step [20/22], Loss: 0.1295\n",
      "Epoch [248/500], Step [10/22], Loss: 0.3672\n",
      "Epoch [248/500], Step [20/22], Loss: 0.1231\n",
      "Epoch [249/500], Step [10/22], Loss: 0.3562\n",
      "Epoch [249/500], Step [20/22], Loss: 0.1337\n",
      "Epoch [250/500], Step [10/22], Loss: 0.1657\n",
      "Epoch [250/500], Step [20/22], Loss: 0.1005\n",
      "Epoch [251/500], Step [10/22], Loss: 0.2358\n",
      "Epoch [251/500], Step [20/22], Loss: 0.0874\n",
      "Epoch [252/500], Step [10/22], Loss: 0.2060\n",
      "Epoch [252/500], Step [20/22], Loss: 0.1046\n",
      "Epoch [253/500], Step [10/22], Loss: 0.1811\n",
      "Epoch [253/500], Step [20/22], Loss: 0.0921\n",
      "Epoch [254/500], Step [10/22], Loss: 0.1793\n",
      "Epoch [254/500], Step [20/22], Loss: 0.0777\n",
      "Epoch [255/500], Step [10/22], Loss: 0.1720\n",
      "Epoch [255/500], Step [20/22], Loss: 0.0982\n",
      "Epoch [256/500], Step [10/22], Loss: 0.1820\n",
      "Epoch [256/500], Step [20/22], Loss: 0.0901\n",
      "Epoch [257/500], Step [10/22], Loss: 0.2299\n",
      "Epoch [257/500], Step [20/22], Loss: 0.1100\n",
      "Epoch [258/500], Step [10/22], Loss: 0.1885\n",
      "Epoch [258/500], Step [20/22], Loss: 0.0782\n",
      "Epoch [259/500], Step [10/22], Loss: 0.1471\n",
      "Epoch [259/500], Step [20/22], Loss: 0.0784\n",
      "Epoch [260/500], Step [10/22], Loss: 0.1462\n",
      "Epoch [260/500], Step [20/22], Loss: 0.0620\n",
      "Epoch [261/500], Step [10/22], Loss: 0.1809\n",
      "Epoch [261/500], Step [20/22], Loss: 0.0711\n",
      "Epoch [262/500], Step [10/22], Loss: 0.1654\n",
      "Epoch [262/500], Step [20/22], Loss: 0.0722\n",
      "Epoch [263/500], Step [10/22], Loss: 0.1622\n",
      "Epoch [263/500], Step [20/22], Loss: 0.0548\n",
      "Epoch [264/500], Step [10/22], Loss: 0.1998\n",
      "Epoch [264/500], Step [20/22], Loss: 0.0610\n",
      "Epoch [265/500], Step [10/22], Loss: 0.1931\n",
      "Epoch [265/500], Step [20/22], Loss: 0.0625\n",
      "Epoch [266/500], Step [10/22], Loss: 0.1577\n",
      "Epoch [266/500], Step [20/22], Loss: 0.0746\n",
      "Epoch [267/500], Step [10/22], Loss: 0.2218\n",
      "Epoch [267/500], Step [20/22], Loss: 0.0610\n",
      "Epoch [268/500], Step [10/22], Loss: 0.1488\n",
      "Epoch [268/500], Step [20/22], Loss: 0.0481\n",
      "Epoch [269/500], Step [10/22], Loss: 0.1614\n",
      "Epoch [269/500], Step [20/22], Loss: 0.0735\n",
      "Epoch [270/500], Step [10/22], Loss: 0.2131\n",
      "Epoch [270/500], Step [20/22], Loss: 0.0669\n",
      "Epoch [271/500], Step [10/22], Loss: 0.2655\n",
      "Epoch [271/500], Step [20/22], Loss: 0.0458\n",
      "Epoch [272/500], Step [10/22], Loss: 0.1422\n",
      "Epoch [272/500], Step [20/22], Loss: 0.1289\n",
      "Epoch [273/500], Step [10/22], Loss: 0.1746\n",
      "Epoch [273/500], Step [20/22], Loss: 0.0663\n",
      "Epoch [274/500], Step [10/22], Loss: 0.1932\n",
      "Epoch [274/500], Step [20/22], Loss: 0.0656\n",
      "Epoch [275/500], Step [10/22], Loss: 0.2491\n",
      "Epoch [275/500], Step [20/22], Loss: 0.1032\n",
      "Epoch [276/500], Step [10/22], Loss: 0.1457\n",
      "Epoch [276/500], Step [20/22], Loss: 0.0616\n",
      "Epoch [277/500], Step [10/22], Loss: 0.1424\n",
      "Epoch [277/500], Step [20/22], Loss: 0.0456\n",
      "Epoch [278/500], Step [10/22], Loss: 0.1673\n",
      "Epoch [278/500], Step [20/22], Loss: 0.0586\n",
      "Epoch [279/500], Step [10/22], Loss: 0.1429\n",
      "Epoch [279/500], Step [20/22], Loss: 0.0477\n",
      "Epoch [280/500], Step [10/22], Loss: 0.1404\n",
      "Epoch [280/500], Step [20/22], Loss: 0.0397\n",
      "Epoch [281/500], Step [10/22], Loss: 0.1651\n",
      "Epoch [281/500], Step [20/22], Loss: 0.0573\n",
      "Epoch [282/500], Step [10/22], Loss: 0.2667\n",
      "Epoch [282/500], Step [20/22], Loss: 0.0446\n",
      "Epoch [283/500], Step [10/22], Loss: 0.3076\n",
      "Epoch [283/500], Step [20/22], Loss: 0.1231\n",
      "Epoch [284/500], Step [10/22], Loss: 0.1487\n",
      "Epoch [284/500], Step [20/22], Loss: 0.0585\n",
      "Epoch [285/500], Step [10/22], Loss: 0.1458\n",
      "Epoch [285/500], Step [20/22], Loss: 0.0396\n",
      "Epoch [286/500], Step [10/22], Loss: 0.1654\n",
      "Epoch [286/500], Step [20/22], Loss: 0.0652\n",
      "Epoch [287/500], Step [10/22], Loss: 0.1392\n",
      "Epoch [287/500], Step [20/22], Loss: 0.0401\n",
      "Epoch [288/500], Step [10/22], Loss: 0.1256\n",
      "Epoch [288/500], Step [20/22], Loss: 0.0394\n",
      "Epoch [289/500], Step [10/22], Loss: 0.1250\n",
      "Epoch [289/500], Step [20/22], Loss: 0.0451\n",
      "Epoch [290/500], Step [10/22], Loss: 0.1337\n",
      "Epoch [290/500], Step [20/22], Loss: 0.0316\n",
      "Epoch [291/500], Step [10/22], Loss: 0.1173\n",
      "Epoch [291/500], Step [20/22], Loss: 0.0322\n",
      "Epoch [292/500], Step [10/22], Loss: 0.1378\n",
      "Epoch [292/500], Step [20/22], Loss: 0.0316\n",
      "Epoch [293/500], Step [10/22], Loss: 0.1808\n",
      "Epoch [293/500], Step [20/22], Loss: 0.0622\n",
      "Epoch [294/500], Step [10/22], Loss: 0.1951\n",
      "Epoch [294/500], Step [20/22], Loss: 0.0308\n",
      "Epoch [295/500], Step [10/22], Loss: 0.1247\n",
      "Epoch [295/500], Step [20/22], Loss: 0.1919\n",
      "Epoch [296/500], Step [10/22], Loss: 0.2721\n",
      "Epoch [296/500], Step [20/22], Loss: 0.1506\n",
      "Epoch [297/500], Step [10/22], Loss: 0.2018\n",
      "Epoch [297/500], Step [20/22], Loss: 0.1111\n",
      "Epoch [298/500], Step [10/22], Loss: 0.1719\n",
      "Epoch [298/500], Step [20/22], Loss: 0.0400\n",
      "Epoch [299/500], Step [10/22], Loss: 0.1658\n",
      "Epoch [299/500], Step [20/22], Loss: 0.0789\n",
      "Epoch [300/500], Step [10/22], Loss: 0.1714\n",
      "Epoch [300/500], Step [20/22], Loss: 0.0562\n",
      "Epoch [301/500], Step [10/22], Loss: 0.1270\n",
      "Epoch [301/500], Step [20/22], Loss: 0.0564\n",
      "Epoch [302/500], Step [10/22], Loss: 0.1196\n",
      "Epoch [302/500], Step [20/22], Loss: 0.0510\n",
      "Epoch [303/500], Step [10/22], Loss: 0.1404\n",
      "Epoch [303/500], Step [20/22], Loss: 0.0419\n",
      "Epoch [304/500], Step [10/22], Loss: 0.1207\n",
      "Epoch [304/500], Step [20/22], Loss: 0.0412\n",
      "Epoch [305/500], Step [10/22], Loss: 0.1286\n",
      "Epoch [305/500], Step [20/22], Loss: 0.0401\n",
      "Epoch [306/500], Step [10/22], Loss: 0.1254\n",
      "Epoch [306/500], Step [20/22], Loss: 0.0330\n",
      "Epoch [307/500], Step [10/22], Loss: 0.1176\n",
      "Epoch [307/500], Step [20/22], Loss: 0.0389\n",
      "Epoch [308/500], Step [10/22], Loss: 0.1224\n",
      "Epoch [308/500], Step [20/22], Loss: 0.0298\n",
      "Epoch [309/500], Step [10/22], Loss: 0.1187\n",
      "Epoch [309/500], Step [20/22], Loss: 0.0321\n",
      "Epoch [310/500], Step [10/22], Loss: 0.1420\n",
      "Epoch [310/500], Step [20/22], Loss: 0.0426\n",
      "Epoch [311/500], Step [10/22], Loss: 0.1653\n",
      "Epoch [311/500], Step [20/22], Loss: 0.0334\n",
      "Epoch [312/500], Step [10/22], Loss: 0.1209\n",
      "Epoch [312/500], Step [20/22], Loss: 0.0666\n",
      "Epoch [313/500], Step [10/22], Loss: 0.2473\n",
      "Epoch [313/500], Step [20/22], Loss: 0.0649\n",
      "Epoch [314/500], Step [10/22], Loss: 0.1486\n",
      "Epoch [314/500], Step [20/22], Loss: 0.0609\n",
      "Epoch [315/500], Step [10/22], Loss: 0.3040\n",
      "Epoch [315/500], Step [20/22], Loss: 0.0732\n",
      "Epoch [316/500], Step [10/22], Loss: 0.2572\n",
      "Epoch [316/500], Step [20/22], Loss: 0.0427\n",
      "Epoch [317/500], Step [10/22], Loss: 0.1861\n",
      "Epoch [317/500], Step [20/22], Loss: 0.0626\n",
      "Epoch [318/500], Step [10/22], Loss: 0.1611\n",
      "Epoch [318/500], Step [20/22], Loss: 0.0426\n",
      "Epoch [319/500], Step [10/22], Loss: 0.1520\n",
      "Epoch [319/500], Step [20/22], Loss: 0.0395\n",
      "Epoch [320/500], Step [10/22], Loss: 0.1497\n",
      "Epoch [320/500], Step [20/22], Loss: 0.0588\n",
      "Epoch [321/500], Step [10/22], Loss: 0.2200\n",
      "Epoch [321/500], Step [20/22], Loss: 0.0374\n",
      "Epoch [322/500], Step [10/22], Loss: 0.1438\n",
      "Epoch [322/500], Step [20/22], Loss: 0.0339\n",
      "Epoch [323/500], Step [10/22], Loss: 0.1393\n",
      "Epoch [323/500], Step [20/22], Loss: 0.0444\n",
      "Epoch [324/500], Step [10/22], Loss: 0.1337\n",
      "Epoch [324/500], Step [20/22], Loss: 0.0415\n",
      "Epoch [325/500], Step [10/22], Loss: 0.1221\n",
      "Epoch [325/500], Step [20/22], Loss: 0.0431\n",
      "Epoch [326/500], Step [10/22], Loss: 0.1267\n",
      "Epoch [326/500], Step [20/22], Loss: 0.0432\n",
      "Epoch [327/500], Step [10/22], Loss: 0.1248\n",
      "Epoch [327/500], Step [20/22], Loss: 0.0291\n",
      "Epoch [328/500], Step [10/22], Loss: 0.1308\n",
      "Epoch [328/500], Step [20/22], Loss: 0.0814\n",
      "Epoch [329/500], Step [10/22], Loss: 0.1329\n",
      "Epoch [329/500], Step [20/22], Loss: 0.0424\n",
      "Epoch [330/500], Step [10/22], Loss: 0.1515\n",
      "Epoch [330/500], Step [20/22], Loss: 0.0311\n",
      "Epoch [331/500], Step [10/22], Loss: 0.1214\n",
      "Epoch [331/500], Step [20/22], Loss: 0.0963\n",
      "Epoch [332/500], Step [10/22], Loss: 0.1512\n",
      "Epoch [332/500], Step [20/22], Loss: 0.0472\n",
      "Epoch [333/500], Step [10/22], Loss: 0.1740\n",
      "Epoch [333/500], Step [20/22], Loss: 0.0427\n",
      "Epoch [334/500], Step [10/22], Loss: 0.2112\n",
      "Epoch [334/500], Step [20/22], Loss: 0.0475\n",
      "Epoch [335/500], Step [10/22], Loss: 0.1906\n",
      "Epoch [335/500], Step [20/22], Loss: 0.0573\n",
      "Epoch [336/500], Step [10/22], Loss: 0.1552\n",
      "Epoch [336/500], Step [20/22], Loss: 0.0488\n",
      "Epoch [337/500], Step [10/22], Loss: 0.1457\n",
      "Epoch [337/500], Step [20/22], Loss: 0.0637\n",
      "Epoch [338/500], Step [10/22], Loss: 0.1838\n",
      "Epoch [338/500], Step [20/22], Loss: 0.0405\n",
      "Epoch [339/500], Step [10/22], Loss: 0.2029\n",
      "Epoch [339/500], Step [20/22], Loss: 0.0410\n",
      "Epoch [340/500], Step [10/22], Loss: 0.1405\n",
      "Epoch [340/500], Step [20/22], Loss: 0.0341\n",
      "Epoch [341/500], Step [10/22], Loss: 0.1221\n",
      "Epoch [341/500], Step [20/22], Loss: 0.0476\n",
      "Epoch [342/500], Step [10/22], Loss: 0.1143\n",
      "Epoch [342/500], Step [20/22], Loss: 0.0554\n",
      "Epoch [343/500], Step [10/22], Loss: 0.1325\n",
      "Epoch [343/500], Step [20/22], Loss: 0.0841\n",
      "Epoch [344/500], Step [10/22], Loss: 0.1302\n",
      "Epoch [344/500], Step [20/22], Loss: 0.0510\n",
      "Epoch [345/500], Step [10/22], Loss: 0.1228\n",
      "Epoch [345/500], Step [20/22], Loss: 0.0413\n",
      "Epoch [346/500], Step [10/22], Loss: 0.1484\n",
      "Epoch [346/500], Step [20/22], Loss: 0.0235\n",
      "Epoch [347/500], Step [10/22], Loss: 0.1139\n",
      "Epoch [347/500], Step [20/22], Loss: 0.0442\n",
      "Epoch [348/500], Step [10/22], Loss: 0.1179\n",
      "Epoch [348/500], Step [20/22], Loss: 0.0325\n",
      "Epoch [349/500], Step [10/22], Loss: 0.1074\n",
      "Epoch [349/500], Step [20/22], Loss: 0.0360\n",
      "Epoch [350/500], Step [10/22], Loss: 0.1153\n",
      "Epoch [350/500], Step [20/22], Loss: 0.0253\n",
      "Epoch [351/500], Step [10/22], Loss: 0.1035\n",
      "Epoch [351/500], Step [20/22], Loss: 0.0371\n",
      "Epoch [352/500], Step [10/22], Loss: 0.1042\n",
      "Epoch [352/500], Step [20/22], Loss: 0.1669\n",
      "Epoch [353/500], Step [10/22], Loss: 0.1401\n",
      "Epoch [353/500], Step [20/22], Loss: 0.1088\n",
      "Epoch [354/500], Step [10/22], Loss: 0.1067\n",
      "Epoch [354/500], Step [20/22], Loss: 0.0438\n",
      "Epoch [355/500], Step [10/22], Loss: 0.1846\n",
      "Epoch [355/500], Step [20/22], Loss: 0.0384\n",
      "Epoch [356/500], Step [10/22], Loss: 0.1791\n",
      "Epoch [356/500], Step [20/22], Loss: 0.0299\n",
      "Epoch [357/500], Step [10/22], Loss: 0.1127\n",
      "Epoch [357/500], Step [20/22], Loss: 0.0274\n",
      "Epoch [358/500], Step [10/22], Loss: 0.3233\n",
      "Epoch [358/500], Step [20/22], Loss: 0.0441\n",
      "Epoch [359/500], Step [10/22], Loss: 0.1427\n",
      "Epoch [359/500], Step [20/22], Loss: 0.0333\n",
      "Epoch [360/500], Step [10/22], Loss: 0.2563\n",
      "Epoch [360/500], Step [20/22], Loss: 0.0251\n",
      "Epoch [361/500], Step [10/22], Loss: 0.1836\n",
      "Epoch [361/500], Step [20/22], Loss: 0.1793\n",
      "Epoch [362/500], Step [10/22], Loss: 0.1297\n",
      "Epoch [362/500], Step [20/22], Loss: 0.1331\n",
      "Epoch [363/500], Step [10/22], Loss: 0.2147\n",
      "Epoch [363/500], Step [20/22], Loss: 0.0689\n",
      "Epoch [364/500], Step [10/22], Loss: 0.2263\n",
      "Epoch [364/500], Step [20/22], Loss: 0.0677\n",
      "Epoch [365/500], Step [10/22], Loss: 0.1456\n",
      "Epoch [365/500], Step [20/22], Loss: 0.0787\n",
      "Epoch [366/500], Step [10/22], Loss: 0.1430\n",
      "Epoch [366/500], Step [20/22], Loss: 0.0664\n",
      "Epoch [367/500], Step [10/22], Loss: 0.1476\n",
      "Epoch [367/500], Step [20/22], Loss: 0.0422\n",
      "Epoch [368/500], Step [10/22], Loss: 0.1372\n",
      "Epoch [368/500], Step [20/22], Loss: 0.0458\n",
      "Epoch [369/500], Step [10/22], Loss: 0.1164\n",
      "Epoch [369/500], Step [20/22], Loss: 0.0473\n",
      "Epoch [370/500], Step [10/22], Loss: 0.1367\n",
      "Epoch [370/500], Step [20/22], Loss: 0.0636\n",
      "Epoch [371/500], Step [10/22], Loss: 0.1141\n",
      "Epoch [371/500], Step [20/22], Loss: 0.0469\n",
      "Epoch [372/500], Step [10/22], Loss: 0.1356\n",
      "Epoch [372/500], Step [20/22], Loss: 0.0324\n",
      "Epoch [373/500], Step [10/22], Loss: 0.1119\n",
      "Epoch [373/500], Step [20/22], Loss: 0.0251\n",
      "Epoch [374/500], Step [10/22], Loss: 0.1001\n",
      "Epoch [374/500], Step [20/22], Loss: 0.0399\n",
      "Epoch [375/500], Step [10/22], Loss: 0.1543\n",
      "Epoch [375/500], Step [20/22], Loss: 0.0354\n",
      "Epoch [376/500], Step [10/22], Loss: 0.1394\n",
      "Epoch [376/500], Step [20/22], Loss: 0.0773\n",
      "Epoch [377/500], Step [10/22], Loss: 0.1113\n",
      "Epoch [377/500], Step [20/22], Loss: 0.0463\n",
      "Epoch [378/500], Step [10/22], Loss: 0.1034\n",
      "Epoch [378/500], Step [20/22], Loss: 0.0340\n",
      "Epoch [379/500], Step [10/22], Loss: 0.1145\n",
      "Epoch [379/500], Step [20/22], Loss: 0.0398\n",
      "Epoch [380/500], Step [10/22], Loss: 0.1468\n",
      "Epoch [380/500], Step [20/22], Loss: 0.0399\n",
      "Epoch [381/500], Step [10/22], Loss: 0.1388\n",
      "Epoch [381/500], Step [20/22], Loss: 0.0568\n",
      "Epoch [382/500], Step [10/22], Loss: 0.1104\n",
      "Epoch [382/500], Step [20/22], Loss: 0.0368\n",
      "Epoch [383/500], Step [10/22], Loss: 0.0959\n",
      "Epoch [383/500], Step [20/22], Loss: 0.0228\n",
      "Epoch [384/500], Step [10/22], Loss: 0.1330\n",
      "Epoch [384/500], Step [20/22], Loss: 0.0314\n",
      "Epoch [385/500], Step [10/22], Loss: 0.1692\n",
      "Epoch [385/500], Step [20/22], Loss: 0.0613\n",
      "Epoch [386/500], Step [10/22], Loss: 0.1675\n",
      "Epoch [386/500], Step [20/22], Loss: 0.0435\n",
      "Epoch [387/500], Step [10/22], Loss: 0.1191\n",
      "Epoch [387/500], Step [20/22], Loss: 0.0441\n",
      "Epoch [388/500], Step [10/22], Loss: 0.1146\n",
      "Epoch [388/500], Step [20/22], Loss: 0.0530\n",
      "Epoch [389/500], Step [10/22], Loss: 0.1274\n",
      "Epoch [389/500], Step [20/22], Loss: 0.0228\n",
      "Epoch [390/500], Step [10/22], Loss: 0.1016\n",
      "Epoch [390/500], Step [20/22], Loss: 0.0332\n",
      "Epoch [391/500], Step [10/22], Loss: 0.1358\n",
      "Epoch [391/500], Step [20/22], Loss: 0.0423\n",
      "Epoch [392/500], Step [10/22], Loss: 0.1031\n",
      "Epoch [392/500], Step [20/22], Loss: 0.0311\n",
      "Epoch [393/500], Step [10/22], Loss: 0.1050\n",
      "Epoch [393/500], Step [20/22], Loss: 0.0403\n",
      "Epoch [394/500], Step [10/22], Loss: 0.1076\n",
      "Epoch [394/500], Step [20/22], Loss: 0.0174\n",
      "Epoch [395/500], Step [10/22], Loss: 0.0918\n",
      "Epoch [395/500], Step [20/22], Loss: 0.0207\n",
      "Epoch [396/500], Step [10/22], Loss: 0.0991\n",
      "Epoch [396/500], Step [20/22], Loss: 0.0203\n",
      "Epoch [397/500], Step [10/22], Loss: 0.0904\n",
      "Epoch [397/500], Step [20/22], Loss: 0.0240\n",
      "Epoch [398/500], Step [10/22], Loss: 0.0925\n",
      "Epoch [398/500], Step [20/22], Loss: 0.0538\n",
      "Epoch [399/500], Step [10/22], Loss: 0.1052\n",
      "Epoch [399/500], Step [20/22], Loss: 0.1149\n",
      "Epoch [400/500], Step [10/22], Loss: 0.0931\n",
      "Epoch [400/500], Step [20/22], Loss: 0.0791\n",
      "Epoch [401/500], Step [10/22], Loss: 0.1113\n",
      "Epoch [401/500], Step [20/22], Loss: 0.0265\n",
      "Epoch [402/500], Step [10/22], Loss: 0.1441\n",
      "Epoch [402/500], Step [20/22], Loss: 0.0217\n",
      "Epoch [403/500], Step [10/22], Loss: 0.1063\n",
      "Epoch [403/500], Step [20/22], Loss: 0.0359\n",
      "Epoch [404/500], Step [10/22], Loss: 0.1287\n",
      "Epoch [404/500], Step [20/22], Loss: 0.0377\n",
      "Epoch [405/500], Step [10/22], Loss: 0.1038\n",
      "Epoch [405/500], Step [20/22], Loss: 0.0421\n",
      "Epoch [406/500], Step [10/22], Loss: 0.1395\n",
      "Epoch [406/500], Step [20/22], Loss: 0.0406\n",
      "Epoch [407/500], Step [10/22], Loss: 0.1848\n",
      "Epoch [407/500], Step [20/22], Loss: 0.0458\n",
      "Epoch [408/500], Step [10/22], Loss: 0.1307\n",
      "Epoch [408/500], Step [20/22], Loss: 0.0361\n",
      "Epoch [409/500], Step [10/22], Loss: 0.1220\n",
      "Epoch [409/500], Step [20/22], Loss: 0.0199\n",
      "Epoch [410/500], Step [10/22], Loss: 0.1341\n",
      "Epoch [410/500], Step [20/22], Loss: 0.0291\n",
      "Epoch [411/500], Step [10/22], Loss: 0.1023\n",
      "Epoch [411/500], Step [20/22], Loss: 0.0367\n",
      "Epoch [412/500], Step [10/22], Loss: 0.0961\n",
      "Epoch [412/500], Step [20/22], Loss: 0.0312\n",
      "Epoch [413/500], Step [10/22], Loss: 0.0920\n",
      "Epoch [413/500], Step [20/22], Loss: 0.0411\n",
      "Epoch [414/500], Step [10/22], Loss: 0.0919\n",
      "Epoch [414/500], Step [20/22], Loss: 0.0236\n",
      "Epoch [415/500], Step [10/22], Loss: 0.0864\n",
      "Epoch [415/500], Step [20/22], Loss: 0.0325\n",
      "Epoch [416/500], Step [10/22], Loss: 0.0834\n",
      "Epoch [416/500], Step [20/22], Loss: 0.0206\n",
      "Epoch [417/500], Step [10/22], Loss: 0.0890\n",
      "Epoch [417/500], Step [20/22], Loss: 0.0290\n",
      "Epoch [418/500], Step [10/22], Loss: 0.0791\n",
      "Epoch [418/500], Step [20/22], Loss: 0.0186\n",
      "Epoch [419/500], Step [10/22], Loss: 0.0850\n",
      "Epoch [419/500], Step [20/22], Loss: 0.0180\n",
      "Epoch [420/500], Step [10/22], Loss: 0.0875\n",
      "Epoch [420/500], Step [20/22], Loss: 0.0372\n",
      "Epoch [421/500], Step [10/22], Loss: 0.0831\n",
      "Epoch [421/500], Step [20/22], Loss: 0.0805\n",
      "Epoch [422/500], Step [10/22], Loss: 0.1088\n",
      "Epoch [422/500], Step [20/22], Loss: 0.0407\n",
      "Epoch [423/500], Step [10/22], Loss: 0.0834\n",
      "Epoch [423/500], Step [20/22], Loss: 0.0379\n",
      "Epoch [424/500], Step [10/22], Loss: 0.0842\n",
      "Epoch [424/500], Step [20/22], Loss: 0.0276\n",
      "Epoch [425/500], Step [10/22], Loss: 0.1434\n",
      "Epoch [425/500], Step [20/22], Loss: 0.0161\n",
      "Epoch [426/500], Step [10/22], Loss: 0.1342\n",
      "Epoch [426/500], Step [20/22], Loss: 0.0385\n",
      "Epoch [427/500], Step [10/22], Loss: 0.1030\n",
      "Epoch [427/500], Step [20/22], Loss: 0.0218\n",
      "Epoch [428/500], Step [10/22], Loss: 0.0868\n",
      "Epoch [428/500], Step [20/22], Loss: 0.0162\n",
      "Epoch [429/500], Step [10/22], Loss: 0.0849\n",
      "Epoch [429/500], Step [20/22], Loss: 0.0389\n",
      "Epoch [430/500], Step [10/22], Loss: 0.0908\n",
      "Epoch [430/500], Step [20/22], Loss: 0.0260\n",
      "Epoch [431/500], Step [10/22], Loss: 0.0796\n",
      "Epoch [431/500], Step [20/22], Loss: 0.0170\n",
      "Epoch [432/500], Step [10/22], Loss: 0.0776\n",
      "Epoch [432/500], Step [20/22], Loss: 0.0312\n",
      "Epoch [433/500], Step [10/22], Loss: 0.0895\n",
      "Epoch [433/500], Step [20/22], Loss: 0.0145\n",
      "Epoch [434/500], Step [10/22], Loss: 0.0747\n",
      "Epoch [434/500], Step [20/22], Loss: 0.0317\n",
      "Epoch [435/500], Step [10/22], Loss: 0.0799\n",
      "Epoch [435/500], Step [20/22], Loss: 0.0171\n",
      "Epoch [436/500], Step [10/22], Loss: 0.0819\n",
      "Epoch [436/500], Step [20/22], Loss: 0.0255\n",
      "Epoch [437/500], Step [10/22], Loss: 0.0710\n",
      "Epoch [437/500], Step [20/22], Loss: 0.1637\n",
      "Epoch [438/500], Step [10/22], Loss: 0.1496\n",
      "Epoch [438/500], Step [20/22], Loss: 0.0483\n",
      "Epoch [439/500], Step [10/22], Loss: 0.1251\n",
      "Epoch [439/500], Step [20/22], Loss: 0.0314\n",
      "Epoch [440/500], Step [10/22], Loss: 0.1149\n",
      "Epoch [440/500], Step [20/22], Loss: 0.0354\n",
      "Epoch [441/500], Step [10/22], Loss: 0.1050\n",
      "Epoch [441/500], Step [20/22], Loss: 0.0606\n",
      "Epoch [442/500], Step [10/22], Loss: 0.2108\n",
      "Epoch [442/500], Step [20/22], Loss: 0.0268\n",
      "Epoch [443/500], Step [10/22], Loss: 0.1277\n",
      "Epoch [443/500], Step [20/22], Loss: 0.0984\n",
      "Epoch [444/500], Step [10/22], Loss: 0.1086\n",
      "Epoch [444/500], Step [20/22], Loss: 0.0283\n",
      "Epoch [445/500], Step [10/22], Loss: 0.1503\n",
      "Epoch [445/500], Step [20/22], Loss: 0.0253\n",
      "Epoch [446/500], Step [10/22], Loss: 0.0874\n",
      "Epoch [446/500], Step [20/22], Loss: 0.0309\n",
      "Epoch [447/500], Step [10/22], Loss: 0.0809\n",
      "Epoch [447/500], Step [20/22], Loss: 0.0183\n",
      "Epoch [448/500], Step [10/22], Loss: 0.0749\n",
      "Epoch [448/500], Step [20/22], Loss: 0.0290\n",
      "Epoch [449/500], Step [10/22], Loss: 0.0797\n",
      "Epoch [449/500], Step [20/22], Loss: 0.0157\n",
      "Epoch [450/500], Step [10/22], Loss: 0.0719\n",
      "Epoch [450/500], Step [20/22], Loss: 0.0223\n",
      "Epoch [451/500], Step [10/22], Loss: 0.0770\n",
      "Epoch [451/500], Step [20/22], Loss: 0.0143\n",
      "Epoch [452/500], Step [10/22], Loss: 0.0686\n",
      "Epoch [452/500], Step [20/22], Loss: 0.0173\n",
      "Epoch [453/500], Step [10/22], Loss: 0.0659\n",
      "Epoch [453/500], Step [20/22], Loss: 0.0240\n",
      "Epoch [454/500], Step [10/22], Loss: 0.0703\n",
      "Epoch [454/500], Step [20/22], Loss: 0.0347\n",
      "Epoch [455/500], Step [10/22], Loss: 0.0657\n",
      "Epoch [455/500], Step [20/22], Loss: 0.0110\n",
      "Epoch [456/500], Step [10/22], Loss: 0.0743\n",
      "Epoch [456/500], Step [20/22], Loss: 0.0133\n",
      "Epoch [457/500], Step [10/22], Loss: 0.0654\n",
      "Epoch [457/500], Step [20/22], Loss: 0.0306\n",
      "Epoch [458/500], Step [10/22], Loss: 0.0705\n",
      "Epoch [458/500], Step [20/22], Loss: 0.0113\n",
      "Epoch [459/500], Step [10/22], Loss: 0.0582\n",
      "Epoch [459/500], Step [20/22], Loss: 0.0129\n",
      "Epoch [460/500], Step [10/22], Loss: 0.0652\n",
      "Epoch [460/500], Step [20/22], Loss: 0.0100\n",
      "Epoch [461/500], Step [10/22], Loss: 0.0596\n",
      "Epoch [461/500], Step [20/22], Loss: 0.0131\n",
      "Epoch [462/500], Step [10/22], Loss: 0.0585\n",
      "Epoch [462/500], Step [20/22], Loss: 0.0202\n",
      "Epoch [463/500], Step [10/22], Loss: 0.0866\n",
      "Epoch [463/500], Step [20/22], Loss: 0.0135\n",
      "Epoch [464/500], Step [10/22], Loss: 0.0621\n",
      "Epoch [464/500], Step [20/22], Loss: 0.0297\n",
      "Epoch [465/500], Step [10/22], Loss: 0.0614\n",
      "Epoch [465/500], Step [20/22], Loss: 0.0209\n",
      "Epoch [466/500], Step [10/22], Loss: 0.0599\n",
      "Epoch [466/500], Step [20/22], Loss: 0.0116\n",
      "Epoch [467/500], Step [10/22], Loss: 0.0603\n",
      "Epoch [467/500], Step [20/22], Loss: 0.0111\n",
      "Epoch [468/500], Step [10/22], Loss: 0.0637\n",
      "Epoch [468/500], Step [20/22], Loss: 0.0270\n",
      "Epoch [469/500], Step [10/22], Loss: 0.0530\n",
      "Epoch [469/500], Step [20/22], Loss: 0.0136\n",
      "Epoch [470/500], Step [10/22], Loss: 0.0535\n",
      "Epoch [470/500], Step [20/22], Loss: 0.0063\n",
      "Epoch [471/500], Step [10/22], Loss: 0.0715\n",
      "Epoch [471/500], Step [20/22], Loss: 0.0448\n",
      "Epoch [472/500], Step [10/22], Loss: 0.0801\n",
      "Epoch [472/500], Step [20/22], Loss: 0.0076\n",
      "Epoch [473/500], Step [10/22], Loss: 0.0770\n",
      "Epoch [473/500], Step [20/22], Loss: 0.0150\n",
      "Epoch [474/500], Step [10/22], Loss: 0.0637\n",
      "Epoch [474/500], Step [20/22], Loss: 0.0517\n",
      "Epoch [475/500], Step [10/22], Loss: 0.2304\n",
      "Epoch [475/500], Step [20/22], Loss: 0.1147\n",
      "Epoch [476/500], Step [10/22], Loss: 0.1057\n",
      "Epoch [476/500], Step [20/22], Loss: 0.0680\n",
      "Epoch [477/500], Step [10/22], Loss: 0.2740\n",
      "Epoch [477/500], Step [20/22], Loss: 0.0445\n",
      "Epoch [478/500], Step [10/22], Loss: 0.1424\n",
      "Epoch [478/500], Step [20/22], Loss: 0.0502\n",
      "Epoch [479/500], Step [10/22], Loss: 0.1691\n",
      "Epoch [479/500], Step [20/22], Loss: 0.0478\n",
      "Epoch [480/500], Step [10/22], Loss: 0.2301\n",
      "Epoch [480/500], Step [20/22], Loss: 0.0563\n",
      "Epoch [481/500], Step [10/22], Loss: 0.2427\n",
      "Epoch [481/500], Step [20/22], Loss: 0.0361\n",
      "Epoch [482/500], Step [10/22], Loss: 0.1404\n",
      "Epoch [482/500], Step [20/22], Loss: 0.0560\n",
      "Epoch [483/500], Step [10/22], Loss: 0.1416\n",
      "Epoch [483/500], Step [20/22], Loss: 0.0207\n",
      "Epoch [484/500], Step [10/22], Loss: 0.0998\n",
      "Epoch [484/500], Step [20/22], Loss: 0.0163\n",
      "Epoch [485/500], Step [10/22], Loss: 0.0749\n",
      "Epoch [485/500], Step [20/22], Loss: 0.0180\n",
      "Epoch [486/500], Step [10/22], Loss: 0.0668\n",
      "Epoch [486/500], Step [20/22], Loss: 0.0162\n",
      "Epoch [487/500], Step [10/22], Loss: 0.0604\n",
      "Epoch [487/500], Step [20/22], Loss: 0.0171\n",
      "Epoch [488/500], Step [10/22], Loss: 0.0556\n",
      "Epoch [488/500], Step [20/22], Loss: 0.0155\n",
      "Epoch [489/500], Step [10/22], Loss: 0.0543\n",
      "Epoch [489/500], Step [20/22], Loss: 0.0148\n",
      "Epoch [490/500], Step [10/22], Loss: 0.0503\n",
      "Epoch [490/500], Step [20/22], Loss: 0.0136\n",
      "Epoch [491/500], Step [10/22], Loss: 0.0527\n",
      "Epoch [491/500], Step [20/22], Loss: 0.0134\n",
      "Epoch [492/500], Step [10/22], Loss: 0.0468\n",
      "Epoch [492/500], Step [20/22], Loss: 0.0112\n",
      "Epoch [493/500], Step [10/22], Loss: 0.0509\n",
      "Epoch [493/500], Step [20/22], Loss: 0.0117\n",
      "Epoch [494/500], Step [10/22], Loss: 0.0439\n",
      "Epoch [494/500], Step [20/22], Loss: 0.0088\n",
      "Epoch [495/500], Step [10/22], Loss: 0.0440\n",
      "Epoch [495/500], Step [20/22], Loss: 0.0082\n",
      "Epoch [496/500], Step [10/22], Loss: 0.0391\n",
      "Epoch [496/500], Step [20/22], Loss: 0.0161\n",
      "Epoch [497/500], Step [10/22], Loss: 0.0463\n",
      "Epoch [497/500], Step [20/22], Loss: 0.0068\n",
      "Epoch [498/500], Step [10/22], Loss: 0.0485\n",
      "Epoch [498/500], Step [20/22], Loss: 0.0057\n",
      "Epoch [499/500], Step [10/22], Loss: 0.0514\n",
      "Epoch [499/500], Step [20/22], Loss: 0.0048\n",
      "Epoch [500/500], Step [10/22], Loss: 0.0384\n",
      "Epoch [500/500], Step [20/22], Loss: 0.0362\n",
      "Epoch [1/500], Step [10/22], Loss: 0.7573\n",
      "Epoch [1/500], Step [20/22], Loss: 0.6492\n",
      "Epoch [2/500], Step [10/22], Loss: 0.7433\n",
      "Epoch [2/500], Step [20/22], Loss: 0.6030\n",
      "Epoch [3/500], Step [10/22], Loss: 0.7482\n",
      "Epoch [3/500], Step [20/22], Loss: 0.5271\n",
      "Epoch [4/500], Step [10/22], Loss: 0.7389\n",
      "Epoch [4/500], Step [20/22], Loss: 0.3787\n",
      "Epoch [5/500], Step [10/22], Loss: 0.6553\n",
      "Epoch [5/500], Step [20/22], Loss: 0.2644\n",
      "Epoch [6/500], Step [10/22], Loss: 0.6159\n",
      "Epoch [6/500], Step [20/22], Loss: 0.1869\n",
      "Epoch [7/500], Step [10/22], Loss: 0.6132\n",
      "Epoch [7/500], Step [20/22], Loss: 0.3268\n",
      "Epoch [8/500], Step [10/22], Loss: 0.6192\n",
      "Epoch [8/500], Step [20/22], Loss: 0.1745\n",
      "Epoch [9/500], Step [10/22], Loss: 0.6044\n",
      "Epoch [9/500], Step [20/22], Loss: 0.4295\n",
      "Epoch [10/500], Step [10/22], Loss: 0.6184\n",
      "Epoch [10/500], Step [20/22], Loss: 0.2745\n",
      "Epoch [11/500], Step [10/22], Loss: 0.5868\n",
      "Epoch [11/500], Step [20/22], Loss: 0.2380\n",
      "Epoch [12/500], Step [10/22], Loss: 0.5469\n",
      "Epoch [12/500], Step [20/22], Loss: 0.2197\n",
      "Epoch [13/500], Step [10/22], Loss: 0.5037\n",
      "Epoch [13/500], Step [20/22], Loss: 0.1789\n",
      "Epoch [14/500], Step [10/22], Loss: 0.4562\n",
      "Epoch [14/500], Step [20/22], Loss: 0.1666\n",
      "Epoch [15/500], Step [10/22], Loss: 0.6250\n",
      "Epoch [15/500], Step [20/22], Loss: 0.1456\n",
      "Epoch [16/500], Step [10/22], Loss: 0.4680\n",
      "Epoch [16/500], Step [20/22], Loss: 0.1492\n",
      "Epoch [17/500], Step [10/22], Loss: 0.4304\n",
      "Epoch [17/500], Step [20/22], Loss: 0.2994\n",
      "Epoch [18/500], Step [10/22], Loss: 0.6772\n",
      "Epoch [18/500], Step [20/22], Loss: 0.2090\n",
      "Epoch [19/500], Step [10/22], Loss: 0.4315\n",
      "Epoch [19/500], Step [20/22], Loss: 0.1802\n",
      "Epoch [20/500], Step [10/22], Loss: 0.6217\n",
      "Epoch [20/500], Step [20/22], Loss: 0.1609\n",
      "Epoch [21/500], Step [10/22], Loss: 0.4563\n",
      "Epoch [21/500], Step [20/22], Loss: 0.1503\n",
      "Epoch [22/500], Step [10/22], Loss: 0.5050\n",
      "Epoch [22/500], Step [20/22], Loss: 0.1485\n",
      "Epoch [23/500], Step [10/22], Loss: 0.4409\n",
      "Epoch [23/500], Step [20/22], Loss: 0.1469\n",
      "Epoch [24/500], Step [10/22], Loss: 0.4074\n",
      "Epoch [24/500], Step [20/22], Loss: 0.1506\n",
      "Epoch [25/500], Step [10/22], Loss: 0.3997\n",
      "Epoch [25/500], Step [20/22], Loss: 0.2906\n",
      "Epoch [26/500], Step [10/22], Loss: 0.5737\n",
      "Epoch [26/500], Step [20/22], Loss: 0.1987\n",
      "Epoch [27/500], Step [10/22], Loss: 0.4014\n",
      "Epoch [27/500], Step [20/22], Loss: 0.1705\n",
      "Epoch [28/500], Step [10/22], Loss: 0.6015\n",
      "Epoch [28/500], Step [20/22], Loss: 0.1552\n",
      "Epoch [29/500], Step [10/22], Loss: 0.4475\n",
      "Epoch [29/500], Step [20/22], Loss: 0.1478\n",
      "Epoch [30/500], Step [10/22], Loss: 0.4020\n",
      "Epoch [30/500], Step [20/22], Loss: 0.1783\n",
      "Epoch [31/500], Step [10/22], Loss: 0.8187\n",
      "Epoch [31/500], Step [20/22], Loss: 0.1488\n",
      "Epoch [32/500], Step [10/22], Loss: 0.5401\n",
      "Epoch [32/500], Step [20/22], Loss: 0.1431\n",
      "Epoch [33/500], Step [10/22], Loss: 0.4742\n",
      "Epoch [33/500], Step [20/22], Loss: 0.1409\n",
      "Epoch [34/500], Step [10/22], Loss: 0.4120\n",
      "Epoch [34/500], Step [20/22], Loss: 0.1423\n",
      "Epoch [35/500], Step [10/22], Loss: 0.4506\n",
      "Epoch [35/500], Step [20/22], Loss: 0.1410\n",
      "Epoch [36/500], Step [10/22], Loss: 0.3763\n",
      "Epoch [36/500], Step [20/22], Loss: 0.1406\n",
      "Epoch [37/500], Step [10/22], Loss: 0.4055\n",
      "Epoch [37/500], Step [20/22], Loss: 0.1399\n",
      "Epoch [38/500], Step [10/22], Loss: 0.3771\n",
      "Epoch [38/500], Step [20/22], Loss: 0.3833\n",
      "Epoch [39/500], Step [10/22], Loss: 0.8701\n",
      "Epoch [39/500], Step [20/22], Loss: 0.2599\n",
      "Epoch [40/500], Step [10/22], Loss: 0.7546\n",
      "Epoch [40/500], Step [20/22], Loss: 0.2103\n",
      "Epoch [41/500], Step [10/22], Loss: 0.6606\n",
      "Epoch [41/500], Step [20/22], Loss: 0.1557\n",
      "Epoch [42/500], Step [10/22], Loss: 0.5437\n",
      "Epoch [42/500], Step [20/22], Loss: 0.1470\n",
      "Epoch [43/500], Step [10/22], Loss: 0.4567\n",
      "Epoch [43/500], Step [20/22], Loss: 0.1425\n",
      "Epoch [44/500], Step [10/22], Loss: 0.3919\n",
      "Epoch [44/500], Step [20/22], Loss: 0.1416\n",
      "Epoch [45/500], Step [10/22], Loss: 0.4078\n",
      "Epoch [45/500], Step [20/22], Loss: 0.1401\n",
      "Epoch [46/500], Step [10/22], Loss: 0.3777\n",
      "Epoch [46/500], Step [20/22], Loss: 0.1400\n",
      "Epoch [47/500], Step [10/22], Loss: 0.3319\n",
      "Epoch [47/500], Step [20/22], Loss: 0.1387\n",
      "Epoch [48/500], Step [10/22], Loss: 0.4662\n",
      "Epoch [48/500], Step [20/22], Loss: 0.1390\n",
      "Epoch [49/500], Step [10/22], Loss: 0.3700\n",
      "Epoch [49/500], Step [20/22], Loss: 0.2715\n",
      "Epoch [50/500], Step [10/22], Loss: 0.8704\n",
      "Epoch [50/500], Step [20/22], Loss: 0.1866\n",
      "Epoch [51/500], Step [10/22], Loss: 0.6590\n",
      "Epoch [51/500], Step [20/22], Loss: 0.1767\n",
      "Epoch [52/500], Step [10/22], Loss: 0.5755\n",
      "Epoch [52/500], Step [20/22], Loss: 0.1410\n",
      "Epoch [53/500], Step [10/22], Loss: 0.4027\n",
      "Epoch [53/500], Step [20/22], Loss: 0.1399\n",
      "Epoch [54/500], Step [10/22], Loss: 0.6578\n",
      "Epoch [54/500], Step [20/22], Loss: 0.1406\n",
      "Epoch [55/500], Step [10/22], Loss: 0.5586\n",
      "Epoch [55/500], Step [20/22], Loss: 0.2122\n",
      "Epoch [56/500], Step [10/22], Loss: 0.4751\n",
      "Epoch [56/500], Step [20/22], Loss: 0.1393\n",
      "Epoch [57/500], Step [10/22], Loss: 0.3813\n",
      "Epoch [57/500], Step [20/22], Loss: 0.1790\n",
      "Epoch [58/500], Step [10/22], Loss: 0.7398\n",
      "Epoch [58/500], Step [20/22], Loss: 0.1430\n",
      "Epoch [59/500], Step [10/22], Loss: 0.6352\n",
      "Epoch [59/500], Step [20/22], Loss: 0.1615\n",
      "Epoch [60/500], Step [10/22], Loss: 0.5736\n",
      "Epoch [60/500], Step [20/22], Loss: 0.1398\n",
      "Epoch [61/500], Step [10/22], Loss: 0.5043\n",
      "Epoch [61/500], Step [20/22], Loss: 0.1394\n",
      "Epoch [62/500], Step [10/22], Loss: 0.4360\n",
      "Epoch [62/500], Step [20/22], Loss: 0.1390\n",
      "Epoch [63/500], Step [10/22], Loss: 0.3656\n",
      "Epoch [63/500], Step [20/22], Loss: 0.1390\n",
      "Epoch [64/500], Step [10/22], Loss: 0.3398\n",
      "Epoch [64/500], Step [20/22], Loss: 0.1389\n",
      "Epoch [65/500], Step [10/22], Loss: 0.3510\n",
      "Epoch [65/500], Step [20/22], Loss: 0.1389\n",
      "Epoch [66/500], Step [10/22], Loss: 0.3227\n",
      "Epoch [66/500], Step [20/22], Loss: 0.1386\n",
      "Epoch [67/500], Step [10/22], Loss: 0.3062\n",
      "Epoch [67/500], Step [20/22], Loss: 0.1383\n",
      "Epoch [68/500], Step [10/22], Loss: 0.3870\n",
      "Epoch [68/500], Step [20/22], Loss: 0.1393\n",
      "Epoch [69/500], Step [10/22], Loss: 0.5598\n",
      "Epoch [69/500], Step [20/22], Loss: 0.1459\n",
      "Epoch [70/500], Step [10/22], Loss: 0.3739\n",
      "Epoch [70/500], Step [20/22], Loss: 0.1392\n",
      "Epoch [71/500], Step [10/22], Loss: 0.3642\n",
      "Epoch [71/500], Step [20/22], Loss: 0.1430\n",
      "Epoch [72/500], Step [10/22], Loss: 0.5597\n",
      "Epoch [72/500], Step [20/22], Loss: 0.1387\n",
      "Epoch [73/500], Step [10/22], Loss: 0.3700\n",
      "Epoch [73/500], Step [20/22], Loss: 0.1440\n",
      "Epoch [74/500], Step [10/22], Loss: 0.5378\n",
      "Epoch [74/500], Step [20/22], Loss: 0.1395\n",
      "Epoch [75/500], Step [10/22], Loss: 0.3858\n",
      "Epoch [75/500], Step [20/22], Loss: 0.1712\n",
      "Epoch [76/500], Step [10/22], Loss: 0.3939\n",
      "Epoch [76/500], Step [20/22], Loss: 0.1383\n",
      "Epoch [77/500], Step [10/22], Loss: 0.3309\n",
      "Epoch [77/500], Step [20/22], Loss: 0.1653\n",
      "Epoch [78/500], Step [10/22], Loss: 0.6296\n",
      "Epoch [78/500], Step [20/22], Loss: 0.1418\n",
      "Epoch [79/500], Step [10/22], Loss: 0.4029\n",
      "Epoch [79/500], Step [20/22], Loss: 0.1357\n",
      "Epoch [80/500], Step [10/22], Loss: 0.5976\n",
      "Epoch [80/500], Step [20/22], Loss: 0.1403\n",
      "Epoch [81/500], Step [10/22], Loss: 0.4693\n",
      "Epoch [81/500], Step [20/22], Loss: 0.1344\n",
      "Epoch [82/500], Step [10/22], Loss: 0.3720\n",
      "Epoch [82/500], Step [20/22], Loss: 0.1458\n",
      "Epoch [83/500], Step [10/22], Loss: 0.3133\n",
      "Epoch [83/500], Step [20/22], Loss: 0.1387\n",
      "Epoch [84/500], Step [10/22], Loss: 0.4285\n",
      "Epoch [84/500], Step [20/22], Loss: 0.1391\n",
      "Epoch [85/500], Step [10/22], Loss: 0.3062\n",
      "Epoch [85/500], Step [20/22], Loss: 0.1434\n",
      "Epoch [86/500], Step [10/22], Loss: 0.2840\n",
      "Epoch [86/500], Step [20/22], Loss: 0.1336\n",
      "Epoch [87/500], Step [10/22], Loss: 0.3791\n",
      "Epoch [87/500], Step [20/22], Loss: 0.1413\n",
      "Epoch [88/500], Step [10/22], Loss: 0.3517\n",
      "Epoch [88/500], Step [20/22], Loss: 0.1400\n",
      "Epoch [89/500], Step [10/22], Loss: 0.3317\n",
      "Epoch [89/500], Step [20/22], Loss: 0.1378\n",
      "Epoch [90/500], Step [10/22], Loss: 0.2985\n",
      "Epoch [90/500], Step [20/22], Loss: 0.1365\n",
      "Epoch [91/500], Step [10/22], Loss: 0.4518\n",
      "Epoch [91/500], Step [20/22], Loss: 0.1363\n",
      "Epoch [92/500], Step [10/22], Loss: 0.2681\n",
      "Epoch [92/500], Step [20/22], Loss: 0.1373\n",
      "Epoch [93/500], Step [10/22], Loss: 0.2628\n",
      "Epoch [93/500], Step [20/22], Loss: 0.1327\n",
      "Epoch [94/500], Step [10/22], Loss: 0.3360\n",
      "Epoch [94/500], Step [20/22], Loss: 0.1272\n",
      "Epoch [95/500], Step [10/22], Loss: 0.6139\n",
      "Epoch [95/500], Step [20/22], Loss: 0.1229\n",
      "Epoch [96/500], Step [10/22], Loss: 0.3597\n",
      "Epoch [96/500], Step [20/22], Loss: 0.1383\n",
      "Epoch [97/500], Step [10/22], Loss: 0.2769\n",
      "Epoch [97/500], Step [20/22], Loss: 0.1298\n",
      "Epoch [98/500], Step [10/22], Loss: 0.3621\n",
      "Epoch [98/500], Step [20/22], Loss: 0.1363\n",
      "Epoch [99/500], Step [10/22], Loss: 0.2376\n",
      "Epoch [99/500], Step [20/22], Loss: 0.1380\n",
      "Epoch [100/500], Step [10/22], Loss: 0.2483\n",
      "Epoch [100/500], Step [20/22], Loss: 0.1344\n",
      "Epoch [101/500], Step [10/22], Loss: 0.2760\n",
      "Epoch [101/500], Step [20/22], Loss: 0.1350\n",
      "Epoch [102/500], Step [10/22], Loss: 0.2217\n",
      "Epoch [102/500], Step [20/22], Loss: 0.1344\n",
      "Epoch [103/500], Step [10/22], Loss: 0.4417\n",
      "Epoch [103/500], Step [20/22], Loss: 0.1350\n",
      "Epoch [104/500], Step [10/22], Loss: 0.3954\n",
      "Epoch [104/500], Step [20/22], Loss: 0.1579\n",
      "Epoch [105/500], Step [10/22], Loss: 0.2721\n",
      "Epoch [105/500], Step [20/22], Loss: 0.1406\n",
      "Epoch [106/500], Step [10/22], Loss: 0.2932\n",
      "Epoch [106/500], Step [20/22], Loss: 0.1372\n",
      "Epoch [107/500], Step [10/22], Loss: 0.7424\n",
      "Epoch [107/500], Step [20/22], Loss: 0.1392\n",
      "Epoch [108/500], Step [10/22], Loss: 0.4238\n",
      "Epoch [108/500], Step [20/22], Loss: 0.1393\n",
      "Epoch [109/500], Step [10/22], Loss: 0.3516\n",
      "Epoch [109/500], Step [20/22], Loss: 0.1413\n",
      "Epoch [110/500], Step [10/22], Loss: 0.2940\n",
      "Epoch [110/500], Step [20/22], Loss: 0.1445\n",
      "Epoch [111/500], Step [10/22], Loss: 0.4350\n",
      "Epoch [111/500], Step [20/22], Loss: 0.1286\n",
      "Epoch [112/500], Step [10/22], Loss: 0.2897\n",
      "Epoch [112/500], Step [20/22], Loss: 0.1379\n",
      "Epoch [113/500], Step [10/22], Loss: 0.3337\n",
      "Epoch [113/500], Step [20/22], Loss: 0.1365\n",
      "Epoch [114/500], Step [10/22], Loss: 0.3973\n",
      "Epoch [114/500], Step [20/22], Loss: 0.1310\n",
      "Epoch [115/500], Step [10/22], Loss: 0.2899\n",
      "Epoch [115/500], Step [20/22], Loss: 0.1314\n",
      "Epoch [116/500], Step [10/22], Loss: 0.4928\n",
      "Epoch [116/500], Step [20/22], Loss: 0.1246\n",
      "Epoch [117/500], Step [10/22], Loss: 0.2745\n",
      "Epoch [117/500], Step [20/22], Loss: 0.1509\n",
      "Epoch [118/500], Step [10/22], Loss: 0.2643\n",
      "Epoch [118/500], Step [20/22], Loss: 0.1296\n",
      "Epoch [119/500], Step [10/22], Loss: 0.2629\n",
      "Epoch [119/500], Step [20/22], Loss: 0.1253\n",
      "Epoch [120/500], Step [10/22], Loss: 0.2125\n",
      "Epoch [120/500], Step [20/22], Loss: 0.1124\n",
      "Epoch [121/500], Step [10/22], Loss: 0.3339\n",
      "Epoch [121/500], Step [20/22], Loss: 0.1376\n",
      "Epoch [122/500], Step [10/22], Loss: 0.2893\n",
      "Epoch [122/500], Step [20/22], Loss: 0.1296\n",
      "Epoch [123/500], Step [10/22], Loss: 0.3347\n",
      "Epoch [123/500], Step [20/22], Loss: 0.1203\n",
      "Epoch [124/500], Step [10/22], Loss: 0.3322\n",
      "Epoch [124/500], Step [20/22], Loss: 0.1235\n",
      "Epoch [125/500], Step [10/22], Loss: 0.5865\n",
      "Epoch [125/500], Step [20/22], Loss: 0.1315\n",
      "Epoch [126/500], Step [10/22], Loss: 0.3340\n",
      "Epoch [126/500], Step [20/22], Loss: 0.1104\n",
      "Epoch [127/500], Step [10/22], Loss: 0.3817\n",
      "Epoch [127/500], Step [20/22], Loss: 0.1150\n",
      "Epoch [128/500], Step [10/22], Loss: 0.2271\n",
      "Epoch [128/500], Step [20/22], Loss: 0.1198\n",
      "Epoch [129/500], Step [10/22], Loss: 0.4801\n",
      "Epoch [129/500], Step [20/22], Loss: 0.0872\n",
      "Epoch [130/500], Step [10/22], Loss: 0.2620\n",
      "Epoch [130/500], Step [20/22], Loss: 0.1258\n",
      "Epoch [131/500], Step [10/22], Loss: 0.2736\n",
      "Epoch [131/500], Step [20/22], Loss: 0.1098\n",
      "Epoch [132/500], Step [10/22], Loss: 0.2038\n",
      "Epoch [132/500], Step [20/22], Loss: 0.1125\n",
      "Epoch [133/500], Step [10/22], Loss: 0.3342\n",
      "Epoch [133/500], Step [20/22], Loss: 0.0825\n",
      "Epoch [134/500], Step [10/22], Loss: 0.2324\n",
      "Epoch [134/500], Step [20/22], Loss: 0.0721\n",
      "Epoch [135/500], Step [10/22], Loss: 0.2661\n",
      "Epoch [135/500], Step [20/22], Loss: 0.0684\n",
      "Epoch [136/500], Step [10/22], Loss: 0.1973\n",
      "Epoch [136/500], Step [20/22], Loss: 0.1226\n",
      "Epoch [137/500], Step [10/22], Loss: 0.3106\n",
      "Epoch [137/500], Step [20/22], Loss: 0.1169\n",
      "Epoch [138/500], Step [10/22], Loss: 0.2216\n",
      "Epoch [138/500], Step [20/22], Loss: 0.0992\n",
      "Epoch [139/500], Step [10/22], Loss: 0.1960\n",
      "Epoch [139/500], Step [20/22], Loss: 0.0757\n",
      "Epoch [140/500], Step [10/22], Loss: 0.1910\n",
      "Epoch [140/500], Step [20/22], Loss: 0.1195\n",
      "Epoch [141/500], Step [10/22], Loss: 0.3844\n",
      "Epoch [141/500], Step [20/22], Loss: 0.0780\n",
      "Epoch [142/500], Step [10/22], Loss: 0.2201\n",
      "Epoch [142/500], Step [20/22], Loss: 0.1265\n",
      "Epoch [143/500], Step [10/22], Loss: 0.2221\n",
      "Epoch [143/500], Step [20/22], Loss: 0.0779\n",
      "Epoch [144/500], Step [10/22], Loss: 0.2016\n",
      "Epoch [144/500], Step [20/22], Loss: 0.1056\n",
      "Epoch [145/500], Step [10/22], Loss: 0.3793\n",
      "Epoch [145/500], Step [20/22], Loss: 0.0639\n",
      "Epoch [146/500], Step [10/22], Loss: 0.2356\n",
      "Epoch [146/500], Step [20/22], Loss: 0.0980\n",
      "Epoch [147/500], Step [10/22], Loss: 0.1828\n",
      "Epoch [147/500], Step [20/22], Loss: 0.0566\n",
      "Epoch [148/500], Step [10/22], Loss: 0.1966\n",
      "Epoch [148/500], Step [20/22], Loss: 0.0582\n",
      "Epoch [149/500], Step [10/22], Loss: 0.3384\n",
      "Epoch [149/500], Step [20/22], Loss: 0.0372\n",
      "Epoch [150/500], Step [10/22], Loss: 0.1919\n",
      "Epoch [150/500], Step [20/22], Loss: 0.1221\n",
      "Epoch [151/500], Step [10/22], Loss: 0.2037\n",
      "Epoch [151/500], Step [20/22], Loss: 0.0793\n",
      "Epoch [152/500], Step [10/22], Loss: 0.2578\n",
      "Epoch [152/500], Step [20/22], Loss: 0.0361\n",
      "Epoch [153/500], Step [10/22], Loss: 0.2299\n",
      "Epoch [153/500], Step [20/22], Loss: 0.0356\n",
      "Epoch [154/500], Step [10/22], Loss: 0.1667\n",
      "Epoch [154/500], Step [20/22], Loss: 0.0358\n",
      "Epoch [155/500], Step [10/22], Loss: 0.1727\n",
      "Epoch [155/500], Step [20/22], Loss: 0.0616\n",
      "Epoch [156/500], Step [10/22], Loss: 0.3784\n",
      "Epoch [156/500], Step [20/22], Loss: 0.0613\n",
      "Epoch [157/500], Step [10/22], Loss: 0.2377\n",
      "Epoch [157/500], Step [20/22], Loss: 0.1085\n",
      "Epoch [158/500], Step [10/22], Loss: 0.2818\n",
      "Epoch [158/500], Step [20/22], Loss: 0.0609\n",
      "Epoch [159/500], Step [10/22], Loss: 0.2285\n",
      "Epoch [159/500], Step [20/22], Loss: 0.1103\n",
      "Epoch [160/500], Step [10/22], Loss: 0.4541\n",
      "Epoch [160/500], Step [20/22], Loss: 0.0632\n",
      "Epoch [161/500], Step [10/22], Loss: 0.2013\n",
      "Epoch [161/500], Step [20/22], Loss: 0.1507\n",
      "Epoch [162/500], Step [10/22], Loss: 0.2412\n",
      "Epoch [162/500], Step [20/22], Loss: 0.0540\n",
      "Epoch [163/500], Step [10/22], Loss: 0.1708\n",
      "Epoch [163/500], Step [20/22], Loss: 0.0455\n",
      "Epoch [164/500], Step [10/22], Loss: 0.2243\n",
      "Epoch [164/500], Step [20/22], Loss: 0.0238\n",
      "Epoch [165/500], Step [10/22], Loss: 0.2127\n",
      "Epoch [165/500], Step [20/22], Loss: 0.0190\n",
      "Epoch [166/500], Step [10/22], Loss: 0.2466\n",
      "Epoch [166/500], Step [20/22], Loss: 0.0408\n",
      "Epoch [167/500], Step [10/22], Loss: 0.1794\n",
      "Epoch [167/500], Step [20/22], Loss: 0.1174\n",
      "Epoch [168/500], Step [10/22], Loss: 0.1663\n",
      "Epoch [168/500], Step [20/22], Loss: 0.0694\n",
      "Epoch [169/500], Step [10/22], Loss: 0.4367\n",
      "Epoch [169/500], Step [20/22], Loss: 0.0744\n",
      "Epoch [170/500], Step [10/22], Loss: 0.2359\n",
      "Epoch [170/500], Step [20/22], Loss: 0.1555\n",
      "Epoch [171/500], Step [10/22], Loss: 0.2484\n",
      "Epoch [171/500], Step [20/22], Loss: 0.1247\n",
      "Epoch [172/500], Step [10/22], Loss: 0.1998\n",
      "Epoch [172/500], Step [20/22], Loss: 0.0978\n",
      "Epoch [173/500], Step [10/22], Loss: 0.1692\n",
      "Epoch [173/500], Step [20/22], Loss: 0.0766\n",
      "Epoch [174/500], Step [10/22], Loss: 0.3918\n",
      "Epoch [174/500], Step [20/22], Loss: 0.0693\n",
      "Epoch [175/500], Step [10/22], Loss: 0.2180\n",
      "Epoch [175/500], Step [20/22], Loss: 0.1514\n",
      "Epoch [176/500], Step [10/22], Loss: 0.1729\n",
      "Epoch [176/500], Step [20/22], Loss: 0.0772\n",
      "Epoch [177/500], Step [10/22], Loss: 0.4047\n",
      "Epoch [177/500], Step [20/22], Loss: 0.0820\n",
      "Epoch [178/500], Step [10/22], Loss: 0.2248\n",
      "Epoch [178/500], Step [20/22], Loss: 0.0950\n",
      "Epoch [179/500], Step [10/22], Loss: 0.2119\n",
      "Epoch [179/500], Step [20/22], Loss: 0.1237\n",
      "Epoch [180/500], Step [10/22], Loss: 0.1666\n",
      "Epoch [180/500], Step [20/22], Loss: 0.0936\n",
      "Epoch [181/500], Step [10/22], Loss: 0.2664\n",
      "Epoch [181/500], Step [20/22], Loss: 0.0512\n",
      "Epoch [182/500], Step [10/22], Loss: 0.2102\n",
      "Epoch [182/500], Step [20/22], Loss: 0.0676\n",
      "Epoch [183/500], Step [10/22], Loss: 0.2279\n",
      "Epoch [183/500], Step [20/22], Loss: 0.0506\n",
      "Epoch [184/500], Step [10/22], Loss: 0.1820\n",
      "Epoch [184/500], Step [20/22], Loss: 0.0518\n",
      "Epoch [185/500], Step [10/22], Loss: 0.2139\n",
      "Epoch [185/500], Step [20/22], Loss: 0.0372\n",
      "Epoch [186/500], Step [10/22], Loss: 0.1983\n",
      "Epoch [186/500], Step [20/22], Loss: 0.0336\n",
      "Epoch [187/500], Step [10/22], Loss: 0.2013\n",
      "Epoch [187/500], Step [20/22], Loss: 0.0280\n",
      "Epoch [188/500], Step [10/22], Loss: 0.1656\n",
      "Epoch [188/500], Step [20/22], Loss: 0.1635\n",
      "Epoch [189/500], Step [10/22], Loss: 0.1539\n",
      "Epoch [189/500], Step [20/22], Loss: 0.0810\n",
      "Epoch [190/500], Step [10/22], Loss: 0.3956\n",
      "Epoch [190/500], Step [20/22], Loss: 0.0935\n",
      "Epoch [191/500], Step [10/22], Loss: 0.5458\n",
      "Epoch [191/500], Step [20/22], Loss: 0.1617\n",
      "Epoch [192/500], Step [10/22], Loss: 0.3482\n",
      "Epoch [192/500], Step [20/22], Loss: 0.1268\n",
      "Epoch [193/500], Step [10/22], Loss: 0.2928\n",
      "Epoch [193/500], Step [20/22], Loss: 0.1201\n",
      "Epoch [194/500], Step [10/22], Loss: 0.4431\n",
      "Epoch [194/500], Step [20/22], Loss: 0.1018\n",
      "Epoch [195/500], Step [10/22], Loss: 0.2589\n",
      "Epoch [195/500], Step [20/22], Loss: 0.1107\n",
      "Epoch [196/500], Step [10/22], Loss: 0.2411\n",
      "Epoch [196/500], Step [20/22], Loss: 0.1284\n",
      "Epoch [197/500], Step [10/22], Loss: 0.3720\n",
      "Epoch [197/500], Step [20/22], Loss: 0.0598\n",
      "Epoch [198/500], Step [10/22], Loss: 0.2319\n",
      "Epoch [198/500], Step [20/22], Loss: 0.0484\n",
      "Epoch [199/500], Step [10/22], Loss: 0.2009\n",
      "Epoch [199/500], Step [20/22], Loss: 0.0504\n",
      "Epoch [200/500], Step [10/22], Loss: 0.4297\n",
      "Epoch [200/500], Step [20/22], Loss: 0.0353\n",
      "Epoch [201/500], Step [10/22], Loss: 0.1856\n",
      "Epoch [201/500], Step [20/22], Loss: 0.1523\n",
      "Epoch [202/500], Step [10/22], Loss: 0.2535\n",
      "Epoch [202/500], Step [20/22], Loss: 0.0186\n",
      "Epoch [203/500], Step [10/22], Loss: 0.1857\n",
      "Epoch [203/500], Step [20/22], Loss: 0.0236\n",
      "Epoch [204/500], Step [10/22], Loss: 0.1625\n",
      "Epoch [204/500], Step [20/22], Loss: 0.0147\n",
      "Epoch [205/500], Step [10/22], Loss: 0.1683\n",
      "Epoch [205/500], Step [20/22], Loss: 0.0178\n",
      "Epoch [206/500], Step [10/22], Loss: 0.2679\n",
      "Epoch [206/500], Step [20/22], Loss: 0.0498\n",
      "Epoch [207/500], Step [10/22], Loss: 0.1779\n",
      "Epoch [207/500], Step [20/22], Loss: 0.1426\n",
      "Epoch [208/500], Step [10/22], Loss: 0.1595\n",
      "Epoch [208/500], Step [20/22], Loss: 0.0286\n",
      "Epoch [209/500], Step [10/22], Loss: 0.2120\n",
      "Epoch [209/500], Step [20/22], Loss: 0.0143\n",
      "Epoch [210/500], Step [10/22], Loss: 0.2640\n",
      "Epoch [210/500], Step [20/22], Loss: 0.0537\n",
      "Epoch [211/500], Step [10/22], Loss: 0.1766\n",
      "Epoch [211/500], Step [20/22], Loss: 0.1406\n",
      "Epoch [212/500], Step [10/22], Loss: 0.1510\n",
      "Epoch [212/500], Step [20/22], Loss: 0.1102\n",
      "Epoch [213/500], Step [10/22], Loss: 0.4406\n",
      "Epoch [213/500], Step [20/22], Loss: 0.0831\n",
      "Epoch [214/500], Step [10/22], Loss: 0.2499\n",
      "Epoch [214/500], Step [20/22], Loss: 0.0764\n",
      "Epoch [215/500], Step [10/22], Loss: 0.2287\n",
      "Epoch [215/500], Step [20/22], Loss: 0.0579\n",
      "Epoch [216/500], Step [10/22], Loss: 0.1711\n",
      "Epoch [216/500], Step [20/22], Loss: 0.0581\n",
      "Epoch [217/500], Step [10/22], Loss: 0.2046\n",
      "Epoch [217/500], Step [20/22], Loss: 0.0383\n",
      "Epoch [218/500], Step [10/22], Loss: 0.2265\n",
      "Epoch [218/500], Step [20/22], Loss: 0.0137\n",
      "Epoch [219/500], Step [10/22], Loss: 0.2082\n",
      "Epoch [219/500], Step [20/22], Loss: 0.0289\n",
      "Epoch [220/500], Step [10/22], Loss: 0.1639\n",
      "Epoch [220/500], Step [20/22], Loss: 0.0480\n",
      "Epoch [221/500], Step [10/22], Loss: 0.2541\n",
      "Epoch [221/500], Step [20/22], Loss: 0.0176\n",
      "Epoch [222/500], Step [10/22], Loss: 0.2562\n",
      "Epoch [222/500], Step [20/22], Loss: 0.0204\n",
      "Epoch [223/500], Step [10/22], Loss: 0.2025\n",
      "Epoch [223/500], Step [20/22], Loss: 0.0483\n",
      "Epoch [224/500], Step [10/22], Loss: 0.5565\n",
      "Epoch [224/500], Step [20/22], Loss: 0.0429\n",
      "Epoch [225/500], Step [10/22], Loss: 0.2406\n",
      "Epoch [225/500], Step [20/22], Loss: 0.0635\n",
      "Epoch [226/500], Step [10/22], Loss: 0.2078\n",
      "Epoch [226/500], Step [20/22], Loss: 0.0410\n",
      "Epoch [227/500], Step [10/22], Loss: 0.6058\n",
      "Epoch [227/500], Step [20/22], Loss: 0.0210\n",
      "Epoch [228/500], Step [10/22], Loss: 0.2404\n",
      "Epoch [228/500], Step [20/22], Loss: 0.0189\n",
      "Epoch [229/500], Step [10/22], Loss: 0.2933\n",
      "Epoch [229/500], Step [20/22], Loss: 0.0284\n",
      "Epoch [230/500], Step [10/22], Loss: 0.1707\n",
      "Epoch [230/500], Step [20/22], Loss: 0.0146\n",
      "Epoch [231/500], Step [10/22], Loss: 0.3473\n",
      "Epoch [231/500], Step [20/22], Loss: 0.1919\n",
      "Epoch [232/500], Step [10/22], Loss: 0.2014\n",
      "Epoch [232/500], Step [20/22], Loss: 0.0951\n",
      "Epoch [233/500], Step [10/22], Loss: 0.1964\n",
      "Epoch [233/500], Step [20/22], Loss: 0.0229\n",
      "Epoch [234/500], Step [10/22], Loss: 0.1812\n",
      "Epoch [234/500], Step [20/22], Loss: 0.0188\n",
      "Epoch [235/500], Step [10/22], Loss: 0.1759\n",
      "Epoch [235/500], Step [20/22], Loss: 0.0115\n",
      "Epoch [236/500], Step [10/22], Loss: 0.1873\n",
      "Epoch [236/500], Step [20/22], Loss: 0.0072\n",
      "Epoch [237/500], Step [10/22], Loss: 0.1919\n",
      "Epoch [237/500], Step [20/22], Loss: 0.0062\n",
      "Epoch [238/500], Step [10/22], Loss: 0.2042\n",
      "Epoch [238/500], Step [20/22], Loss: 0.0036\n",
      "Epoch [239/500], Step [10/22], Loss: 0.2027\n",
      "Epoch [239/500], Step [20/22], Loss: 0.0038\n",
      "Epoch [240/500], Step [10/22], Loss: 0.2053\n",
      "Epoch [240/500], Step [20/22], Loss: 0.0029\n",
      "Epoch [241/500], Step [10/22], Loss: 0.2112\n",
      "Epoch [241/500], Step [20/22], Loss: 0.0016\n",
      "Epoch [242/500], Step [10/22], Loss: 0.2792\n",
      "Epoch [242/500], Step [20/22], Loss: 0.0025\n",
      "Epoch [243/500], Step [10/22], Loss: 0.1827\n",
      "Epoch [243/500], Step [20/22], Loss: 0.0170\n",
      "Epoch [244/500], Step [10/22], Loss: 0.1767\n",
      "Epoch [244/500], Step [20/22], Loss: 0.0098\n",
      "Epoch [245/500], Step [10/22], Loss: 0.2542\n",
      "Epoch [245/500], Step [20/22], Loss: 0.0149\n",
      "Epoch [246/500], Step [10/22], Loss: 0.1650\n",
      "Epoch [246/500], Step [20/22], Loss: 0.0618\n",
      "Epoch [247/500], Step [10/22], Loss: 0.1418\n",
      "Epoch [247/500], Step [20/22], Loss: 0.0492\n",
      "Epoch [248/500], Step [10/22], Loss: 0.2969\n",
      "Epoch [248/500], Step [20/22], Loss: 0.0311\n",
      "Epoch [249/500], Step [10/22], Loss: 0.5495\n",
      "Epoch [249/500], Step [20/22], Loss: 0.0542\n",
      "Epoch [250/500], Step [10/22], Loss: 0.4493\n",
      "Epoch [250/500], Step [20/22], Loss: 0.0933\n",
      "Epoch [251/500], Step [10/22], Loss: 0.6390\n",
      "Epoch [251/500], Step [20/22], Loss: 0.1821\n",
      "Epoch [252/500], Step [10/22], Loss: 0.3242\n",
      "Epoch [252/500], Step [20/22], Loss: 0.2745\n",
      "Epoch [253/500], Step [10/22], Loss: 0.2589\n",
      "Epoch [253/500], Step [20/22], Loss: 0.1762\n",
      "Epoch [254/500], Step [10/22], Loss: 0.2435\n",
      "Epoch [254/500], Step [20/22], Loss: 0.1506\n",
      "Epoch [255/500], Step [10/22], Loss: 0.2182\n",
      "Epoch [255/500], Step [20/22], Loss: 0.1196\n",
      "Epoch [256/500], Step [10/22], Loss: 0.2007\n",
      "Epoch [256/500], Step [20/22], Loss: 0.0996\n",
      "Epoch [257/500], Step [10/22], Loss: 0.1829\n",
      "Epoch [257/500], Step [20/22], Loss: 0.0853\n",
      "Epoch [258/500], Step [10/22], Loss: 0.1697\n",
      "Epoch [258/500], Step [20/22], Loss: 0.0762\n",
      "Epoch [259/500], Step [10/22], Loss: 0.1603\n",
      "Epoch [259/500], Step [20/22], Loss: 0.0716\n",
      "Epoch [260/500], Step [10/22], Loss: 0.1573\n",
      "Epoch [260/500], Step [20/22], Loss: 0.0640\n",
      "Epoch [261/500], Step [10/22], Loss: 0.1808\n",
      "Epoch [261/500], Step [20/22], Loss: 0.0513\n",
      "Epoch [262/500], Step [10/22], Loss: 0.2140\n",
      "Epoch [262/500], Step [20/22], Loss: 0.0431\n",
      "Epoch [263/500], Step [10/22], Loss: 0.1748\n",
      "Epoch [263/500], Step [20/22], Loss: 0.0344\n",
      "Epoch [264/500], Step [10/22], Loss: 0.1678\n",
      "Epoch [264/500], Step [20/22], Loss: 0.0461\n",
      "Epoch [265/500], Step [10/22], Loss: 0.1465\n",
      "Epoch [265/500], Step [20/22], Loss: 0.0255\n",
      "Epoch [266/500], Step [10/22], Loss: 0.3210\n",
      "Epoch [266/500], Step [20/22], Loss: 0.0347\n",
      "Epoch [267/500], Step [10/22], Loss: 0.3234\n",
      "Epoch [267/500], Step [20/22], Loss: 0.0239\n",
      "Epoch [268/500], Step [10/22], Loss: 0.2320\n",
      "Epoch [268/500], Step [20/22], Loss: 0.0656\n",
      "Epoch [269/500], Step [10/22], Loss: 0.2087\n",
      "Epoch [269/500], Step [20/22], Loss: 0.1316\n",
      "Epoch [270/500], Step [10/22], Loss: 0.6784\n",
      "Epoch [270/500], Step [20/22], Loss: 0.0994\n",
      "Epoch [271/500], Step [10/22], Loss: 0.2269\n",
      "Epoch [271/500], Step [20/22], Loss: 0.1083\n",
      "Epoch [272/500], Step [10/22], Loss: 0.2099\n",
      "Epoch [272/500], Step [20/22], Loss: 0.0776\n",
      "Epoch [273/500], Step [10/22], Loss: 0.2014\n",
      "Epoch [273/500], Step [20/22], Loss: 0.0637\n",
      "Epoch [274/500], Step [10/22], Loss: 0.1614\n",
      "Epoch [274/500], Step [20/22], Loss: 0.0592\n",
      "Epoch [275/500], Step [10/22], Loss: 0.1715\n",
      "Epoch [275/500], Step [20/22], Loss: 0.0527\n",
      "Epoch [276/500], Step [10/22], Loss: 0.1970\n",
      "Epoch [276/500], Step [20/22], Loss: 0.0253\n",
      "Epoch [277/500], Step [10/22], Loss: 0.1395\n",
      "Epoch [277/500], Step [20/22], Loss: 0.0294\n",
      "Epoch [278/500], Step [10/22], Loss: 0.1653\n",
      "Epoch [278/500], Step [20/22], Loss: 0.0409\n",
      "Epoch [279/500], Step [10/22], Loss: 0.3169\n",
      "Epoch [279/500], Step [20/22], Loss: 0.0224\n",
      "Epoch [280/500], Step [10/22], Loss: 0.1549\n",
      "Epoch [280/500], Step [20/22], Loss: 0.0206\n",
      "Epoch [281/500], Step [10/22], Loss: 0.1825\n",
      "Epoch [281/500], Step [20/22], Loss: 0.0149\n",
      "Epoch [282/500], Step [10/22], Loss: 0.2755\n",
      "Epoch [282/500], Step [20/22], Loss: 0.0104\n",
      "Epoch [283/500], Step [10/22], Loss: 0.1503\n",
      "Epoch [283/500], Step [20/22], Loss: 0.0135\n",
      "Epoch [284/500], Step [10/22], Loss: 0.2998\n",
      "Epoch [284/500], Step [20/22], Loss: 0.0145\n",
      "Epoch [285/500], Step [10/22], Loss: 0.1586\n",
      "Epoch [285/500], Step [20/22], Loss: 0.0304\n",
      "Epoch [286/500], Step [10/22], Loss: 0.1402\n",
      "Epoch [286/500], Step [20/22], Loss: 0.0120\n",
      "Epoch [287/500], Step [10/22], Loss: 0.3844\n",
      "Epoch [287/500], Step [20/22], Loss: 0.0303\n",
      "Epoch [288/500], Step [10/22], Loss: 0.1609\n",
      "Epoch [288/500], Step [20/22], Loss: 0.0330\n",
      "Epoch [289/500], Step [10/22], Loss: 0.2213\n",
      "Epoch [289/500], Step [20/22], Loss: 0.0225\n",
      "Epoch [290/500], Step [10/22], Loss: 0.1626\n",
      "Epoch [290/500], Step [20/22], Loss: 0.0362\n",
      "Epoch [291/500], Step [10/22], Loss: 0.1530\n",
      "Epoch [291/500], Step [20/22], Loss: 0.0189\n",
      "Epoch [292/500], Step [10/22], Loss: 0.2489\n",
      "Epoch [292/500], Step [20/22], Loss: 0.0082\n",
      "Epoch [293/500], Step [10/22], Loss: 0.1192\n",
      "Epoch [293/500], Step [20/22], Loss: 0.0076\n",
      "Epoch [294/500], Step [10/22], Loss: 0.2040\n",
      "Epoch [294/500], Step [20/22], Loss: 0.0077\n",
      "Epoch [295/500], Step [10/22], Loss: 0.1721\n",
      "Epoch [295/500], Step [20/22], Loss: 0.0056\n",
      "Epoch [296/500], Step [10/22], Loss: 0.1241\n",
      "Epoch [296/500], Step [20/22], Loss: 0.0071\n",
      "Epoch [297/500], Step [10/22], Loss: 0.2418\n",
      "Epoch [297/500], Step [20/22], Loss: 0.0094\n",
      "Epoch [298/500], Step [10/22], Loss: 0.1341\n",
      "Epoch [298/500], Step [20/22], Loss: 0.0181\n",
      "Epoch [299/500], Step [10/22], Loss: 0.1768\n",
      "Epoch [299/500], Step [20/22], Loss: 0.0087\n",
      "Epoch [300/500], Step [10/22], Loss: 0.2254\n",
      "Epoch [300/500], Step [20/22], Loss: 0.0068\n",
      "Epoch [301/500], Step [10/22], Loss: 0.1603\n",
      "Epoch [301/500], Step [20/22], Loss: 0.0231\n",
      "Epoch [302/500], Step [10/22], Loss: 0.2060\n",
      "Epoch [302/500], Step [20/22], Loss: 0.0198\n",
      "Epoch [303/500], Step [10/22], Loss: 0.1404\n",
      "Epoch [303/500], Step [20/22], Loss: 0.0356\n",
      "Epoch [304/500], Step [10/22], Loss: 0.4670\n",
      "Epoch [304/500], Step [20/22], Loss: 0.0165\n",
      "Epoch [305/500], Step [10/22], Loss: 0.1515\n",
      "Epoch [305/500], Step [20/22], Loss: 0.0214\n",
      "Epoch [306/500], Step [10/22], Loss: 0.1935\n",
      "Epoch [306/500], Step [20/22], Loss: 0.0193\n",
      "Epoch [307/500], Step [10/22], Loss: 0.1516\n",
      "Epoch [307/500], Step [20/22], Loss: 0.0238\n",
      "Epoch [308/500], Step [10/22], Loss: 0.1217\n",
      "Epoch [308/500], Step [20/22], Loss: 0.0115\n",
      "Epoch [309/500], Step [10/22], Loss: 0.1441\n",
      "Epoch [309/500], Step [20/22], Loss: 0.0095\n",
      "Epoch [310/500], Step [10/22], Loss: 0.1282\n",
      "Epoch [310/500], Step [20/22], Loss: 0.0076\n",
      "Epoch [311/500], Step [10/22], Loss: 0.1294\n",
      "Epoch [311/500], Step [20/22], Loss: 0.0072\n",
      "Epoch [312/500], Step [10/22], Loss: 0.1689\n",
      "Epoch [312/500], Step [20/22], Loss: 0.0070\n",
      "Epoch [313/500], Step [10/22], Loss: 0.1271\n",
      "Epoch [313/500], Step [20/22], Loss: 0.0093\n",
      "Epoch [314/500], Step [10/22], Loss: 0.1451\n",
      "Epoch [314/500], Step [20/22], Loss: 0.0429\n",
      "Epoch [315/500], Step [10/22], Loss: 0.1474\n",
      "Epoch [315/500], Step [20/22], Loss: 0.0830\n",
      "Epoch [316/500], Step [10/22], Loss: 0.2790\n",
      "Epoch [316/500], Step [20/22], Loss: 0.0124\n",
      "Epoch [317/500], Step [10/22], Loss: 0.1509\n",
      "Epoch [317/500], Step [20/22], Loss: 0.0794\n",
      "Epoch [318/500], Step [10/22], Loss: 0.5378\n",
      "Epoch [318/500], Step [20/22], Loss: 0.0228\n",
      "Epoch [319/500], Step [10/22], Loss: 0.1371\n",
      "Epoch [319/500], Step [20/22], Loss: 0.0263\n",
      "Epoch [320/500], Step [10/22], Loss: 0.1648\n",
      "Epoch [320/500], Step [20/22], Loss: 0.0174\n",
      "Epoch [321/500], Step [10/22], Loss: 0.1264\n",
      "Epoch [321/500], Step [20/22], Loss: 0.0134\n",
      "Epoch [322/500], Step [10/22], Loss: 0.1277\n",
      "Epoch [322/500], Step [20/22], Loss: 0.0117\n",
      "Epoch [323/500], Step [10/22], Loss: 0.1448\n",
      "Epoch [323/500], Step [20/22], Loss: 0.0104\n",
      "Epoch [324/500], Step [10/22], Loss: 0.1221\n",
      "Epoch [324/500], Step [20/22], Loss: 0.0105\n",
      "Epoch [325/500], Step [10/22], Loss: 0.1314\n",
      "Epoch [325/500], Step [20/22], Loss: 0.0061\n",
      "Epoch [326/500], Step [10/22], Loss: 0.1852\n",
      "Epoch [326/500], Step [20/22], Loss: 0.0107\n",
      "Epoch [327/500], Step [10/22], Loss: 0.1193\n",
      "Epoch [327/500], Step [20/22], Loss: 0.0108\n",
      "Epoch [328/500], Step [10/22], Loss: 0.1186\n",
      "Epoch [328/500], Step [20/22], Loss: 0.0107\n",
      "Epoch [329/500], Step [10/22], Loss: 0.3382\n",
      "Epoch [329/500], Step [20/22], Loss: 0.0349\n",
      "Epoch [330/500], Step [10/22], Loss: 0.1528\n",
      "Epoch [330/500], Step [20/22], Loss: 0.0347\n",
      "Epoch [331/500], Step [10/22], Loss: 0.1401\n",
      "Epoch [331/500], Step [20/22], Loss: 0.1856\n",
      "Epoch [332/500], Step [10/22], Loss: 0.2953\n",
      "Epoch [332/500], Step [20/22], Loss: 0.0559\n",
      "Epoch [333/500], Step [10/22], Loss: 0.1321\n",
      "Epoch [333/500], Step [20/22], Loss: 0.0784\n",
      "Epoch [334/500], Step [10/22], Loss: 0.1637\n",
      "Epoch [334/500], Step [20/22], Loss: 0.0194\n",
      "Epoch [335/500], Step [10/22], Loss: 0.1199\n",
      "Epoch [335/500], Step [20/22], Loss: 0.0384\n",
      "Epoch [336/500], Step [10/22], Loss: 0.2163\n",
      "Epoch [336/500], Step [20/22], Loss: 0.0175\n",
      "Epoch [337/500], Step [10/22], Loss: 0.1147\n",
      "Epoch [337/500], Step [20/22], Loss: 0.0140\n",
      "Epoch [338/500], Step [10/22], Loss: 0.1270\n",
      "Epoch [338/500], Step [20/22], Loss: 0.0131\n",
      "Epoch [339/500], Step [10/22], Loss: 0.1661\n",
      "Epoch [339/500], Step [20/22], Loss: 0.0140\n",
      "Epoch [340/500], Step [10/22], Loss: 0.1020\n",
      "Epoch [340/500], Step [20/22], Loss: 0.0132\n",
      "Epoch [341/500], Step [10/22], Loss: 0.1727\n",
      "Epoch [341/500], Step [20/22], Loss: 0.0153\n",
      "Epoch [342/500], Step [10/22], Loss: 0.1610\n",
      "Epoch [342/500], Step [20/22], Loss: 0.0289\n",
      "Epoch [343/500], Step [10/22], Loss: 0.1062\n",
      "Epoch [343/500], Step [20/22], Loss: 0.0088\n",
      "Epoch [344/500], Step [10/22], Loss: 0.1667\n",
      "Epoch [344/500], Step [20/22], Loss: 0.0108\n",
      "Epoch [345/500], Step [10/22], Loss: 0.1500\n",
      "Epoch [345/500], Step [20/22], Loss: 0.0124\n",
      "Epoch [346/500], Step [10/22], Loss: 0.1070\n",
      "Epoch [346/500], Step [20/22], Loss: 0.0065\n",
      "Epoch [347/500], Step [10/22], Loss: 0.2287\n",
      "Epoch [347/500], Step [20/22], Loss: 0.0086\n",
      "Epoch [348/500], Step [10/22], Loss: 0.1118\n",
      "Epoch [348/500], Step [20/22], Loss: 0.0076\n",
      "Epoch [349/500], Step [10/22], Loss: 0.1140\n",
      "Epoch [349/500], Step [20/22], Loss: 0.0183\n",
      "Epoch [350/500], Step [10/22], Loss: 0.3202\n",
      "Epoch [350/500], Step [20/22], Loss: 0.0128\n",
      "Epoch [351/500], Step [10/22], Loss: 0.1464\n",
      "Epoch [351/500], Step [20/22], Loss: 0.4724\n",
      "Epoch [352/500], Step [10/22], Loss: 0.1191\n",
      "Epoch [352/500], Step [20/22], Loss: 0.2619\n",
      "Epoch [353/500], Step [10/22], Loss: 0.2201\n",
      "Epoch [353/500], Step [20/22], Loss: 0.0370\n",
      "Epoch [354/500], Step [10/22], Loss: 0.2942\n",
      "Epoch [354/500], Step [20/22], Loss: 0.0574\n",
      "Epoch [355/500], Step [10/22], Loss: 0.1365\n",
      "Epoch [355/500], Step [20/22], Loss: 0.0285\n",
      "Epoch [356/500], Step [10/22], Loss: 0.1194\n",
      "Epoch [356/500], Step [20/22], Loss: 0.0228\n",
      "Epoch [357/500], Step [10/22], Loss: 0.1858\n",
      "Epoch [357/500], Step [20/22], Loss: 0.0162\n",
      "Epoch [358/500], Step [10/22], Loss: 0.1067\n",
      "Epoch [358/500], Step [20/22], Loss: 0.0202\n",
      "Epoch [359/500], Step [10/22], Loss: 0.1352\n",
      "Epoch [359/500], Step [20/22], Loss: 0.0192\n",
      "Epoch [360/500], Step [10/22], Loss: 0.1708\n",
      "Epoch [360/500], Step [20/22], Loss: 0.0105\n",
      "Epoch [361/500], Step [10/22], Loss: 0.0929\n",
      "Epoch [361/500], Step [20/22], Loss: 0.0159\n",
      "Epoch [362/500], Step [10/22], Loss: 0.2082\n",
      "Epoch [362/500], Step [20/22], Loss: 0.0142\n",
      "Epoch [363/500], Step [10/22], Loss: 0.1594\n",
      "Epoch [363/500], Step [20/22], Loss: 0.0068\n",
      "Epoch [364/500], Step [10/22], Loss: 0.0951\n",
      "Epoch [364/500], Step [20/22], Loss: 0.0111\n",
      "Epoch [365/500], Step [10/22], Loss: 0.1747\n",
      "Epoch [365/500], Step [20/22], Loss: 0.0089\n",
      "Epoch [366/500], Step [10/22], Loss: 0.1350\n",
      "Epoch [366/500], Step [20/22], Loss: 0.0044\n",
      "Epoch [367/500], Step [10/22], Loss: 0.0986\n",
      "Epoch [367/500], Step [20/22], Loss: 0.0110\n",
      "Epoch [368/500], Step [10/22], Loss: 0.2728\n",
      "Epoch [368/500], Step [20/22], Loss: 0.0124\n",
      "Epoch [369/500], Step [10/22], Loss: 0.1136\n",
      "Epoch [369/500], Step [20/22], Loss: 0.0131\n",
      "Epoch [370/500], Step [10/22], Loss: 0.1046\n",
      "Epoch [370/500], Step [20/22], Loss: 0.0191\n",
      "Epoch [371/500], Step [10/22], Loss: 0.6441\n",
      "Epoch [371/500], Step [20/22], Loss: 0.0605\n",
      "Epoch [372/500], Step [10/22], Loss: 0.2057\n",
      "Epoch [372/500], Step [20/22], Loss: 0.1196\n",
      "Epoch [373/500], Step [10/22], Loss: 0.1525\n",
      "Epoch [373/500], Step [20/22], Loss: 0.3500\n",
      "Epoch [374/500], Step [10/22], Loss: 0.3768\n",
      "Epoch [374/500], Step [20/22], Loss: 0.0511\n",
      "Epoch [375/500], Step [10/22], Loss: 0.1551\n",
      "Epoch [375/500], Step [20/22], Loss: 0.0595\n",
      "Epoch [376/500], Step [10/22], Loss: 0.1303\n",
      "Epoch [376/500], Step [20/22], Loss: 0.0305\n",
      "Epoch [377/500], Step [10/22], Loss: 0.1476\n",
      "Epoch [377/500], Step [20/22], Loss: 0.0253\n",
      "Epoch [378/500], Step [10/22], Loss: 0.1170\n",
      "Epoch [378/500], Step [20/22], Loss: 0.0214\n",
      "Epoch [379/500], Step [10/22], Loss: 0.1194\n",
      "Epoch [379/500], Step [20/22], Loss: 0.0172\n",
      "Epoch [380/500], Step [10/22], Loss: 0.1018\n",
      "Epoch [380/500], Step [20/22], Loss: 0.0141\n",
      "Epoch [381/500], Step [10/22], Loss: 0.1045\n",
      "Epoch [381/500], Step [20/22], Loss: 0.0123\n",
      "Epoch [382/500], Step [10/22], Loss: 0.1105\n",
      "Epoch [382/500], Step [20/22], Loss: 0.0098\n",
      "Epoch [383/500], Step [10/22], Loss: 0.0988\n",
      "Epoch [383/500], Step [20/22], Loss: 0.0109\n",
      "Epoch [384/500], Step [10/22], Loss: 0.1081\n",
      "Epoch [384/500], Step [20/22], Loss: 0.0118\n",
      "Epoch [385/500], Step [10/22], Loss: 0.1492\n",
      "Epoch [385/500], Step [20/22], Loss: 0.0110\n",
      "Epoch [386/500], Step [10/22], Loss: 0.0939\n",
      "Epoch [386/500], Step [20/22], Loss: 0.0052\n",
      "Epoch [387/500], Step [10/22], Loss: 0.1223\n",
      "Epoch [387/500], Step [20/22], Loss: 0.0440\n",
      "Epoch [388/500], Step [10/22], Loss: 0.1375\n",
      "Epoch [388/500], Step [20/22], Loss: 0.0817\n",
      "Epoch [389/500], Step [10/22], Loss: 0.0884\n",
      "Epoch [389/500], Step [20/22], Loss: 0.0357\n",
      "Epoch [390/500], Step [10/22], Loss: 0.1126\n",
      "Epoch [390/500], Step [20/22], Loss: 0.0830\n",
      "Epoch [391/500], Step [10/22], Loss: 0.1275\n",
      "Epoch [391/500], Step [20/22], Loss: 0.0432\n",
      "Epoch [392/500], Step [10/22], Loss: 0.1685\n",
      "Epoch [392/500], Step [20/22], Loss: 0.0759\n",
      "Epoch [393/500], Step [10/22], Loss: 0.1108\n",
      "Epoch [393/500], Step [20/22], Loss: 0.0364\n",
      "Epoch [394/500], Step [10/22], Loss: 0.1044\n",
      "Epoch [394/500], Step [20/22], Loss: 0.0718\n",
      "Epoch [395/500], Step [10/22], Loss: 0.2248\n",
      "Epoch [395/500], Step [20/22], Loss: 0.0945\n",
      "Epoch [396/500], Step [10/22], Loss: 0.0880\n",
      "Epoch [396/500], Step [20/22], Loss: 0.0310\n",
      "Epoch [397/500], Step [10/22], Loss: 0.1032\n",
      "Epoch [397/500], Step [20/22], Loss: 0.0163\n",
      "Epoch [398/500], Step [10/22], Loss: 0.2701\n",
      "Epoch [398/500], Step [20/22], Loss: 0.0171\n",
      "Epoch [399/500], Step [10/22], Loss: 0.1186\n",
      "Epoch [399/500], Step [20/22], Loss: 0.0299\n",
      "Epoch [400/500], Step [10/22], Loss: 0.1459\n",
      "Epoch [400/500], Step [20/22], Loss: 0.0169\n",
      "Epoch [401/500], Step [10/22], Loss: 0.1161\n",
      "Epoch [401/500], Step [20/22], Loss: 0.0135\n",
      "Epoch [402/500], Step [10/22], Loss: 0.0800\n",
      "Epoch [402/500], Step [20/22], Loss: 0.0182\n",
      "Epoch [403/500], Step [10/22], Loss: 0.1785\n",
      "Epoch [403/500], Step [20/22], Loss: 0.0146\n",
      "Epoch [404/500], Step [10/22], Loss: 0.0906\n",
      "Epoch [404/500], Step [20/22], Loss: 0.0256\n",
      "Epoch [405/500], Step [10/22], Loss: 0.0997\n",
      "Epoch [405/500], Step [20/22], Loss: 0.0082\n",
      "Epoch [406/500], Step [10/22], Loss: 0.1685\n",
      "Epoch [406/500], Step [20/22], Loss: 0.0572\n",
      "Epoch [407/500], Step [10/22], Loss: 0.1136\n",
      "Epoch [407/500], Step [20/22], Loss: 0.0255\n",
      "Epoch [408/500], Step [10/22], Loss: 0.1063\n",
      "Epoch [408/500], Step [20/22], Loss: 0.0179\n",
      "Epoch [409/500], Step [10/22], Loss: 0.1249\n",
      "Epoch [409/500], Step [20/22], Loss: 0.0091\n",
      "Epoch [410/500], Step [10/22], Loss: 0.1187\n",
      "Epoch [410/500], Step [20/22], Loss: 0.0293\n",
      "Epoch [411/500], Step [10/22], Loss: 0.1332\n",
      "Epoch [411/500], Step [20/22], Loss: 0.0615\n",
      "Epoch [412/500], Step [10/22], Loss: 0.0986\n",
      "Epoch [412/500], Step [20/22], Loss: 0.0096\n",
      "Epoch [413/500], Step [10/22], Loss: 0.1074\n",
      "Epoch [413/500], Step [20/22], Loss: 0.0100\n",
      "Epoch [414/500], Step [10/22], Loss: 0.1568\n",
      "Epoch [414/500], Step [20/22], Loss: 0.0182\n",
      "Epoch [415/500], Step [10/22], Loss: 0.1236\n",
      "Epoch [415/500], Step [20/22], Loss: 0.0508\n",
      "Epoch [416/500], Step [10/22], Loss: 0.1334\n",
      "Epoch [416/500], Step [20/22], Loss: 0.0387\n",
      "Epoch [417/500], Step [10/22], Loss: 0.0923\n",
      "Epoch [417/500], Step [20/22], Loss: 0.0183\n",
      "Epoch [418/500], Step [10/22], Loss: 0.1567\n",
      "Epoch [418/500], Step [20/22], Loss: 0.0124\n",
      "Epoch [419/500], Step [10/22], Loss: 0.1052\n",
      "Epoch [419/500], Step [20/22], Loss: 0.0122\n",
      "Epoch [420/500], Step [10/22], Loss: 0.0860\n",
      "Epoch [420/500], Step [20/22], Loss: 0.0099\n",
      "Epoch [421/500], Step [10/22], Loss: 0.1068\n",
      "Epoch [421/500], Step [20/22], Loss: 0.0074\n",
      "Epoch [422/500], Step [10/22], Loss: 0.2366\n",
      "Epoch [422/500], Step [20/22], Loss: 0.0052\n",
      "Epoch [423/500], Step [10/22], Loss: 0.1041\n",
      "Epoch [423/500], Step [20/22], Loss: 0.0125\n",
      "Epoch [424/500], Step [10/22], Loss: 0.0630\n",
      "Epoch [424/500], Step [20/22], Loss: 0.0596\n",
      "Epoch [425/500], Step [10/22], Loss: 0.1441\n",
      "Epoch [425/500], Step [20/22], Loss: 0.0539\n",
      "Epoch [426/500], Step [10/22], Loss: 0.1470\n",
      "Epoch [426/500], Step [20/22], Loss: 0.0166\n",
      "Epoch [427/500], Step [10/22], Loss: 0.1143\n",
      "Epoch [427/500], Step [20/22], Loss: 0.0700\n",
      "Epoch [428/500], Step [10/22], Loss: 0.1218\n",
      "Epoch [428/500], Step [20/22], Loss: 0.1631\n",
      "Epoch [429/500], Step [10/22], Loss: 0.1026\n",
      "Epoch [429/500], Step [20/22], Loss: 0.0189\n",
      "Epoch [430/500], Step [10/22], Loss: 0.0870\n",
      "Epoch [430/500], Step [20/22], Loss: 0.0128\n",
      "Epoch [431/500], Step [10/22], Loss: 0.0914\n",
      "Epoch [431/500], Step [20/22], Loss: 0.0132\n",
      "Epoch [432/500], Step [10/22], Loss: 0.1088\n",
      "Epoch [432/500], Step [20/22], Loss: 0.0098\n",
      "Epoch [433/500], Step [10/22], Loss: 0.0553\n",
      "Epoch [433/500], Step [20/22], Loss: 0.0100\n",
      "Epoch [434/500], Step [10/22], Loss: 0.1730\n",
      "Epoch [434/500], Step [20/22], Loss: 0.0119\n",
      "Epoch [435/500], Step [10/22], Loss: 0.0772\n",
      "Epoch [435/500], Step [20/22], Loss: 0.0093\n",
      "Epoch [436/500], Step [10/22], Loss: 0.0481\n",
      "Epoch [436/500], Step [20/22], Loss: 0.0103\n",
      "Epoch [437/500], Step [10/22], Loss: 0.3562\n",
      "Epoch [437/500], Step [20/22], Loss: 0.0242\n",
      "Epoch [438/500], Step [10/22], Loss: 0.0821\n",
      "Epoch [438/500], Step [20/22], Loss: 0.0293\n",
      "Epoch [439/500], Step [10/22], Loss: 0.0883\n",
      "Epoch [439/500], Step [20/22], Loss: 0.0190\n",
      "Epoch [440/500], Step [10/22], Loss: 0.4826\n",
      "Epoch [440/500], Step [20/22], Loss: 0.0653\n",
      "Epoch [441/500], Step [10/22], Loss: 0.1675\n",
      "Epoch [441/500], Step [20/22], Loss: 0.0239\n",
      "Epoch [442/500], Step [10/22], Loss: 0.0883\n",
      "Epoch [442/500], Step [20/22], Loss: 0.1375\n",
      "Epoch [443/500], Step [10/22], Loss: 0.1480\n",
      "Epoch [443/500], Step [20/22], Loss: 0.0310\n",
      "Epoch [444/500], Step [10/22], Loss: 0.0634\n",
      "Epoch [444/500], Step [20/22], Loss: 0.0139\n",
      "Epoch [445/500], Step [10/22], Loss: 0.1274\n",
      "Epoch [445/500], Step [20/22], Loss: 0.0121\n",
      "Epoch [446/500], Step [10/22], Loss: 0.0868\n",
      "Epoch [446/500], Step [20/22], Loss: 0.0093\n",
      "Epoch [447/500], Step [10/22], Loss: 0.0704\n",
      "Epoch [447/500], Step [20/22], Loss: 0.0079\n",
      "Epoch [448/500], Step [10/22], Loss: 0.1230\n",
      "Epoch [448/500], Step [20/22], Loss: 0.0159\n",
      "Epoch [449/500], Step [10/22], Loss: 0.0547\n",
      "Epoch [449/500], Step [20/22], Loss: 0.0173\n",
      "Epoch [450/500], Step [10/22], Loss: 0.1195\n",
      "Epoch [450/500], Step [20/22], Loss: 0.0366\n",
      "Epoch [451/500], Step [10/22], Loss: 0.0896\n",
      "Epoch [451/500], Step [20/22], Loss: 0.0254\n",
      "Epoch [452/500], Step [10/22], Loss: 0.0478\n",
      "Epoch [452/500], Step [20/22], Loss: 0.0817\n",
      "Epoch [453/500], Step [10/22], Loss: 0.0928\n",
      "Epoch [453/500], Step [20/22], Loss: 0.0614\n",
      "Epoch [454/500], Step [10/22], Loss: 0.1515\n",
      "Epoch [454/500], Step [20/22], Loss: 0.0212\n",
      "Epoch [455/500], Step [10/22], Loss: 0.0956\n",
      "Epoch [455/500], Step [20/22], Loss: 0.0462\n",
      "Epoch [456/500], Step [10/22], Loss: 0.0894\n",
      "Epoch [456/500], Step [20/22], Loss: 0.0135\n",
      "Epoch [457/500], Step [10/22], Loss: 0.0756\n",
      "Epoch [457/500], Step [20/22], Loss: 0.0103\n",
      "Epoch [458/500], Step [10/22], Loss: 0.0582\n",
      "Epoch [458/500], Step [20/22], Loss: 0.0084\n",
      "Epoch [459/500], Step [10/22], Loss: 0.0884\n",
      "Epoch [459/500], Step [20/22], Loss: 0.0076\n",
      "Epoch [460/500], Step [10/22], Loss: 0.0519\n",
      "Epoch [460/500], Step [20/22], Loss: 0.0066\n",
      "Epoch [461/500], Step [10/22], Loss: 0.0838\n",
      "Epoch [461/500], Step [20/22], Loss: 0.0062\n",
      "Epoch [462/500], Step [10/22], Loss: 0.0733\n",
      "Epoch [462/500], Step [20/22], Loss: 0.0093\n",
      "Epoch [463/500], Step [10/22], Loss: 0.0371\n",
      "Epoch [463/500], Step [20/22], Loss: 0.0314\n",
      "Epoch [464/500], Step [10/22], Loss: 0.0861\n",
      "Epoch [464/500], Step [20/22], Loss: 0.0434\n",
      "Epoch [465/500], Step [10/22], Loss: 0.0935\n",
      "Epoch [465/500], Step [20/22], Loss: 0.0078\n",
      "Epoch [466/500], Step [10/22], Loss: 0.0975\n",
      "Epoch [466/500], Step [20/22], Loss: 0.0206\n",
      "Epoch [467/500], Step [10/22], Loss: 0.0562\n",
      "Epoch [467/500], Step [20/22], Loss: 0.0594\n",
      "Epoch [468/500], Step [10/22], Loss: 0.0545\n",
      "Epoch [468/500], Step [20/22], Loss: 0.0183\n",
      "Epoch [469/500], Step [10/22], Loss: 0.2409\n",
      "Epoch [469/500], Step [20/22], Loss: 0.0493\n",
      "Epoch [470/500], Step [10/22], Loss: 0.1068\n",
      "Epoch [470/500], Step [20/22], Loss: 0.1899\n",
      "Epoch [471/500], Step [10/22], Loss: 0.0779\n",
      "Epoch [471/500], Step [20/22], Loss: 0.0636\n",
      "Epoch [472/500], Step [10/22], Loss: 0.0859\n",
      "Epoch [472/500], Step [20/22], Loss: 0.0130\n",
      "Epoch [473/500], Step [10/22], Loss: 0.0494\n",
      "Epoch [473/500], Step [20/22], Loss: 0.0156\n",
      "Epoch [474/500], Step [10/22], Loss: 0.0768\n",
      "Epoch [474/500], Step [20/22], Loss: 0.0108\n",
      "Epoch [475/500], Step [10/22], Loss: 0.0537\n",
      "Epoch [475/500], Step [20/22], Loss: 0.0111\n",
      "Epoch [476/500], Step [10/22], Loss: 0.0449\n",
      "Epoch [476/500], Step [20/22], Loss: 0.0094\n",
      "Epoch [477/500], Step [10/22], Loss: 0.0529\n",
      "Epoch [477/500], Step [20/22], Loss: 0.0077\n",
      "Epoch [478/500], Step [10/22], Loss: 0.0421\n",
      "Epoch [478/500], Step [20/22], Loss: 0.0062\n",
      "Epoch [479/500], Step [10/22], Loss: 0.0452\n",
      "Epoch [479/500], Step [20/22], Loss: 0.0054\n",
      "Epoch [480/500], Step [10/22], Loss: 0.0429\n",
      "Epoch [480/500], Step [20/22], Loss: 0.0039\n",
      "Epoch [481/500], Step [10/22], Loss: 0.0381\n",
      "Epoch [481/500], Step [20/22], Loss: 0.0036\n",
      "Epoch [482/500], Step [10/22], Loss: 0.0424\n",
      "Epoch [482/500], Step [20/22], Loss: 0.0030\n",
      "Epoch [483/500], Step [10/22], Loss: 0.0479\n",
      "Epoch [483/500], Step [20/22], Loss: 0.0020\n",
      "Epoch [484/500], Step [10/22], Loss: 0.0338\n",
      "Epoch [484/500], Step [20/22], Loss: 0.0033\n",
      "Epoch [485/500], Step [10/22], Loss: 0.1424\n",
      "Epoch [485/500], Step [20/22], Loss: 0.0967\n",
      "Epoch [486/500], Step [10/22], Loss: 0.0461\n",
      "Epoch [486/500], Step [20/22], Loss: 0.0485\n",
      "Epoch [487/500], Step [10/22], Loss: 0.1905\n",
      "Epoch [487/500], Step [20/22], Loss: 0.0072\n",
      "Epoch [488/500], Step [10/22], Loss: 0.1228\n",
      "Epoch [488/500], Step [20/22], Loss: 0.0148\n",
      "Epoch [489/500], Step [10/22], Loss: 0.0869\n",
      "Epoch [489/500], Step [20/22], Loss: 0.0139\n",
      "Epoch [490/500], Step [10/22], Loss: 0.1186\n",
      "Epoch [490/500], Step [20/22], Loss: 0.0112\n",
      "Epoch [491/500], Step [10/22], Loss: 0.0527\n",
      "Epoch [491/500], Step [20/22], Loss: 0.0175\n",
      "Epoch [492/500], Step [10/22], Loss: 0.0389\n",
      "Epoch [492/500], Step [20/22], Loss: 0.0309\n",
      "Epoch [493/500], Step [10/22], Loss: 0.3036\n",
      "Epoch [493/500], Step [20/22], Loss: 0.0502\n",
      "Epoch [494/500], Step [10/22], Loss: 0.0422\n",
      "Epoch [494/500], Step [20/22], Loss: 0.0111\n",
      "Epoch [495/500], Step [10/22], Loss: 0.0387\n",
      "Epoch [495/500], Step [20/22], Loss: 0.0629\n",
      "Epoch [496/500], Step [10/22], Loss: 0.2548\n",
      "Epoch [496/500], Step [20/22], Loss: 0.0390\n",
      "Epoch [497/500], Step [10/22], Loss: 0.0480\n",
      "Epoch [497/500], Step [20/22], Loss: 0.0121\n",
      "Epoch [498/500], Step [10/22], Loss: 0.0869\n",
      "Epoch [498/500], Step [20/22], Loss: 0.0135\n",
      "Epoch [499/500], Step [10/22], Loss: 0.0430\n",
      "Epoch [499/500], Step [20/22], Loss: 0.0092\n",
      "Epoch [500/500], Step [10/22], Loss: 0.0488\n",
      "Epoch [500/500], Step [20/22], Loss: 0.0066\n",
      "Epoch [1/500], Step [10/22], Loss: 0.6956\n",
      "Epoch [1/500], Step [20/22], Loss: 0.6868\n",
      "Epoch [2/500], Step [10/22], Loss: 0.6984\n",
      "Epoch [2/500], Step [20/22], Loss: 0.6257\n",
      "Epoch [3/500], Step [10/22], Loss: 0.7051\n",
      "Epoch [3/500], Step [20/22], Loss: 0.5398\n",
      "Epoch [4/500], Step [10/22], Loss: 0.7090\n",
      "Epoch [4/500], Step [20/22], Loss: 0.4319\n",
      "Epoch [5/500], Step [10/22], Loss: 0.6926\n",
      "Epoch [5/500], Step [20/22], Loss: 0.3247\n",
      "Epoch [6/500], Step [10/22], Loss: 0.6852\n",
      "Epoch [6/500], Step [20/22], Loss: 0.2767\n",
      "Epoch [7/500], Step [10/22], Loss: 0.6832\n",
      "Epoch [7/500], Step [20/22], Loss: 0.2404\n",
      "Epoch [8/500], Step [10/22], Loss: 0.6833\n",
      "Epoch [8/500], Step [20/22], Loss: 0.4983\n",
      "Epoch [9/500], Step [10/22], Loss: 0.6840\n",
      "Epoch [9/500], Step [20/22], Loss: 0.2785\n",
      "Epoch [10/500], Step [10/22], Loss: 0.6827\n",
      "Epoch [10/500], Step [20/22], Loss: 0.2629\n",
      "Epoch [11/500], Step [10/22], Loss: 0.6813\n",
      "Epoch [11/500], Step [20/22], Loss: 0.2477\n",
      "Epoch [12/500], Step [10/22], Loss: 0.6806\n",
      "Epoch [12/500], Step [20/22], Loss: 0.2366\n",
      "Epoch [13/500], Step [10/22], Loss: 0.6802\n",
      "Epoch [13/500], Step [20/22], Loss: 0.2372\n",
      "Epoch [14/500], Step [10/22], Loss: 0.6786\n",
      "Epoch [14/500], Step [20/22], Loss: 0.2356\n",
      "Epoch [15/500], Step [10/22], Loss: 0.6795\n",
      "Epoch [15/500], Step [20/22], Loss: 0.2674\n",
      "Epoch [16/500], Step [10/22], Loss: 0.6751\n",
      "Epoch [16/500], Step [20/22], Loss: 0.2347\n",
      "Epoch [17/500], Step [10/22], Loss: 0.6850\n",
      "Epoch [17/500], Step [20/22], Loss: 0.5920\n",
      "Epoch [18/500], Step [10/22], Loss: 0.6849\n",
      "Epoch [18/500], Step [20/22], Loss: 0.4090\n",
      "Epoch [19/500], Step [10/22], Loss: 0.6803\n",
      "Epoch [19/500], Step [20/22], Loss: 0.2779\n",
      "Epoch [20/500], Step [10/22], Loss: 0.6727\n",
      "Epoch [20/500], Step [20/22], Loss: 0.2423\n",
      "Epoch [21/500], Step [10/22], Loss: 0.6841\n",
      "Epoch [21/500], Step [20/22], Loss: 0.2416\n",
      "Epoch [22/500], Step [10/22], Loss: 0.7036\n",
      "Epoch [22/500], Step [20/22], Loss: 0.2354\n",
      "Epoch [23/500], Step [10/22], Loss: 0.7092\n",
      "Epoch [23/500], Step [20/22], Loss: 0.2354\n",
      "Epoch [24/500], Step [10/22], Loss: 0.7133\n",
      "Epoch [24/500], Step [20/22], Loss: 0.2353\n",
      "Epoch [25/500], Step [10/22], Loss: 0.6993\n",
      "Epoch [25/500], Step [20/22], Loss: 0.2355\n",
      "Epoch [26/500], Step [10/22], Loss: 0.7357\n",
      "Epoch [26/500], Step [20/22], Loss: 0.2360\n",
      "Epoch [27/500], Step [10/22], Loss: 0.7116\n",
      "Epoch [27/500], Step [20/22], Loss: 0.2393\n",
      "Epoch [28/500], Step [10/22], Loss: 0.6558\n",
      "Epoch [28/500], Step [20/22], Loss: 0.2372\n",
      "Epoch [29/500], Step [10/22], Loss: 0.6592\n",
      "Epoch [29/500], Step [20/22], Loss: 0.2379\n",
      "Epoch [30/500], Step [10/22], Loss: 0.6948\n",
      "Epoch [30/500], Step [20/22], Loss: 0.3506\n",
      "Epoch [31/500], Step [10/22], Loss: 0.6335\n",
      "Epoch [31/500], Step [20/22], Loss: 0.2405\n",
      "Epoch [32/500], Step [10/22], Loss: 0.6917\n",
      "Epoch [32/500], Step [20/22], Loss: 0.4795\n",
      "Epoch [33/500], Step [10/22], Loss: 0.6423\n",
      "Epoch [33/500], Step [20/22], Loss: 0.2566\n",
      "Epoch [34/500], Step [10/22], Loss: 0.5665\n",
      "Epoch [34/500], Step [20/22], Loss: 0.2345\n",
      "Epoch [35/500], Step [10/22], Loss: 0.6082\n",
      "Epoch [35/500], Step [20/22], Loss: 0.2510\n",
      "Epoch [36/500], Step [10/22], Loss: 0.6548\n",
      "Epoch [36/500], Step [20/22], Loss: 0.2359\n",
      "Epoch [37/500], Step [10/22], Loss: 0.6265\n",
      "Epoch [37/500], Step [20/22], Loss: 0.2354\n",
      "Epoch [38/500], Step [10/22], Loss: 0.7440\n",
      "Epoch [38/500], Step [20/22], Loss: 0.2454\n",
      "Epoch [39/500], Step [10/22], Loss: 0.6770\n",
      "Epoch [39/500], Step [20/22], Loss: 0.2369\n",
      "Epoch [40/500], Step [10/22], Loss: 0.6190\n",
      "Epoch [40/500], Step [20/22], Loss: 0.2366\n",
      "Epoch [41/500], Step [10/22], Loss: 0.7712\n",
      "Epoch [41/500], Step [20/22], Loss: 0.2408\n",
      "Epoch [42/500], Step [10/22], Loss: 0.6057\n",
      "Epoch [42/500], Step [20/22], Loss: 0.2480\n",
      "Epoch [43/500], Step [10/22], Loss: 0.6629\n",
      "Epoch [43/500], Step [20/22], Loss: 0.2540\n",
      "Epoch [44/500], Step [10/22], Loss: 0.6110\n",
      "Epoch [44/500], Step [20/22], Loss: 0.2385\n",
      "Epoch [45/500], Step [10/22], Loss: 0.5353\n",
      "Epoch [45/500], Step [20/22], Loss: 0.2220\n",
      "Epoch [46/500], Step [10/22], Loss: 0.5066\n",
      "Epoch [46/500], Step [20/22], Loss: 0.2276\n",
      "Epoch [47/500], Step [10/22], Loss: 0.5738\n",
      "Epoch [47/500], Step [20/22], Loss: 0.4707\n",
      "Epoch [48/500], Step [10/22], Loss: 0.6133\n",
      "Epoch [48/500], Step [20/22], Loss: 0.2357\n",
      "Epoch [49/500], Step [10/22], Loss: 0.7458\n",
      "Epoch [49/500], Step [20/22], Loss: 0.2313\n",
      "Epoch [50/500], Step [10/22], Loss: 0.5864\n",
      "Epoch [50/500], Step [20/22], Loss: 0.2324\n",
      "Epoch [51/500], Step [10/22], Loss: 0.5804\n",
      "Epoch [51/500], Step [20/22], Loss: 0.2373\n",
      "Epoch [52/500], Step [10/22], Loss: 0.8843\n",
      "Epoch [52/500], Step [20/22], Loss: 0.2194\n",
      "Epoch [53/500], Step [10/22], Loss: 0.5481\n",
      "Epoch [53/500], Step [20/22], Loss: 0.2189\n",
      "Epoch [54/500], Step [10/22], Loss: 0.5704\n",
      "Epoch [54/500], Step [20/22], Loss: 0.2374\n",
      "Epoch [55/500], Step [10/22], Loss: 0.5470\n",
      "Epoch [55/500], Step [20/22], Loss: 0.2215\n",
      "Epoch [56/500], Step [10/22], Loss: 0.5337\n",
      "Epoch [56/500], Step [20/22], Loss: 0.2217\n",
      "Epoch [57/500], Step [10/22], Loss: 0.5901\n",
      "Epoch [57/500], Step [20/22], Loss: 0.2257\n",
      "Epoch [58/500], Step [10/22], Loss: 0.5207\n",
      "Epoch [58/500], Step [20/22], Loss: 0.2104\n",
      "Epoch [59/500], Step [10/22], Loss: 0.6361\n",
      "Epoch [59/500], Step [20/22], Loss: 0.2175\n",
      "Epoch [60/500], Step [10/22], Loss: 0.5280\n",
      "Epoch [60/500], Step [20/22], Loss: 0.2254\n",
      "Epoch [61/500], Step [10/22], Loss: 0.4906\n",
      "Epoch [61/500], Step [20/22], Loss: 0.2176\n",
      "Epoch [62/500], Step [10/22], Loss: 0.5561\n",
      "Epoch [62/500], Step [20/22], Loss: 0.2346\n",
      "Epoch [63/500], Step [10/22], Loss: 0.5491\n",
      "Epoch [63/500], Step [20/22], Loss: 0.1941\n",
      "Epoch [64/500], Step [10/22], Loss: 0.5091\n",
      "Epoch [64/500], Step [20/22], Loss: 0.2113\n",
      "Epoch [65/500], Step [10/22], Loss: 0.4826\n",
      "Epoch [65/500], Step [20/22], Loss: 0.2426\n",
      "Epoch [66/500], Step [10/22], Loss: 0.5392\n",
      "Epoch [66/500], Step [20/22], Loss: 0.1988\n",
      "Epoch [67/500], Step [10/22], Loss: 0.4566\n",
      "Epoch [67/500], Step [20/22], Loss: 0.1790\n",
      "Epoch [68/500], Step [10/22], Loss: 0.4571\n",
      "Epoch [68/500], Step [20/22], Loss: 0.2189\n",
      "Epoch [69/500], Step [10/22], Loss: 0.4615\n",
      "Epoch [69/500], Step [20/22], Loss: 0.2163\n",
      "Epoch [70/500], Step [10/22], Loss: 0.4186\n",
      "Epoch [70/500], Step [20/22], Loss: 0.1890\n",
      "Epoch [71/500], Step [10/22], Loss: 0.4879\n",
      "Epoch [71/500], Step [20/22], Loss: 0.1897\n",
      "Epoch [72/500], Step [10/22], Loss: 0.4572\n",
      "Epoch [72/500], Step [20/22], Loss: 0.1849\n",
      "Epoch [73/500], Step [10/22], Loss: 0.4012\n",
      "Epoch [73/500], Step [20/22], Loss: 0.1828\n",
      "Epoch [74/500], Step [10/22], Loss: 0.4163\n",
      "Epoch [74/500], Step [20/22], Loss: 0.1674\n",
      "Epoch [75/500], Step [10/22], Loss: 0.5106\n",
      "Epoch [75/500], Step [20/22], Loss: 0.2493\n",
      "Epoch [76/500], Step [10/22], Loss: 0.4141\n",
      "Epoch [76/500], Step [20/22], Loss: 0.2167\n",
      "Epoch [77/500], Step [10/22], Loss: 0.5380\n",
      "Epoch [77/500], Step [20/22], Loss: 0.1739\n",
      "Epoch [78/500], Step [10/22], Loss: 0.5226\n",
      "Epoch [78/500], Step [20/22], Loss: 0.1699\n",
      "Epoch [79/500], Step [10/22], Loss: 0.4432\n",
      "Epoch [79/500], Step [20/22], Loss: 0.1904\n",
      "Epoch [80/500], Step [10/22], Loss: 0.5822\n",
      "Epoch [80/500], Step [20/22], Loss: 0.1644\n",
      "Epoch [81/500], Step [10/22], Loss: 0.4783\n",
      "Epoch [81/500], Step [20/22], Loss: 0.1518\n",
      "Epoch [82/500], Step [10/22], Loss: 0.4649\n",
      "Epoch [82/500], Step [20/22], Loss: 0.1644\n",
      "Epoch [83/500], Step [10/22], Loss: 0.4118\n",
      "Epoch [83/500], Step [20/22], Loss: 0.1501\n",
      "Epoch [84/500], Step [10/22], Loss: 0.4545\n",
      "Epoch [84/500], Step [20/22], Loss: 0.1445\n",
      "Epoch [85/500], Step [10/22], Loss: 0.3671\n",
      "Epoch [85/500], Step [20/22], Loss: 0.1473\n",
      "Epoch [86/500], Step [10/22], Loss: 0.4239\n",
      "Epoch [86/500], Step [20/22], Loss: 0.1396\n",
      "Epoch [87/500], Step [10/22], Loss: 0.4499\n",
      "Epoch [87/500], Step [20/22], Loss: 0.1468\n",
      "Epoch [88/500], Step [10/22], Loss: 0.3751\n",
      "Epoch [88/500], Step [20/22], Loss: 0.1466\n",
      "Epoch [89/500], Step [10/22], Loss: 0.4807\n",
      "Epoch [89/500], Step [20/22], Loss: 0.1390\n",
      "Epoch [90/500], Step [10/22], Loss: 0.3828\n",
      "Epoch [90/500], Step [20/22], Loss: 0.1204\n",
      "Epoch [91/500], Step [10/22], Loss: 0.4484\n",
      "Epoch [91/500], Step [20/22], Loss: 0.1500\n",
      "Epoch [92/500], Step [10/22], Loss: 0.4553\n",
      "Epoch [92/500], Step [20/22], Loss: 0.1143\n",
      "Epoch [93/500], Step [10/22], Loss: 0.5110\n",
      "Epoch [93/500], Step [20/22], Loss: 0.2988\n",
      "Epoch [94/500], Step [10/22], Loss: 0.4365\n",
      "Epoch [94/500], Step [20/22], Loss: 0.1328\n",
      "Epoch [95/500], Step [10/22], Loss: 0.5342\n",
      "Epoch [95/500], Step [20/22], Loss: 0.2079\n",
      "Epoch [96/500], Step [10/22], Loss: 0.3786\n",
      "Epoch [96/500], Step [20/22], Loss: 0.2196\n",
      "Epoch [97/500], Step [10/22], Loss: 0.3785\n",
      "Epoch [97/500], Step [20/22], Loss: 0.1922\n",
      "Epoch [98/500], Step [10/22], Loss: 0.5260\n",
      "Epoch [98/500], Step [20/22], Loss: 0.1447\n",
      "Epoch [99/500], Step [10/22], Loss: 0.3972\n",
      "Epoch [99/500], Step [20/22], Loss: 0.1405\n",
      "Epoch [100/500], Step [10/22], Loss: 0.4423\n",
      "Epoch [100/500], Step [20/22], Loss: 0.1597\n",
      "Epoch [101/500], Step [10/22], Loss: 0.4000\n",
      "Epoch [101/500], Step [20/22], Loss: 0.1226\n",
      "Epoch [102/500], Step [10/22], Loss: 0.4747\n",
      "Epoch [102/500], Step [20/22], Loss: 0.0940\n",
      "Epoch [103/500], Step [10/22], Loss: 0.3991\n",
      "Epoch [103/500], Step [20/22], Loss: 0.1425\n",
      "Epoch [104/500], Step [10/22], Loss: 0.3377\n",
      "Epoch [104/500], Step [20/22], Loss: 0.1735\n",
      "Epoch [105/500], Step [10/22], Loss: 0.5974\n",
      "Epoch [105/500], Step [20/22], Loss: 0.1375\n",
      "Epoch [106/500], Step [10/22], Loss: 0.3896\n",
      "Epoch [106/500], Step [20/22], Loss: 0.1703\n",
      "Epoch [107/500], Step [10/22], Loss: 0.4344\n",
      "Epoch [107/500], Step [20/22], Loss: 0.0995\n",
      "Epoch [108/500], Step [10/22], Loss: 0.4637\n",
      "Epoch [108/500], Step [20/22], Loss: 0.0966\n",
      "Epoch [109/500], Step [10/22], Loss: 0.3655\n",
      "Epoch [109/500], Step [20/22], Loss: 0.1006\n",
      "Epoch [110/500], Step [10/22], Loss: 0.3910\n",
      "Epoch [110/500], Step [20/22], Loss: 0.0739\n",
      "Epoch [111/500], Step [10/22], Loss: 0.3736\n",
      "Epoch [111/500], Step [20/22], Loss: 0.0892\n",
      "Epoch [112/500], Step [10/22], Loss: 0.3267\n",
      "Epoch [112/500], Step [20/22], Loss: 0.1948\n",
      "Epoch [113/500], Step [10/22], Loss: 0.4719\n",
      "Epoch [113/500], Step [20/22], Loss: 0.1629\n",
      "Epoch [114/500], Step [10/22], Loss: 0.4299\n",
      "Epoch [114/500], Step [20/22], Loss: 0.2394\n",
      "Epoch [115/500], Step [10/22], Loss: 0.3702\n",
      "Epoch [115/500], Step [20/22], Loss: 0.1890\n",
      "Epoch [116/500], Step [10/22], Loss: 0.4333\n",
      "Epoch [116/500], Step [20/22], Loss: 0.1990\n",
      "Epoch [117/500], Step [10/22], Loss: 0.3709\n",
      "Epoch [117/500], Step [20/22], Loss: 0.1128\n",
      "Epoch [118/500], Step [10/22], Loss: 0.3565\n",
      "Epoch [118/500], Step [20/22], Loss: 0.1359\n",
      "Epoch [119/500], Step [10/22], Loss: 0.3807\n",
      "Epoch [119/500], Step [20/22], Loss: 0.1316\n",
      "Epoch [120/500], Step [10/22], Loss: 0.3562\n",
      "Epoch [120/500], Step [20/22], Loss: 0.1239\n",
      "Epoch [121/500], Step [10/22], Loss: 0.3569\n",
      "Epoch [121/500], Step [20/22], Loss: 0.0936\n",
      "Epoch [122/500], Step [10/22], Loss: 0.3890\n",
      "Epoch [122/500], Step [20/22], Loss: 0.0927\n",
      "Epoch [123/500], Step [10/22], Loss: 0.3464\n",
      "Epoch [123/500], Step [20/22], Loss: 0.1118\n",
      "Epoch [124/500], Step [10/22], Loss: 0.4793\n",
      "Epoch [124/500], Step [20/22], Loss: 0.0959\n",
      "Epoch [125/500], Step [10/22], Loss: 0.3633\n",
      "Epoch [125/500], Step [20/22], Loss: 0.2298\n",
      "Epoch [126/500], Step [10/22], Loss: 0.4411\n",
      "Epoch [126/500], Step [20/22], Loss: 0.0993\n",
      "Epoch [127/500], Step [10/22], Loss: 0.4894\n",
      "Epoch [127/500], Step [20/22], Loss: 0.2228\n",
      "Epoch [128/500], Step [10/22], Loss: 0.3402\n",
      "Epoch [128/500], Step [20/22], Loss: 0.1929\n",
      "Epoch [129/500], Step [10/22], Loss: 0.3417\n",
      "Epoch [129/500], Step [20/22], Loss: 0.1057\n",
      "Epoch [130/500], Step [10/22], Loss: 0.4851\n",
      "Epoch [130/500], Step [20/22], Loss: 0.0935\n",
      "Epoch [131/500], Step [10/22], Loss: 0.3249\n",
      "Epoch [131/500], Step [20/22], Loss: 0.1029\n",
      "Epoch [132/500], Step [10/22], Loss: 0.3360\n",
      "Epoch [132/500], Step [20/22], Loss: 0.0941\n",
      "Epoch [133/500], Step [10/22], Loss: 0.3939\n",
      "Epoch [133/500], Step [20/22], Loss: 0.1032\n",
      "Epoch [134/500], Step [10/22], Loss: 0.4049\n",
      "Epoch [134/500], Step [20/22], Loss: 0.0709\n",
      "Epoch [135/500], Step [10/22], Loss: 0.3431\n",
      "Epoch [135/500], Step [20/22], Loss: 0.0873\n",
      "Epoch [136/500], Step [10/22], Loss: 0.3818\n",
      "Epoch [136/500], Step [20/22], Loss: 0.1094\n",
      "Epoch [137/500], Step [10/22], Loss: 0.4643\n",
      "Epoch [137/500], Step [20/22], Loss: 0.1448\n",
      "Epoch [138/500], Step [10/22], Loss: 0.3491\n",
      "Epoch [138/500], Step [20/22], Loss: 0.2484\n",
      "Epoch [139/500], Step [10/22], Loss: 0.3136\n",
      "Epoch [139/500], Step [20/22], Loss: 0.1230\n",
      "Epoch [140/500], Step [10/22], Loss: 0.4070\n",
      "Epoch [140/500], Step [20/22], Loss: 0.0765\n",
      "Epoch [141/500], Step [10/22], Loss: 0.3128\n",
      "Epoch [141/500], Step [20/22], Loss: 0.1475\n",
      "Epoch [142/500], Step [10/22], Loss: 0.2923\n",
      "Epoch [142/500], Step [20/22], Loss: 0.0835\n",
      "Epoch [143/500], Step [10/22], Loss: 0.2909\n",
      "Epoch [143/500], Step [20/22], Loss: 0.0851\n",
      "Epoch [144/500], Step [10/22], Loss: 0.3214\n",
      "Epoch [144/500], Step [20/22], Loss: 0.1880\n",
      "Epoch [145/500], Step [10/22], Loss: 0.3769\n",
      "Epoch [145/500], Step [20/22], Loss: 0.0803\n",
      "Epoch [146/500], Step [10/22], Loss: 0.2979\n",
      "Epoch [146/500], Step [20/22], Loss: 0.1165\n",
      "Epoch [147/500], Step [10/22], Loss: 0.3530\n",
      "Epoch [147/500], Step [20/22], Loss: 0.0940\n",
      "Epoch [148/500], Step [10/22], Loss: 0.2914\n",
      "Epoch [148/500], Step [20/22], Loss: 0.0834\n",
      "Epoch [149/500], Step [10/22], Loss: 0.3116\n",
      "Epoch [149/500], Step [20/22], Loss: 0.0698\n",
      "Epoch [150/500], Step [10/22], Loss: 0.4909\n",
      "Epoch [150/500], Step [20/22], Loss: 0.0859\n",
      "Epoch [151/500], Step [10/22], Loss: 0.3619\n",
      "Epoch [151/500], Step [20/22], Loss: 0.1016\n",
      "Epoch [152/500], Step [10/22], Loss: 0.3036\n",
      "Epoch [152/500], Step [20/22], Loss: 0.0926\n",
      "Epoch [153/500], Step [10/22], Loss: 0.2696\n",
      "Epoch [153/500], Step [20/22], Loss: 0.1116\n",
      "Epoch [154/500], Step [10/22], Loss: 0.4682\n",
      "Epoch [154/500], Step [20/22], Loss: 0.1162\n",
      "Epoch [155/500], Step [10/22], Loss: 0.4800\n",
      "Epoch [155/500], Step [20/22], Loss: 0.1115\n",
      "Epoch [156/500], Step [10/22], Loss: 0.2562\n",
      "Epoch [156/500], Step [20/22], Loss: 0.0783\n",
      "Epoch [157/500], Step [10/22], Loss: 0.2911\n",
      "Epoch [157/500], Step [20/22], Loss: 0.1535\n",
      "Epoch [158/500], Step [10/22], Loss: 0.3252\n",
      "Epoch [158/500], Step [20/22], Loss: 0.1608\n",
      "Epoch [159/500], Step [10/22], Loss: 0.2882\n",
      "Epoch [159/500], Step [20/22], Loss: 0.1095\n",
      "Epoch [160/500], Step [10/22], Loss: 0.3405\n",
      "Epoch [160/500], Step [20/22], Loss: 0.0746\n",
      "Epoch [161/500], Step [10/22], Loss: 0.2985\n",
      "Epoch [161/500], Step [20/22], Loss: 0.0652\n",
      "Epoch [162/500], Step [10/22], Loss: 0.2634\n",
      "Epoch [162/500], Step [20/22], Loss: 0.1060\n",
      "Epoch [163/500], Step [10/22], Loss: 0.3864\n",
      "Epoch [163/500], Step [20/22], Loss: 0.1398\n",
      "Epoch [164/500], Step [10/22], Loss: 0.2843\n",
      "Epoch [164/500], Step [20/22], Loss: 0.1127\n",
      "Epoch [165/500], Step [10/22], Loss: 0.3214\n",
      "Epoch [165/500], Step [20/22], Loss: 0.0843\n",
      "Epoch [166/500], Step [10/22], Loss: 0.2561\n",
      "Epoch [166/500], Step [20/22], Loss: 0.0840\n",
      "Epoch [167/500], Step [10/22], Loss: 0.3415\n",
      "Epoch [167/500], Step [20/22], Loss: 0.0693\n",
      "Epoch [168/500], Step [10/22], Loss: 0.2106\n",
      "Epoch [168/500], Step [20/22], Loss: 0.0869\n",
      "Epoch [169/500], Step [10/22], Loss: 0.2563\n",
      "Epoch [169/500], Step [20/22], Loss: 0.1401\n",
      "Epoch [170/500], Step [10/22], Loss: 0.4942\n",
      "Epoch [170/500], Step [20/22], Loss: 0.2246\n",
      "Epoch [171/500], Step [10/22], Loss: 0.2812\n",
      "Epoch [171/500], Step [20/22], Loss: 0.1500\n",
      "Epoch [172/500], Step [10/22], Loss: 0.3606\n",
      "Epoch [172/500], Step [20/22], Loss: 0.1390\n",
      "Epoch [173/500], Step [10/22], Loss: 0.4647\n",
      "Epoch [173/500], Step [20/22], Loss: 0.1469\n",
      "Epoch [174/500], Step [10/22], Loss: 0.2633\n",
      "Epoch [174/500], Step [20/22], Loss: 0.0977\n",
      "Epoch [175/500], Step [10/22], Loss: 0.2247\n",
      "Epoch [175/500], Step [20/22], Loss: 0.0892\n",
      "Epoch [176/500], Step [10/22], Loss: 0.4222\n",
      "Epoch [176/500], Step [20/22], Loss: 0.1004\n",
      "Epoch [177/500], Step [10/22], Loss: 0.2859\n",
      "Epoch [177/500], Step [20/22], Loss: 0.0901\n",
      "Epoch [178/500], Step [10/22], Loss: 0.3237\n",
      "Epoch [178/500], Step [20/22], Loss: 0.1123\n",
      "Epoch [179/500], Step [10/22], Loss: 0.3207\n",
      "Epoch [179/500], Step [20/22], Loss: 0.0913\n",
      "Epoch [180/500], Step [10/22], Loss: 0.2843\n",
      "Epoch [180/500], Step [20/22], Loss: 0.0906\n",
      "Epoch [181/500], Step [10/22], Loss: 0.3084\n",
      "Epoch [181/500], Step [20/22], Loss: 0.1018\n",
      "Epoch [182/500], Step [10/22], Loss: 0.2556\n",
      "Epoch [182/500], Step [20/22], Loss: 0.0789\n",
      "Epoch [183/500], Step [10/22], Loss: 0.2248\n",
      "Epoch [183/500], Step [20/22], Loss: 0.0754\n",
      "Epoch [184/500], Step [10/22], Loss: 0.2804\n",
      "Epoch [184/500], Step [20/22], Loss: 0.0666\n",
      "Epoch [185/500], Step [10/22], Loss: 0.2244\n",
      "Epoch [185/500], Step [20/22], Loss: 0.0656\n",
      "Epoch [186/500], Step [10/22], Loss: 0.2589\n",
      "Epoch [186/500], Step [20/22], Loss: 0.0791\n",
      "Epoch [187/500], Step [10/22], Loss: 0.2491\n",
      "Epoch [187/500], Step [20/22], Loss: 0.0679\n",
      "Epoch [188/500], Step [10/22], Loss: 0.2524\n",
      "Epoch [188/500], Step [20/22], Loss: 0.0930\n",
      "Epoch [189/500], Step [10/22], Loss: 0.2307\n",
      "Epoch [189/500], Step [20/22], Loss: 0.1469\n",
      "Epoch [190/500], Step [10/22], Loss: 0.2241\n",
      "Epoch [190/500], Step [20/22], Loss: 0.0829\n",
      "Epoch [191/500], Step [10/22], Loss: 0.2394\n",
      "Epoch [191/500], Step [20/22], Loss: 0.1080\n",
      "Epoch [192/500], Step [10/22], Loss: 0.2528\n",
      "Epoch [192/500], Step [20/22], Loss: 0.1483\n",
      "Epoch [193/500], Step [10/22], Loss: 0.3917\n",
      "Epoch [193/500], Step [20/22], Loss: 0.1402\n",
      "Epoch [194/500], Step [10/22], Loss: 0.2870\n",
      "Epoch [194/500], Step [20/22], Loss: 0.1042\n",
      "Epoch [195/500], Step [10/22], Loss: 0.3238\n",
      "Epoch [195/500], Step [20/22], Loss: 0.1056\n",
      "Epoch [196/500], Step [10/22], Loss: 0.3632\n",
      "Epoch [196/500], Step [20/22], Loss: 0.0739\n",
      "Epoch [197/500], Step [10/22], Loss: 0.1912\n",
      "Epoch [197/500], Step [20/22], Loss: 0.0516\n",
      "Epoch [198/500], Step [10/22], Loss: 0.2026\n",
      "Epoch [198/500], Step [20/22], Loss: 0.0553\n",
      "Epoch [199/500], Step [10/22], Loss: 0.2456\n",
      "Epoch [199/500], Step [20/22], Loss: 0.0977\n",
      "Epoch [200/500], Step [10/22], Loss: 0.3112\n",
      "Epoch [200/500], Step [20/22], Loss: 0.1049\n",
      "Epoch [201/500], Step [10/22], Loss: 0.2517\n",
      "Epoch [201/500], Step [20/22], Loss: 0.0581\n",
      "Epoch [202/500], Step [10/22], Loss: 0.2168\n",
      "Epoch [202/500], Step [20/22], Loss: 0.0984\n",
      "Epoch [203/500], Step [10/22], Loss: 0.2379\n",
      "Epoch [203/500], Step [20/22], Loss: 0.0982\n",
      "Epoch [204/500], Step [10/22], Loss: 0.2390\n",
      "Epoch [204/500], Step [20/22], Loss: 0.1512\n",
      "Epoch [205/500], Step [10/22], Loss: 0.2808\n",
      "Epoch [205/500], Step [20/22], Loss: 0.1058\n",
      "Epoch [206/500], Step [10/22], Loss: 0.2530\n",
      "Epoch [206/500], Step [20/22], Loss: 0.0738\n",
      "Epoch [207/500], Step [10/22], Loss: 0.2180\n",
      "Epoch [207/500], Step [20/22], Loss: 0.1732\n",
      "Epoch [208/500], Step [10/22], Loss: 0.2163\n",
      "Epoch [208/500], Step [20/22], Loss: 0.0777\n",
      "Epoch [209/500], Step [10/22], Loss: 0.1819\n",
      "Epoch [209/500], Step [20/22], Loss: 0.0570\n",
      "Epoch [210/500], Step [10/22], Loss: 0.2685\n",
      "Epoch [210/500], Step [20/22], Loss: 0.0714\n",
      "Epoch [211/500], Step [10/22], Loss: 0.2767\n",
      "Epoch [211/500], Step [20/22], Loss: 0.0901\n",
      "Epoch [212/500], Step [10/22], Loss: 0.2007\n",
      "Epoch [212/500], Step [20/22], Loss: 0.2939\n",
      "Epoch [213/500], Step [10/22], Loss: 0.2063\n",
      "Epoch [213/500], Step [20/22], Loss: 0.1434\n",
      "Epoch [214/500], Step [10/22], Loss: 0.1898\n",
      "Epoch [214/500], Step [20/22], Loss: 0.0840\n",
      "Epoch [215/500], Step [10/22], Loss: 0.2936\n",
      "Epoch [215/500], Step [20/22], Loss: 0.0752\n",
      "Epoch [216/500], Step [10/22], Loss: 0.1949\n",
      "Epoch [216/500], Step [20/22], Loss: 0.0992\n",
      "Epoch [217/500], Step [10/22], Loss: 0.1988\n",
      "Epoch [217/500], Step [20/22], Loss: 0.0707\n",
      "Epoch [218/500], Step [10/22], Loss: 0.3480\n",
      "Epoch [218/500], Step [20/22], Loss: 0.0414\n",
      "Epoch [219/500], Step [10/22], Loss: 0.1704\n",
      "Epoch [219/500], Step [20/22], Loss: 0.1168\n",
      "Epoch [220/500], Step [10/22], Loss: 0.2501\n",
      "Epoch [220/500], Step [20/22], Loss: 0.0837\n",
      "Epoch [221/500], Step [10/22], Loss: 0.1434\n",
      "Epoch [221/500], Step [20/22], Loss: 0.0582\n",
      "Epoch [222/500], Step [10/22], Loss: 0.2073\n",
      "Epoch [222/500], Step [20/22], Loss: 0.0465\n",
      "Epoch [223/500], Step [10/22], Loss: 0.2002\n",
      "Epoch [223/500], Step [20/22], Loss: 0.1348\n",
      "Epoch [224/500], Step [10/22], Loss: 0.3896\n",
      "Epoch [224/500], Step [20/22], Loss: 0.0905\n",
      "Epoch [225/500], Step [10/22], Loss: 0.2063\n",
      "Epoch [225/500], Step [20/22], Loss: 0.0827\n",
      "Epoch [226/500], Step [10/22], Loss: 0.2150\n",
      "Epoch [226/500], Step [20/22], Loss: 0.0784\n",
      "Epoch [227/500], Step [10/22], Loss: 0.1758\n",
      "Epoch [227/500], Step [20/22], Loss: 0.0616\n",
      "Epoch [228/500], Step [10/22], Loss: 0.1634\n",
      "Epoch [228/500], Step [20/22], Loss: 0.0649\n",
      "Epoch [229/500], Step [10/22], Loss: 0.1881\n",
      "Epoch [229/500], Step [20/22], Loss: 0.0623\n",
      "Epoch [230/500], Step [10/22], Loss: 0.2499\n",
      "Epoch [230/500], Step [20/22], Loss: 0.0649\n",
      "Epoch [231/500], Step [10/22], Loss: 0.3278\n",
      "Epoch [231/500], Step [20/22], Loss: 0.0778\n",
      "Epoch [232/500], Step [10/22], Loss: 0.2134\n",
      "Epoch [232/500], Step [20/22], Loss: 0.1131\n",
      "Epoch [233/500], Step [10/22], Loss: 0.3353\n",
      "Epoch [233/500], Step [20/22], Loss: 0.1231\n",
      "Epoch [234/500], Step [10/22], Loss: 0.2437\n",
      "Epoch [234/500], Step [20/22], Loss: 0.1100\n",
      "Epoch [235/500], Step [10/22], Loss: 0.2224\n",
      "Epoch [235/500], Step [20/22], Loss: 0.0754\n",
      "Epoch [236/500], Step [10/22], Loss: 0.2715\n",
      "Epoch [236/500], Step [20/22], Loss: 0.0648\n",
      "Epoch [237/500], Step [10/22], Loss: 0.2199\n",
      "Epoch [237/500], Step [20/22], Loss: 0.0529\n",
      "Epoch [238/500], Step [10/22], Loss: 0.2222\n",
      "Epoch [238/500], Step [20/22], Loss: 0.0424\n",
      "Epoch [239/500], Step [10/22], Loss: 0.2202\n",
      "Epoch [239/500], Step [20/22], Loss: 0.0382\n",
      "Epoch [240/500], Step [10/22], Loss: 0.1314\n",
      "Epoch [240/500], Step [20/22], Loss: 0.0446\n",
      "Epoch [241/500], Step [10/22], Loss: 0.1703\n",
      "Epoch [241/500], Step [20/22], Loss: 0.0479\n",
      "Epoch [242/500], Step [10/22], Loss: 0.3643\n",
      "Epoch [242/500], Step [20/22], Loss: 0.1030\n",
      "Epoch [243/500], Step [10/22], Loss: 0.2462\n",
      "Epoch [243/500], Step [20/22], Loss: 0.0960\n",
      "Epoch [244/500], Step [10/22], Loss: 0.2194\n",
      "Epoch [244/500], Step [20/22], Loss: 0.0599\n",
      "Epoch [245/500], Step [10/22], Loss: 0.1845\n",
      "Epoch [245/500], Step [20/22], Loss: 0.0699\n",
      "Epoch [246/500], Step [10/22], Loss: 0.1694\n",
      "Epoch [246/500], Step [20/22], Loss: 0.0668\n",
      "Epoch [247/500], Step [10/22], Loss: 0.1611\n",
      "Epoch [247/500], Step [20/22], Loss: 0.0910\n",
      "Epoch [248/500], Step [10/22], Loss: 0.1688\n",
      "Epoch [248/500], Step [20/22], Loss: 0.0475\n",
      "Epoch [249/500], Step [10/22], Loss: 0.1370\n",
      "Epoch [249/500], Step [20/22], Loss: 0.0445\n",
      "Epoch [250/500], Step [10/22], Loss: 0.1546\n",
      "Epoch [250/500], Step [20/22], Loss: 0.0478\n",
      "Epoch [251/500], Step [10/22], Loss: 0.1469\n",
      "Epoch [251/500], Step [20/22], Loss: 0.0485\n",
      "Epoch [252/500], Step [10/22], Loss: 0.1066\n",
      "Epoch [252/500], Step [20/22], Loss: 0.0472\n",
      "Epoch [253/500], Step [10/22], Loss: 0.1713\n",
      "Epoch [253/500], Step [20/22], Loss: 0.0287\n",
      "Epoch [254/500], Step [10/22], Loss: 0.1280\n",
      "Epoch [254/500], Step [20/22], Loss: 0.0256\n",
      "Epoch [255/500], Step [10/22], Loss: 0.1503\n",
      "Epoch [255/500], Step [20/22], Loss: 0.0980\n",
      "Epoch [256/500], Step [10/22], Loss: 0.3021\n",
      "Epoch [256/500], Step [20/22], Loss: 0.0233\n",
      "Epoch [257/500], Step [10/22], Loss: 0.2634\n",
      "Epoch [257/500], Step [20/22], Loss: 0.0776\n",
      "Epoch [258/500], Step [10/22], Loss: 0.2095\n",
      "Epoch [258/500], Step [20/22], Loss: 0.0888\n",
      "Epoch [259/500], Step [10/22], Loss: 0.1704\n",
      "Epoch [259/500], Step [20/22], Loss: 0.1467\n",
      "Epoch [260/500], Step [10/22], Loss: 0.1114\n",
      "Epoch [260/500], Step [20/22], Loss: 0.0575\n",
      "Epoch [261/500], Step [10/22], Loss: 0.1477\n",
      "Epoch [261/500], Step [20/22], Loss: 0.0517\n",
      "Epoch [262/500], Step [10/22], Loss: 0.1709\n",
      "Epoch [262/500], Step [20/22], Loss: 0.0637\n",
      "Epoch [263/500], Step [10/22], Loss: 0.1439\n",
      "Epoch [263/500], Step [20/22], Loss: 0.0464\n",
      "Epoch [264/500], Step [10/22], Loss: 0.2390\n",
      "Epoch [264/500], Step [20/22], Loss: 0.0372\n",
      "Epoch [265/500], Step [10/22], Loss: 0.0973\n",
      "Epoch [265/500], Step [20/22], Loss: 0.0587\n",
      "Epoch [266/500], Step [10/22], Loss: 0.3830\n",
      "Epoch [266/500], Step [20/22], Loss: 0.0463\n",
      "Epoch [267/500], Step [10/22], Loss: 0.1541\n",
      "Epoch [267/500], Step [20/22], Loss: 0.0507\n",
      "Epoch [268/500], Step [10/22], Loss: 0.1419\n",
      "Epoch [268/500], Step [20/22], Loss: 0.0956\n",
      "Epoch [269/500], Step [10/22], Loss: 0.1635\n",
      "Epoch [269/500], Step [20/22], Loss: 0.0830\n",
      "Epoch [270/500], Step [10/22], Loss: 0.2967\n",
      "Epoch [270/500], Step [20/22], Loss: 0.0336\n",
      "Epoch [271/500], Step [10/22], Loss: 0.1499\n",
      "Epoch [271/500], Step [20/22], Loss: 0.0671\n",
      "Epoch [272/500], Step [10/22], Loss: 0.2670\n",
      "Epoch [272/500], Step [20/22], Loss: 0.0286\n",
      "Epoch [273/500], Step [10/22], Loss: 0.1628\n",
      "Epoch [273/500], Step [20/22], Loss: 0.0651\n",
      "Epoch [274/500], Step [10/22], Loss: 0.1285\n",
      "Epoch [274/500], Step [20/22], Loss: 0.1933\n",
      "Epoch [275/500], Step [10/22], Loss: 0.1627\n",
      "Epoch [275/500], Step [20/22], Loss: 0.1082\n",
      "Epoch [276/500], Step [10/22], Loss: 0.1602\n",
      "Epoch [276/500], Step [20/22], Loss: 0.1365\n",
      "Epoch [277/500], Step [10/22], Loss: 0.1937\n",
      "Epoch [277/500], Step [20/22], Loss: 0.1117\n",
      "Epoch [278/500], Step [10/22], Loss: 0.2555\n",
      "Epoch [278/500], Step [20/22], Loss: 0.1024\n",
      "Epoch [279/500], Step [10/22], Loss: 0.1845\n",
      "Epoch [279/500], Step [20/22], Loss: 0.1664\n",
      "Epoch [280/500], Step [10/22], Loss: 0.1916\n",
      "Epoch [280/500], Step [20/22], Loss: 0.1004\n",
      "Epoch [281/500], Step [10/22], Loss: 0.1701\n",
      "Epoch [281/500], Step [20/22], Loss: 0.0667\n",
      "Epoch [282/500], Step [10/22], Loss: 0.1555\n",
      "Epoch [282/500], Step [20/22], Loss: 0.0592\n",
      "Epoch [283/500], Step [10/22], Loss: 0.1537\n",
      "Epoch [283/500], Step [20/22], Loss: 0.0393\n",
      "Epoch [284/500], Step [10/22], Loss: 0.1153\n",
      "Epoch [284/500], Step [20/22], Loss: 0.0441\n",
      "Epoch [285/500], Step [10/22], Loss: 0.1434\n",
      "Epoch [285/500], Step [20/22], Loss: 0.0489\n",
      "Epoch [286/500], Step [10/22], Loss: 0.1892\n",
      "Epoch [286/500], Step [20/22], Loss: 0.0281\n",
      "Epoch [287/500], Step [10/22], Loss: 0.1174\n",
      "Epoch [287/500], Step [20/22], Loss: 0.0396\n",
      "Epoch [288/500], Step [10/22], Loss: 0.1802\n",
      "Epoch [288/500], Step [20/22], Loss: 0.0490\n",
      "Epoch [289/500], Step [10/22], Loss: 0.1820\n",
      "Epoch [289/500], Step [20/22], Loss: 0.0910\n",
      "Epoch [290/500], Step [10/22], Loss: 0.3499\n",
      "Epoch [290/500], Step [20/22], Loss: 0.0843\n",
      "Epoch [291/500], Step [10/22], Loss: 0.3988\n",
      "Epoch [291/500], Step [20/22], Loss: 0.1214\n",
      "Epoch [292/500], Step [10/22], Loss: 0.1259\n",
      "Epoch [292/500], Step [20/22], Loss: 0.1832\n",
      "Epoch [293/500], Step [10/22], Loss: 0.2141\n",
      "Epoch [293/500], Step [20/22], Loss: 0.0883\n",
      "Epoch [294/500], Step [10/22], Loss: 0.1434\n",
      "Epoch [294/500], Step [20/22], Loss: 0.0619\n",
      "Epoch [295/500], Step [10/22], Loss: 0.1409\n",
      "Epoch [295/500], Step [20/22], Loss: 0.0729\n",
      "Epoch [296/500], Step [10/22], Loss: 0.1895\n",
      "Epoch [296/500], Step [20/22], Loss: 0.0764\n",
      "Epoch [297/500], Step [10/22], Loss: 0.2003\n",
      "Epoch [297/500], Step [20/22], Loss: 0.0697\n",
      "Epoch [298/500], Step [10/22], Loss: 0.1558\n",
      "Epoch [298/500], Step [20/22], Loss: 0.0537\n",
      "Epoch [299/500], Step [10/22], Loss: 0.1248\n",
      "Epoch [299/500], Step [20/22], Loss: 0.0947\n",
      "Epoch [300/500], Step [10/22], Loss: 0.1478\n",
      "Epoch [300/500], Step [20/22], Loss: 0.0410\n",
      "Epoch [301/500], Step [10/22], Loss: 0.1064\n",
      "Epoch [301/500], Step [20/22], Loss: 0.0392\n",
      "Epoch [302/500], Step [10/22], Loss: 0.1400\n",
      "Epoch [302/500], Step [20/22], Loss: 0.0417\n",
      "Epoch [303/500], Step [10/22], Loss: 0.1168\n",
      "Epoch [303/500], Step [20/22], Loss: 0.0264\n",
      "Epoch [304/500], Step [10/22], Loss: 0.1109\n",
      "Epoch [304/500], Step [20/22], Loss: 0.0449\n",
      "Epoch [305/500], Step [10/22], Loss: 0.1717\n",
      "Epoch [305/500], Step [20/22], Loss: 0.0270\n",
      "Epoch [306/500], Step [10/22], Loss: 0.0967\n",
      "Epoch [306/500], Step [20/22], Loss: 0.0520\n",
      "Epoch [307/500], Step [10/22], Loss: 0.0880\n",
      "Epoch [307/500], Step [20/22], Loss: 0.1013\n",
      "Epoch [308/500], Step [10/22], Loss: 0.2835\n",
      "Epoch [308/500], Step [20/22], Loss: 0.0276\n",
      "Epoch [309/500], Step [10/22], Loss: 0.1162\n",
      "Epoch [309/500], Step [20/22], Loss: 0.0371\n",
      "Epoch [310/500], Step [10/22], Loss: 0.1646\n",
      "Epoch [310/500], Step [20/22], Loss: 0.0400\n",
      "Epoch [311/500], Step [10/22], Loss: 0.1019\n",
      "Epoch [311/500], Step [20/22], Loss: 0.0402\n",
      "Epoch [312/500], Step [10/22], Loss: 0.0808\n",
      "Epoch [312/500], Step [20/22], Loss: 0.1629\n",
      "Epoch [313/500], Step [10/22], Loss: 0.2563\n",
      "Epoch [313/500], Step [20/22], Loss: 0.0370\n",
      "Epoch [314/500], Step [10/22], Loss: 0.1246\n",
      "Epoch [314/500], Step [20/22], Loss: 0.0818\n",
      "Epoch [315/500], Step [10/22], Loss: 0.3072\n",
      "Epoch [315/500], Step [20/22], Loss: 0.1213\n",
      "Epoch [316/500], Step [10/22], Loss: 0.1701\n",
      "Epoch [316/500], Step [20/22], Loss: 0.0516\n",
      "Epoch [317/500], Step [10/22], Loss: 0.1222\n",
      "Epoch [317/500], Step [20/22], Loss: 0.1099\n",
      "Epoch [318/500], Step [10/22], Loss: 0.1696\n",
      "Epoch [318/500], Step [20/22], Loss: 0.0635\n",
      "Epoch [319/500], Step [10/22], Loss: 0.1098\n",
      "Epoch [319/500], Step [20/22], Loss: 0.0338\n",
      "Epoch [320/500], Step [10/22], Loss: 0.1175\n",
      "Epoch [320/500], Step [20/22], Loss: 0.0391\n",
      "Epoch [321/500], Step [10/22], Loss: 0.1181\n",
      "Epoch [321/500], Step [20/22], Loss: 0.0262\n",
      "Epoch [322/500], Step [10/22], Loss: 0.0779\n",
      "Epoch [322/500], Step [20/22], Loss: 0.0332\n",
      "Epoch [323/500], Step [10/22], Loss: 0.0769\n",
      "Epoch [323/500], Step [20/22], Loss: 0.0371\n",
      "Epoch [324/500], Step [10/22], Loss: 0.2064\n",
      "Epoch [324/500], Step [20/22], Loss: 0.0469\n",
      "Epoch [325/500], Step [10/22], Loss: 0.0731\n",
      "Epoch [325/500], Step [20/22], Loss: 0.0858\n",
      "Epoch [326/500], Step [10/22], Loss: 0.1342\n",
      "Epoch [326/500], Step [20/22], Loss: 0.1213\n",
      "Epoch [327/500], Step [10/22], Loss: 0.1270\n",
      "Epoch [327/500], Step [20/22], Loss: 0.0965\n",
      "Epoch [328/500], Step [10/22], Loss: 0.1332\n",
      "Epoch [328/500], Step [20/22], Loss: 0.0971\n",
      "Epoch [329/500], Step [10/22], Loss: 0.1278\n",
      "Epoch [329/500], Step [20/22], Loss: 0.0363\n",
      "Epoch [330/500], Step [10/22], Loss: 0.0924\n",
      "Epoch [330/500], Step [20/22], Loss: 0.0346\n",
      "Epoch [331/500], Step [10/22], Loss: 0.0826\n",
      "Epoch [331/500], Step [20/22], Loss: 0.0749\n",
      "Epoch [332/500], Step [10/22], Loss: 0.0906\n",
      "Epoch [332/500], Step [20/22], Loss: 0.0288\n",
      "Epoch [333/500], Step [10/22], Loss: 0.0803\n",
      "Epoch [333/500], Step [20/22], Loss: 0.0227\n",
      "Epoch [334/500], Step [10/22], Loss: 0.2179\n",
      "Epoch [334/500], Step [20/22], Loss: 0.0440\n",
      "Epoch [335/500], Step [10/22], Loss: 0.1627\n",
      "Epoch [335/500], Step [20/22], Loss: 0.0135\n",
      "Epoch [336/500], Step [10/22], Loss: 0.1381\n",
      "Epoch [336/500], Step [20/22], Loss: 0.0197\n",
      "Epoch [337/500], Step [10/22], Loss: 0.1343\n",
      "Epoch [337/500], Step [20/22], Loss: 0.0696\n",
      "Epoch [338/500], Step [10/22], Loss: 0.1306\n",
      "Epoch [338/500], Step [20/22], Loss: 0.2365\n",
      "Epoch [339/500], Step [10/22], Loss: 0.1242\n",
      "Epoch [339/500], Step [20/22], Loss: 0.0537\n",
      "Epoch [340/500], Step [10/22], Loss: 0.1022\n",
      "Epoch [340/500], Step [20/22], Loss: 0.0969\n",
      "Epoch [341/500], Step [10/22], Loss: 0.0751\n",
      "Epoch [341/500], Step [20/22], Loss: 0.0333\n",
      "Epoch [342/500], Step [10/22], Loss: 0.0790\n",
      "Epoch [342/500], Step [20/22], Loss: 0.1179\n",
      "Epoch [343/500], Step [10/22], Loss: 0.1054\n",
      "Epoch [343/500], Step [20/22], Loss: 0.0712\n",
      "Epoch [344/500], Step [10/22], Loss: 0.0824\n",
      "Epoch [344/500], Step [20/22], Loss: 0.0190\n",
      "Epoch [345/500], Step [10/22], Loss: 0.1238\n",
      "Epoch [345/500], Step [20/22], Loss: 0.0323\n",
      "Epoch [346/500], Step [10/22], Loss: 0.1179\n",
      "Epoch [346/500], Step [20/22], Loss: 0.0392\n",
      "Epoch [347/500], Step [10/22], Loss: 0.1434\n",
      "Epoch [347/500], Step [20/22], Loss: 0.0924\n",
      "Epoch [348/500], Step [10/22], Loss: 0.1947\n",
      "Epoch [348/500], Step [20/22], Loss: 0.0949\n",
      "Epoch [349/500], Step [10/22], Loss: 0.1011\n",
      "Epoch [349/500], Step [20/22], Loss: 0.0729\n",
      "Epoch [350/500], Step [10/22], Loss: 0.1267\n",
      "Epoch [350/500], Step [20/22], Loss: 0.0409\n",
      "Epoch [351/500], Step [10/22], Loss: 0.1733\n",
      "Epoch [351/500], Step [20/22], Loss: 0.0351\n",
      "Epoch [352/500], Step [10/22], Loss: 0.2360\n",
      "Epoch [352/500], Step [20/22], Loss: 0.0348\n",
      "Epoch [353/500], Step [10/22], Loss: 0.1163\n",
      "Epoch [353/500], Step [20/22], Loss: 0.0587\n",
      "Epoch [354/500], Step [10/22], Loss: 0.1014\n",
      "Epoch [354/500], Step [20/22], Loss: 0.0330\n",
      "Epoch [355/500], Step [10/22], Loss: 0.0793\n",
      "Epoch [355/500], Step [20/22], Loss: 0.0311\n",
      "Epoch [356/500], Step [10/22], Loss: 0.1158\n",
      "Epoch [356/500], Step [20/22], Loss: 0.0167\n",
      "Epoch [357/500], Step [10/22], Loss: 0.0773\n",
      "Epoch [357/500], Step [20/22], Loss: 0.0238\n",
      "Epoch [358/500], Step [10/22], Loss: 0.1283\n",
      "Epoch [358/500], Step [20/22], Loss: 0.0379\n",
      "Epoch [359/500], Step [10/22], Loss: 0.0729\n",
      "Epoch [359/500], Step [20/22], Loss: 0.0200\n",
      "Epoch [360/500], Step [10/22], Loss: 0.0706\n",
      "Epoch [360/500], Step [20/22], Loss: 0.1187\n",
      "Epoch [361/500], Step [10/22], Loss: 0.1736\n",
      "Epoch [361/500], Step [20/22], Loss: 0.0189\n",
      "Epoch [362/500], Step [10/22], Loss: 0.0447\n",
      "Epoch [362/500], Step [20/22], Loss: 0.4040\n",
      "Epoch [363/500], Step [10/22], Loss: 0.1378\n",
      "Epoch [363/500], Step [20/22], Loss: 0.1756\n",
      "Epoch [364/500], Step [10/22], Loss: 0.1019\n",
      "Epoch [364/500], Step [20/22], Loss: 0.0542\n",
      "Epoch [365/500], Step [10/22], Loss: 0.0809\n",
      "Epoch [365/500], Step [20/22], Loss: 0.0682\n",
      "Epoch [366/500], Step [10/22], Loss: 0.1522\n",
      "Epoch [366/500], Step [20/22], Loss: 0.0496\n",
      "Epoch [367/500], Step [10/22], Loss: 0.1183\n",
      "Epoch [367/500], Step [20/22], Loss: 0.0447\n",
      "Epoch [368/500], Step [10/22], Loss: 0.1180\n",
      "Epoch [368/500], Step [20/22], Loss: 0.0512\n",
      "Epoch [369/500], Step [10/22], Loss: 0.1538\n",
      "Epoch [369/500], Step [20/22], Loss: 0.0409\n",
      "Epoch [370/500], Step [10/22], Loss: 0.1057\n",
      "Epoch [370/500], Step [20/22], Loss: 0.0592\n",
      "Epoch [371/500], Step [10/22], Loss: 0.1225\n",
      "Epoch [371/500], Step [20/22], Loss: 0.0860\n",
      "Epoch [372/500], Step [10/22], Loss: 0.0814\n",
      "Epoch [372/500], Step [20/22], Loss: 0.0523\n",
      "Epoch [373/500], Step [10/22], Loss: 0.0824\n",
      "Epoch [373/500], Step [20/22], Loss: 0.0231\n",
      "Epoch [374/500], Step [10/22], Loss: 0.1309\n",
      "Epoch [374/500], Step [20/22], Loss: 0.0427\n",
      "Epoch [375/500], Step [10/22], Loss: 0.0650\n",
      "Epoch [375/500], Step [20/22], Loss: 0.0265\n",
      "Epoch [376/500], Step [10/22], Loss: 0.0630\n",
      "Epoch [376/500], Step [20/22], Loss: 0.0384\n",
      "Epoch [377/500], Step [10/22], Loss: 0.0896\n",
      "Epoch [377/500], Step [20/22], Loss: 0.0182\n",
      "Epoch [378/500], Step [10/22], Loss: 0.0564\n",
      "Epoch [378/500], Step [20/22], Loss: 0.0272\n",
      "Epoch [379/500], Step [10/22], Loss: 0.1075\n",
      "Epoch [379/500], Step [20/22], Loss: 0.0412\n",
      "Epoch [380/500], Step [10/22], Loss: 0.0760\n",
      "Epoch [380/500], Step [20/22], Loss: 0.0210\n",
      "Epoch [381/500], Step [10/22], Loss: 0.0669\n",
      "Epoch [381/500], Step [20/22], Loss: 0.0431\n",
      "Epoch [382/500], Step [10/22], Loss: 0.0769\n",
      "Epoch [382/500], Step [20/22], Loss: 0.0284\n",
      "Epoch [383/500], Step [10/22], Loss: 0.1410\n",
      "Epoch [383/500], Step [20/22], Loss: 0.0417\n",
      "Epoch [384/500], Step [10/22], Loss: 0.0597\n",
      "Epoch [384/500], Step [20/22], Loss: 0.0470\n",
      "Epoch [385/500], Step [10/22], Loss: 0.1173\n",
      "Epoch [385/500], Step [20/22], Loss: 0.1513\n",
      "Epoch [386/500], Step [10/22], Loss: 0.2764\n",
      "Epoch [386/500], Step [20/22], Loss: 0.0625\n",
      "Epoch [387/500], Step [10/22], Loss: 0.2357\n",
      "Epoch [387/500], Step [20/22], Loss: 0.0433\n",
      "Epoch [388/500], Step [10/22], Loss: 0.1231\n",
      "Epoch [388/500], Step [20/22], Loss: 0.0319\n",
      "Epoch [389/500], Step [10/22], Loss: 0.0910\n",
      "Epoch [389/500], Step [20/22], Loss: 0.0321\n",
      "Epoch [390/500], Step [10/22], Loss: 0.0974\n",
      "Epoch [390/500], Step [20/22], Loss: 0.0401\n",
      "Epoch [391/500], Step [10/22], Loss: 0.1631\n",
      "Epoch [391/500], Step [20/22], Loss: 0.0172\n",
      "Epoch [392/500], Step [10/22], Loss: 0.1318\n",
      "Epoch [392/500], Step [20/22], Loss: 0.0211\n",
      "Epoch [393/500], Step [10/22], Loss: 0.1274\n",
      "Epoch [393/500], Step [20/22], Loss: 0.1069\n",
      "Epoch [394/500], Step [10/22], Loss: 0.1138\n",
      "Epoch [394/500], Step [20/22], Loss: 0.0156\n",
      "Epoch [395/500], Step [10/22], Loss: 0.0673\n",
      "Epoch [395/500], Step [20/22], Loss: 0.0409\n",
      "Epoch [396/500], Step [10/22], Loss: 0.1951\n",
      "Epoch [396/500], Step [20/22], Loss: 0.0183\n",
      "Epoch [397/500], Step [10/22], Loss: 0.3425\n",
      "Epoch [397/500], Step [20/22], Loss: 0.0129\n",
      "Epoch [398/500], Step [10/22], Loss: 0.0767\n",
      "Epoch [398/500], Step [20/22], Loss: 0.0198\n",
      "Epoch [399/500], Step [10/22], Loss: 0.0916\n",
      "Epoch [399/500], Step [20/22], Loss: 0.0122\n",
      "Epoch [400/500], Step [10/22], Loss: 0.0471\n",
      "Epoch [400/500], Step [20/22], Loss: 0.0286\n",
      "Epoch [401/500], Step [10/22], Loss: 0.0936\n",
      "Epoch [401/500], Step [20/22], Loss: 0.0184\n",
      "Epoch [402/500], Step [10/22], Loss: 0.0450\n",
      "Epoch [402/500], Step [20/22], Loss: 0.0402\n",
      "Epoch [403/500], Step [10/22], Loss: 0.0574\n",
      "Epoch [403/500], Step [20/22], Loss: 0.0302\n",
      "Epoch [404/500], Step [10/22], Loss: 0.1136\n",
      "Epoch [404/500], Step [20/22], Loss: 0.0045\n",
      "Epoch [405/500], Step [10/22], Loss: 0.0590\n",
      "Epoch [405/500], Step [20/22], Loss: 0.0257\n",
      "Epoch [406/500], Step [10/22], Loss: 0.1610\n",
      "Epoch [406/500], Step [20/22], Loss: 0.2469\n",
      "Epoch [407/500], Step [10/22], Loss: 0.3961\n",
      "Epoch [407/500], Step [20/22], Loss: 0.1230\n",
      "Epoch [408/500], Step [10/22], Loss: 0.0433\n",
      "Epoch [408/500], Step [20/22], Loss: 0.1175\n",
      "Epoch [409/500], Step [10/22], Loss: 0.1756\n",
      "Epoch [409/500], Step [20/22], Loss: 0.1692\n",
      "Epoch [410/500], Step [10/22], Loss: 0.1990\n",
      "Epoch [410/500], Step [20/22], Loss: 0.1309\n",
      "Epoch [411/500], Step [10/22], Loss: 0.1235\n",
      "Epoch [411/500], Step [20/22], Loss: 0.0247\n",
      "Epoch [412/500], Step [10/22], Loss: 0.0926\n",
      "Epoch [412/500], Step [20/22], Loss: 0.0196\n",
      "Epoch [413/500], Step [10/22], Loss: 0.0929\n",
      "Epoch [413/500], Step [20/22], Loss: 0.0207\n",
      "Epoch [414/500], Step [10/22], Loss: 0.0962\n",
      "Epoch [414/500], Step [20/22], Loss: 0.0104\n",
      "Epoch [415/500], Step [10/22], Loss: 0.1062\n",
      "Epoch [415/500], Step [20/22], Loss: 0.0096\n",
      "Epoch [416/500], Step [10/22], Loss: 0.0708\n",
      "Epoch [416/500], Step [20/22], Loss: 0.0286\n",
      "Epoch [417/500], Step [10/22], Loss: 0.0626\n",
      "Epoch [417/500], Step [20/22], Loss: 0.1135\n",
      "Epoch [418/500], Step [10/22], Loss: 0.0949\n",
      "Epoch [418/500], Step [20/22], Loss: 0.0402\n",
      "Epoch [419/500], Step [10/22], Loss: 0.0814\n",
      "Epoch [419/500], Step [20/22], Loss: 0.1312\n",
      "Epoch [420/500], Step [10/22], Loss: 0.0834\n",
      "Epoch [420/500], Step [20/22], Loss: 0.0222\n",
      "Epoch [421/500], Step [10/22], Loss: 0.0795\n",
      "Epoch [421/500], Step [20/22], Loss: 0.0225\n",
      "Epoch [422/500], Step [10/22], Loss: 0.0817\n",
      "Epoch [422/500], Step [20/22], Loss: 0.0079\n",
      "Epoch [423/500], Step [10/22], Loss: 0.0598\n",
      "Epoch [423/500], Step [20/22], Loss: 0.0078\n",
      "Epoch [424/500], Step [10/22], Loss: 0.0640\n",
      "Epoch [424/500], Step [20/22], Loss: 0.0068\n",
      "Epoch [425/500], Step [10/22], Loss: 0.0525\n",
      "Epoch [425/500], Step [20/22], Loss: 0.0078\n",
      "Epoch [426/500], Step [10/22], Loss: 0.0446\n",
      "Epoch [426/500], Step [20/22], Loss: 0.0036\n",
      "Epoch [427/500], Step [10/22], Loss: 0.0445\n",
      "Epoch [427/500], Step [20/22], Loss: 0.0273\n",
      "Epoch [428/500], Step [10/22], Loss: 0.1403\n",
      "Epoch [428/500], Step [20/22], Loss: 0.2558\n",
      "Epoch [429/500], Step [10/22], Loss: 0.1071\n",
      "Epoch [429/500], Step [20/22], Loss: 0.0928\n",
      "Epoch [430/500], Step [10/22], Loss: 0.1926\n",
      "Epoch [430/500], Step [20/22], Loss: 0.0599\n",
      "Epoch [431/500], Step [10/22], Loss: 0.1726\n",
      "Epoch [431/500], Step [20/22], Loss: 0.1041\n",
      "Epoch [432/500], Step [10/22], Loss: 0.2026\n",
      "Epoch [432/500], Step [20/22], Loss: 0.0428\n",
      "Epoch [433/500], Step [10/22], Loss: 0.1108\n",
      "Epoch [433/500], Step [20/22], Loss: 0.0780\n",
      "Epoch [434/500], Step [10/22], Loss: 0.2080\n",
      "Epoch [434/500], Step [20/22], Loss: 0.0180\n",
      "Epoch [435/500], Step [10/22], Loss: 0.0784\n",
      "Epoch [435/500], Step [20/22], Loss: 0.0559\n",
      "Epoch [436/500], Step [10/22], Loss: 0.1520\n",
      "Epoch [436/500], Step [20/22], Loss: 0.1333\n",
      "Epoch [437/500], Step [10/22], Loss: 0.0936\n",
      "Epoch [437/500], Step [20/22], Loss: 0.2712\n",
      "Epoch [438/500], Step [10/22], Loss: 0.1387\n",
      "Epoch [438/500], Step [20/22], Loss: 0.0751\n",
      "Epoch [439/500], Step [10/22], Loss: 0.1023\n",
      "Epoch [439/500], Step [20/22], Loss: 0.0414\n",
      "Epoch [440/500], Step [10/22], Loss: 0.1092\n",
      "Epoch [440/500], Step [20/22], Loss: 0.0231\n",
      "Epoch [441/500], Step [10/22], Loss: 0.0748\n",
      "Epoch [441/500], Step [20/22], Loss: 0.0550\n",
      "Epoch [442/500], Step [10/22], Loss: 0.0685\n",
      "Epoch [442/500], Step [20/22], Loss: 0.0254\n",
      "Epoch [443/500], Step [10/22], Loss: 0.0492\n",
      "Epoch [443/500], Step [20/22], Loss: 0.0213\n",
      "Epoch [444/500], Step [10/22], Loss: 0.0537\n",
      "Epoch [444/500], Step [20/22], Loss: 0.0173\n",
      "Epoch [445/500], Step [10/22], Loss: 0.0414\n",
      "Epoch [445/500], Step [20/22], Loss: 0.0115\n",
      "Epoch [446/500], Step [10/22], Loss: 0.0295\n",
      "Epoch [446/500], Step [20/22], Loss: 0.0131\n",
      "Epoch [447/500], Step [10/22], Loss: 0.0449\n",
      "Epoch [447/500], Step [20/22], Loss: 0.0120\n",
      "Epoch [448/500], Step [10/22], Loss: 0.0299\n",
      "Epoch [448/500], Step [20/22], Loss: 0.0079\n",
      "Epoch [449/500], Step [10/22], Loss: 0.0377\n",
      "Epoch [449/500], Step [20/22], Loss: 0.0135\n",
      "Epoch [450/500], Step [10/22], Loss: 0.0406\n",
      "Epoch [450/500], Step [20/22], Loss: 0.0062\n",
      "Epoch [451/500], Step [10/22], Loss: 0.0278\n",
      "Epoch [451/500], Step [20/22], Loss: 0.0141\n",
      "Epoch [452/500], Step [10/22], Loss: 0.0831\n",
      "Epoch [452/500], Step [20/22], Loss: 0.0137\n",
      "Epoch [453/500], Step [10/22], Loss: 0.0386\n",
      "Epoch [453/500], Step [20/22], Loss: 0.0116\n",
      "Epoch [454/500], Step [10/22], Loss: 0.0625\n",
      "Epoch [454/500], Step [20/22], Loss: 0.0139\n",
      "Epoch [455/500], Step [10/22], Loss: 0.2502\n",
      "Epoch [455/500], Step [20/22], Loss: 0.0042\n",
      "Epoch [456/500], Step [10/22], Loss: 0.0525\n",
      "Epoch [456/500], Step [20/22], Loss: 0.0173\n",
      "Epoch [457/500], Step [10/22], Loss: 0.1285\n",
      "Epoch [457/500], Step [20/22], Loss: 0.0069\n",
      "Epoch [458/500], Step [10/22], Loss: 0.0343\n",
      "Epoch [458/500], Step [20/22], Loss: 0.0086\n",
      "Epoch [459/500], Step [10/22], Loss: 0.0413\n",
      "Epoch [459/500], Step [20/22], Loss: 0.0052\n",
      "Epoch [460/500], Step [10/22], Loss: 0.0418\n",
      "Epoch [460/500], Step [20/22], Loss: 0.0054\n",
      "Epoch [461/500], Step [10/22], Loss: 0.0327\n",
      "Epoch [461/500], Step [20/22], Loss: 0.0020\n",
      "Epoch [462/500], Step [10/22], Loss: 0.0349\n",
      "Epoch [462/500], Step [20/22], Loss: 0.0020\n",
      "Epoch [463/500], Step [10/22], Loss: 0.0325\n",
      "Epoch [463/500], Step [20/22], Loss: 0.0013\n",
      "Epoch [464/500], Step [10/22], Loss: 0.0278\n",
      "Epoch [464/500], Step [20/22], Loss: 0.0022\n",
      "Epoch [465/500], Step [10/22], Loss: 0.0351\n",
      "Epoch [465/500], Step [20/22], Loss: 0.0015\n",
      "Epoch [466/500], Step [10/22], Loss: 0.0263\n",
      "Epoch [466/500], Step [20/22], Loss: 0.0095\n",
      "Epoch [467/500], Step [10/22], Loss: 0.0313\n",
      "Epoch [467/500], Step [20/22], Loss: 0.0048\n",
      "Epoch [468/500], Step [10/22], Loss: 0.0884\n",
      "Epoch [468/500], Step [20/22], Loss: 0.3385\n",
      "Epoch [469/500], Step [10/22], Loss: 0.0761\n",
      "Epoch [469/500], Step [20/22], Loss: 0.2623\n",
      "Epoch [470/500], Step [10/22], Loss: 0.1705\n",
      "Epoch [470/500], Step [20/22], Loss: 0.0819\n",
      "Epoch [471/500], Step [10/22], Loss: 0.1963\n",
      "Epoch [471/500], Step [20/22], Loss: 0.1667\n",
      "Epoch [472/500], Step [10/22], Loss: 0.2407\n",
      "Epoch [472/500], Step [20/22], Loss: 0.0783\n",
      "Epoch [473/500], Step [10/22], Loss: 0.1639\n",
      "Epoch [473/500], Step [20/22], Loss: 0.0594\n",
      "Epoch [474/500], Step [10/22], Loss: 0.0728\n",
      "Epoch [474/500], Step [20/22], Loss: 0.0541\n",
      "Epoch [475/500], Step [10/22], Loss: 0.0758\n",
      "Epoch [475/500], Step [20/22], Loss: 0.0408\n",
      "Epoch [476/500], Step [10/22], Loss: 0.0575\n",
      "Epoch [476/500], Step [20/22], Loss: 0.0645\n",
      "Epoch [477/500], Step [10/22], Loss: 0.0666\n",
      "Epoch [477/500], Step [20/22], Loss: 0.0269\n",
      "Epoch [478/500], Step [10/22], Loss: 0.1181\n",
      "Epoch [478/500], Step [20/22], Loss: 0.0528\n",
      "Epoch [479/500], Step [10/22], Loss: 0.0842\n",
      "Epoch [479/500], Step [20/22], Loss: 0.1461\n",
      "Epoch [480/500], Step [10/22], Loss: 0.1518\n",
      "Epoch [480/500], Step [20/22], Loss: 0.0250\n",
      "Epoch [481/500], Step [10/22], Loss: 0.2484\n",
      "Epoch [481/500], Step [20/22], Loss: 0.1469\n",
      "Epoch [482/500], Step [10/22], Loss: 0.0747\n",
      "Epoch [482/500], Step [20/22], Loss: 0.0867\n",
      "Epoch [483/500], Step [10/22], Loss: 0.1263\n",
      "Epoch [483/500], Step [20/22], Loss: 0.0535\n",
      "Epoch [484/500], Step [10/22], Loss: 0.0785\n",
      "Epoch [484/500], Step [20/22], Loss: 0.0777\n",
      "Epoch [485/500], Step [10/22], Loss: 0.0932\n",
      "Epoch [485/500], Step [20/22], Loss: 0.0335\n",
      "Epoch [486/500], Step [10/22], Loss: 0.0572\n",
      "Epoch [486/500], Step [20/22], Loss: 0.0230\n",
      "Epoch [487/500], Step [10/22], Loss: 0.0522\n",
      "Epoch [487/500], Step [20/22], Loss: 0.0173\n",
      "Epoch [488/500], Step [10/22], Loss: 0.0737\n",
      "Epoch [488/500], Step [20/22], Loss: 0.0127\n",
      "Epoch [489/500], Step [10/22], Loss: 0.0370\n",
      "Epoch [489/500], Step [20/22], Loss: 0.0110\n",
      "Epoch [490/500], Step [10/22], Loss: 0.0641\n",
      "Epoch [490/500], Step [20/22], Loss: 0.0116\n",
      "Epoch [491/500], Step [10/22], Loss: 0.0365\n",
      "Epoch [491/500], Step [20/22], Loss: 0.0082\n",
      "Epoch [492/500], Step [10/22], Loss: 0.0389\n",
      "Epoch [492/500], Step [20/22], Loss: 0.0102\n",
      "Epoch [493/500], Step [10/22], Loss: 0.0536\n",
      "Epoch [493/500], Step [20/22], Loss: 0.0247\n",
      "Epoch [494/500], Step [10/22], Loss: 0.0331\n",
      "Epoch [494/500], Step [20/22], Loss: 0.0514\n",
      "Epoch [495/500], Step [10/22], Loss: 0.0754\n",
      "Epoch [495/500], Step [20/22], Loss: 0.0057\n",
      "Epoch [496/500], Step [10/22], Loss: 0.0659\n",
      "Epoch [496/500], Step [20/22], Loss: 0.0042\n",
      "Epoch [497/500], Step [10/22], Loss: 0.0290\n",
      "Epoch [497/500], Step [20/22], Loss: 0.0075\n",
      "Epoch [498/500], Step [10/22], Loss: 0.0595\n",
      "Epoch [498/500], Step [20/22], Loss: 0.0057\n",
      "Epoch [499/500], Step [10/22], Loss: 0.0246\n",
      "Epoch [499/500], Step [20/22], Loss: 0.0047\n",
      "Epoch [500/500], Step [10/22], Loss: 0.0326\n",
      "Epoch [500/500], Step [20/22], Loss: 0.0051\n",
      "Epoch [1/500], Step [10/22], Loss: 0.6929\n",
      "Epoch [1/500], Step [20/22], Loss: 0.6839\n",
      "Epoch [2/500], Step [10/22], Loss: 0.6928\n",
      "Epoch [2/500], Step [20/22], Loss: 0.5722\n",
      "Epoch [3/500], Step [10/22], Loss: 0.6947\n",
      "Epoch [3/500], Step [20/22], Loss: 0.4411\n",
      "Epoch [4/500], Step [10/22], Loss: 0.6901\n",
      "Epoch [4/500], Step [20/22], Loss: 0.3447\n",
      "Epoch [5/500], Step [10/22], Loss: 0.6927\n",
      "Epoch [5/500], Step [20/22], Loss: 0.3091\n",
      "Epoch [6/500], Step [10/22], Loss: 0.6944\n",
      "Epoch [6/500], Step [20/22], Loss: 0.2682\n",
      "Epoch [7/500], Step [10/22], Loss: 0.6988\n",
      "Epoch [7/500], Step [20/22], Loss: 0.3278\n",
      "Epoch [8/500], Step [10/22], Loss: 0.6950\n",
      "Epoch [8/500], Step [20/22], Loss: 0.2608\n",
      "Epoch [9/500], Step [10/22], Loss: 0.7049\n",
      "Epoch [9/500], Step [20/22], Loss: 0.3854\n",
      "Epoch [10/500], Step [10/22], Loss: 0.6937\n",
      "Epoch [10/500], Step [20/22], Loss: 0.2795\n",
      "Epoch [11/500], Step [10/22], Loss: 0.7019\n",
      "Epoch [11/500], Step [20/22], Loss: 0.3230\n",
      "Epoch [12/500], Step [10/22], Loss: 0.7149\n",
      "Epoch [12/500], Step [20/22], Loss: 0.2868\n",
      "Epoch [13/500], Step [10/22], Loss: 0.7420\n",
      "Epoch [13/500], Step [20/22], Loss: 0.2761\n",
      "Epoch [14/500], Step [10/22], Loss: 0.7951\n",
      "Epoch [14/500], Step [20/22], Loss: 0.2593\n",
      "Epoch [15/500], Step [10/22], Loss: 0.8181\n",
      "Epoch [15/500], Step [20/22], Loss: 0.2573\n",
      "Epoch [16/500], Step [10/22], Loss: 0.7594\n",
      "Epoch [16/500], Step [20/22], Loss: 0.2517\n",
      "Epoch [17/500], Step [10/22], Loss: 0.8302\n",
      "Epoch [17/500], Step [20/22], Loss: 0.2426\n",
      "Epoch [18/500], Step [10/22], Loss: 0.8188\n",
      "Epoch [18/500], Step [20/22], Loss: 0.2482\n",
      "Epoch [19/500], Step [10/22], Loss: 0.8340\n",
      "Epoch [19/500], Step [20/22], Loss: 0.2751\n",
      "Epoch [20/500], Step [10/22], Loss: 0.6755\n",
      "Epoch [20/500], Step [20/22], Loss: 0.2391\n",
      "Epoch [21/500], Step [10/22], Loss: 0.8241\n",
      "Epoch [21/500], Step [20/22], Loss: 0.2373\n",
      "Epoch [22/500], Step [10/22], Loss: 0.6748\n",
      "Epoch [22/500], Step [20/22], Loss: 0.2341\n",
      "Epoch [23/500], Step [10/22], Loss: 0.7182\n",
      "Epoch [23/500], Step [20/22], Loss: 0.2337\n",
      "Epoch [24/500], Step [10/22], Loss: 0.7023\n",
      "Epoch [24/500], Step [20/22], Loss: 0.2339\n",
      "Epoch [25/500], Step [10/22], Loss: 0.7451\n",
      "Epoch [25/500], Step [20/22], Loss: 0.2292\n",
      "Epoch [26/500], Step [10/22], Loss: 0.8466\n",
      "Epoch [26/500], Step [20/22], Loss: 0.2363\n",
      "Epoch [27/500], Step [10/22], Loss: 0.8217\n",
      "Epoch [27/500], Step [20/22], Loss: 0.4531\n",
      "Epoch [28/500], Step [10/22], Loss: 0.6856\n",
      "Epoch [28/500], Step [20/22], Loss: 0.3060\n",
      "Epoch [29/500], Step [10/22], Loss: 0.6678\n",
      "Epoch [29/500], Step [20/22], Loss: 0.2437\n",
      "Epoch [30/500], Step [10/22], Loss: 0.6939\n",
      "Epoch [30/500], Step [20/22], Loss: 0.2343\n",
      "Epoch [31/500], Step [10/22], Loss: 0.8060\n",
      "Epoch [31/500], Step [20/22], Loss: 0.2315\n",
      "Epoch [32/500], Step [10/22], Loss: 0.8333\n",
      "Epoch [32/500], Step [20/22], Loss: 0.2289\n",
      "Epoch [33/500], Step [10/22], Loss: 0.6544\n",
      "Epoch [33/500], Step [20/22], Loss: 0.2358\n",
      "Epoch [34/500], Step [10/22], Loss: 0.6726\n",
      "Epoch [34/500], Step [20/22], Loss: 0.2362\n",
      "Epoch [35/500], Step [10/22], Loss: 0.8421\n",
      "Epoch [35/500], Step [20/22], Loss: 0.4286\n",
      "Epoch [36/500], Step [10/22], Loss: 0.6789\n",
      "Epoch [36/500], Step [20/22], Loss: 0.2918\n",
      "Epoch [37/500], Step [10/22], Loss: 0.6698\n",
      "Epoch [37/500], Step [20/22], Loss: 0.2385\n",
      "Epoch [38/500], Step [10/22], Loss: 0.6599\n",
      "Epoch [38/500], Step [20/22], Loss: 0.2311\n",
      "Epoch [39/500], Step [10/22], Loss: 0.6602\n",
      "Epoch [39/500], Step [20/22], Loss: 0.2284\n",
      "Epoch [40/500], Step [10/22], Loss: 0.6569\n",
      "Epoch [40/500], Step [20/22], Loss: 0.2218\n",
      "Epoch [41/500], Step [10/22], Loss: 0.6714\n",
      "Epoch [41/500], Step [20/22], Loss: 0.2333\n",
      "Epoch [42/500], Step [10/22], Loss: 0.7035\n",
      "Epoch [42/500], Step [20/22], Loss: 0.3217\n",
      "Epoch [43/500], Step [10/22], Loss: 0.6145\n",
      "Epoch [43/500], Step [20/22], Loss: 0.2402\n",
      "Epoch [44/500], Step [10/22], Loss: 0.9828\n",
      "Epoch [44/500], Step [20/22], Loss: 0.2271\n",
      "Epoch [45/500], Step [10/22], Loss: 0.6126\n",
      "Epoch [45/500], Step [20/22], Loss: 0.2214\n",
      "Epoch [46/500], Step [10/22], Loss: 0.6582\n",
      "Epoch [46/500], Step [20/22], Loss: 0.2263\n",
      "Epoch [47/500], Step [10/22], Loss: 0.7517\n",
      "Epoch [47/500], Step [20/22], Loss: 0.2199\n",
      "Epoch [48/500], Step [10/22], Loss: 0.5572\n",
      "Epoch [48/500], Step [20/22], Loss: 0.2223\n",
      "Epoch [49/500], Step [10/22], Loss: 0.6345\n",
      "Epoch [49/500], Step [20/22], Loss: 0.1423\n",
      "Epoch [50/500], Step [10/22], Loss: 0.4980\n",
      "Epoch [50/500], Step [20/22], Loss: 0.2375\n",
      "Epoch [51/500], Step [10/22], Loss: 0.7799\n",
      "Epoch [51/500], Step [20/22], Loss: 0.2094\n",
      "Epoch [52/500], Step [10/22], Loss: 0.5506\n",
      "Epoch [52/500], Step [20/22], Loss: 0.3913\n",
      "Epoch [53/500], Step [10/22], Loss: 0.6192\n",
      "Epoch [53/500], Step [20/22], Loss: 0.2352\n",
      "Epoch [54/500], Step [10/22], Loss: 0.8435\n",
      "Epoch [54/500], Step [20/22], Loss: 0.2486\n",
      "Epoch [55/500], Step [10/22], Loss: 0.6697\n",
      "Epoch [55/500], Step [20/22], Loss: 0.2260\n",
      "Epoch [56/500], Step [10/22], Loss: 0.7704\n",
      "Epoch [56/500], Step [20/22], Loss: 0.2070\n",
      "Epoch [57/500], Step [10/22], Loss: 0.6303\n",
      "Epoch [57/500], Step [20/22], Loss: 0.1837\n",
      "Epoch [58/500], Step [10/22], Loss: 0.6959\n",
      "Epoch [58/500], Step [20/22], Loss: 0.1561\n",
      "Epoch [59/500], Step [10/22], Loss: 0.4768\n",
      "Epoch [59/500], Step [20/22], Loss: 0.1281\n",
      "Epoch [60/500], Step [10/22], Loss: 0.7338\n",
      "Epoch [60/500], Step [20/22], Loss: 0.1181\n",
      "Epoch [61/500], Step [10/22], Loss: 0.4852\n",
      "Epoch [61/500], Step [20/22], Loss: 0.1281\n",
      "Epoch [62/500], Step [10/22], Loss: 0.5199\n",
      "Epoch [62/500], Step [20/22], Loss: 0.1445\n",
      "Epoch [63/500], Step [10/22], Loss: 0.4543\n",
      "Epoch [63/500], Step [20/22], Loss: 0.0885\n",
      "Epoch [64/500], Step [10/22], Loss: 0.4575\n",
      "Epoch [64/500], Step [20/22], Loss: 0.1610\n",
      "Epoch [65/500], Step [10/22], Loss: 0.4748\n",
      "Epoch [65/500], Step [20/22], Loss: 0.1550\n",
      "Epoch [66/500], Step [10/22], Loss: 0.7327\n",
      "Epoch [66/500], Step [20/22], Loss: 0.2344\n",
      "Epoch [67/500], Step [10/22], Loss: 0.5038\n",
      "Epoch [67/500], Step [20/22], Loss: 0.2018\n",
      "Epoch [68/500], Step [10/22], Loss: 0.8483\n",
      "Epoch [68/500], Step [20/22], Loss: 0.1500\n",
      "Epoch [69/500], Step [10/22], Loss: 0.3876\n",
      "Epoch [69/500], Step [20/22], Loss: 0.1627\n",
      "Epoch [70/500], Step [10/22], Loss: 0.5719\n",
      "Epoch [70/500], Step [20/22], Loss: 0.1890\n",
      "Epoch [71/500], Step [10/22], Loss: 0.5133\n",
      "Epoch [71/500], Step [20/22], Loss: 0.2210\n",
      "Epoch [72/500], Step [10/22], Loss: 0.4402\n",
      "Epoch [72/500], Step [20/22], Loss: 0.1928\n",
      "Epoch [73/500], Step [10/22], Loss: 0.5058\n",
      "Epoch [73/500], Step [20/22], Loss: 0.2093\n",
      "Epoch [74/500], Step [10/22], Loss: 0.4077\n",
      "Epoch [74/500], Step [20/22], Loss: 0.1816\n",
      "Epoch [75/500], Step [10/22], Loss: 0.4647\n",
      "Epoch [75/500], Step [20/22], Loss: 0.1891\n",
      "Epoch [76/500], Step [10/22], Loss: 0.4491\n",
      "Epoch [76/500], Step [20/22], Loss: 0.1892\n",
      "Epoch [77/500], Step [10/22], Loss: 0.3847\n",
      "Epoch [77/500], Step [20/22], Loss: 0.2338\n",
      "Epoch [78/500], Step [10/22], Loss: 0.4680\n",
      "Epoch [78/500], Step [20/22], Loss: 0.1957\n",
      "Epoch [79/500], Step [10/22], Loss: 0.4287\n",
      "Epoch [79/500], Step [20/22], Loss: 0.1981\n",
      "Epoch [80/500], Step [10/22], Loss: 0.4516\n",
      "Epoch [80/500], Step [20/22], Loss: 0.1682\n",
      "Epoch [81/500], Step [10/22], Loss: 0.4147\n",
      "Epoch [81/500], Step [20/22], Loss: 0.2286\n",
      "Epoch [82/500], Step [10/22], Loss: 0.5489\n",
      "Epoch [82/500], Step [20/22], Loss: 0.2142\n",
      "Epoch [83/500], Step [10/22], Loss: 0.3875\n",
      "Epoch [83/500], Step [20/22], Loss: 0.2014\n",
      "Epoch [84/500], Step [10/22], Loss: 0.4850\n",
      "Epoch [84/500], Step [20/22], Loss: 0.1792\n",
      "Epoch [85/500], Step [10/22], Loss: 0.4263\n",
      "Epoch [85/500], Step [20/22], Loss: 0.1757\n",
      "Epoch [86/500], Step [10/22], Loss: 0.5232\n",
      "Epoch [86/500], Step [20/22], Loss: 0.1323\n",
      "Epoch [87/500], Step [10/22], Loss: 0.4705\n",
      "Epoch [87/500], Step [20/22], Loss: 0.1051\n",
      "Epoch [88/500], Step [10/22], Loss: 0.5083\n",
      "Epoch [88/500], Step [20/22], Loss: 0.1335\n",
      "Epoch [89/500], Step [10/22], Loss: 0.4093\n",
      "Epoch [89/500], Step [20/22], Loss: 0.1805\n",
      "Epoch [90/500], Step [10/22], Loss: 0.5807\n",
      "Epoch [90/500], Step [20/22], Loss: 0.1695\n",
      "Epoch [91/500], Step [10/22], Loss: 0.4620\n",
      "Epoch [91/500], Step [20/22], Loss: 0.1772\n",
      "Epoch [92/500], Step [10/22], Loss: 0.4691\n",
      "Epoch [92/500], Step [20/22], Loss: 0.1734\n",
      "Epoch [93/500], Step [10/22], Loss: 0.4913\n",
      "Epoch [93/500], Step [20/22], Loss: 0.1137\n",
      "Epoch [94/500], Step [10/22], Loss: 0.5151\n",
      "Epoch [94/500], Step [20/22], Loss: 0.1109\n",
      "Epoch [95/500], Step [10/22], Loss: 0.4530\n",
      "Epoch [95/500], Step [20/22], Loss: 0.1302\n",
      "Epoch [96/500], Step [10/22], Loss: 0.4756\n",
      "Epoch [96/500], Step [20/22], Loss: 0.0717\n",
      "Epoch [97/500], Step [10/22], Loss: 0.4254\n",
      "Epoch [97/500], Step [20/22], Loss: 0.0820\n",
      "Epoch [98/500], Step [10/22], Loss: 0.6130\n",
      "Epoch [98/500], Step [20/22], Loss: 0.0938\n",
      "Epoch [99/500], Step [10/22], Loss: 0.4897\n",
      "Epoch [99/500], Step [20/22], Loss: 0.0631\n",
      "Epoch [100/500], Step [10/22], Loss: 0.5213\n",
      "Epoch [100/500], Step [20/22], Loss: 0.0785\n",
      "Epoch [101/500], Step [10/22], Loss: 0.3592\n",
      "Epoch [101/500], Step [20/22], Loss: 0.0922\n",
      "Epoch [102/500], Step [10/22], Loss: 0.4453\n",
      "Epoch [102/500], Step [20/22], Loss: 0.0584\n",
      "Epoch [103/500], Step [10/22], Loss: 0.4972\n",
      "Epoch [103/500], Step [20/22], Loss: 0.0972\n",
      "Epoch [104/500], Step [10/22], Loss: 0.2968\n",
      "Epoch [104/500], Step [20/22], Loss: 0.1078\n",
      "Epoch [105/500], Step [10/22], Loss: 0.4621\n",
      "Epoch [105/500], Step [20/22], Loss: 0.1118\n",
      "Epoch [106/500], Step [10/22], Loss: 0.4108\n",
      "Epoch [106/500], Step [20/22], Loss: 0.1060\n",
      "Epoch [107/500], Step [10/22], Loss: 0.5940\n",
      "Epoch [107/500], Step [20/22], Loss: 0.1177\n",
      "Epoch [108/500], Step [10/22], Loss: 0.4220\n",
      "Epoch [108/500], Step [20/22], Loss: 0.1013\n",
      "Epoch [109/500], Step [10/22], Loss: 0.5551\n",
      "Epoch [109/500], Step [20/22], Loss: 0.1027\n",
      "Epoch [110/500], Step [10/22], Loss: 0.4218\n",
      "Epoch [110/500], Step [20/22], Loss: 0.0699\n",
      "Epoch [111/500], Step [10/22], Loss: 0.6198\n",
      "Epoch [111/500], Step [20/22], Loss: 0.0777\n",
      "Epoch [112/500], Step [10/22], Loss: 0.3415\n",
      "Epoch [112/500], Step [20/22], Loss: 0.1061\n",
      "Epoch [113/500], Step [10/22], Loss: 0.4578\n",
      "Epoch [113/500], Step [20/22], Loss: 0.1297\n",
      "Epoch [114/500], Step [10/22], Loss: 0.3909\n",
      "Epoch [114/500], Step [20/22], Loss: 0.1813\n",
      "Epoch [115/500], Step [10/22], Loss: 0.4717\n",
      "Epoch [115/500], Step [20/22], Loss: 0.1920\n",
      "Epoch [116/500], Step [10/22], Loss: 0.5591\n",
      "Epoch [116/500], Step [20/22], Loss: 0.1149\n",
      "Epoch [117/500], Step [10/22], Loss: 0.3968\n",
      "Epoch [117/500], Step [20/22], Loss: 0.1066\n",
      "Epoch [118/500], Step [10/22], Loss: 0.4414\n",
      "Epoch [118/500], Step [20/22], Loss: 0.0956\n",
      "Epoch [119/500], Step [10/22], Loss: 0.4795\n",
      "Epoch [119/500], Step [20/22], Loss: 0.1012\n",
      "Epoch [120/500], Step [10/22], Loss: 0.4588\n",
      "Epoch [120/500], Step [20/22], Loss: 0.1012\n",
      "Epoch [121/500], Step [10/22], Loss: 0.4079\n",
      "Epoch [121/500], Step [20/22], Loss: 0.0756\n",
      "Epoch [122/500], Step [10/22], Loss: 0.3568\n",
      "Epoch [122/500], Step [20/22], Loss: 0.1077\n",
      "Epoch [123/500], Step [10/22], Loss: 0.5047\n",
      "Epoch [123/500], Step [20/22], Loss: 0.1308\n",
      "Epoch [124/500], Step [10/22], Loss: 0.4491\n",
      "Epoch [124/500], Step [20/22], Loss: 0.1135\n",
      "Epoch [125/500], Step [10/22], Loss: 0.4746\n",
      "Epoch [125/500], Step [20/22], Loss: 0.1112\n",
      "Epoch [126/500], Step [10/22], Loss: 0.3675\n",
      "Epoch [126/500], Step [20/22], Loss: 0.0835\n",
      "Epoch [127/500], Step [10/22], Loss: 0.4051\n",
      "Epoch [127/500], Step [20/22], Loss: 0.0575\n",
      "Epoch [128/500], Step [10/22], Loss: 0.4963\n",
      "Epoch [128/500], Step [20/22], Loss: 0.0523\n",
      "Epoch [129/500], Step [10/22], Loss: 0.4139\n",
      "Epoch [129/500], Step [20/22], Loss: 0.1186\n",
      "Epoch [130/500], Step [10/22], Loss: 0.3145\n",
      "Epoch [130/500], Step [20/22], Loss: 0.0901\n",
      "Epoch [131/500], Step [10/22], Loss: 0.4206\n",
      "Epoch [131/500], Step [20/22], Loss: 0.1556\n",
      "Epoch [132/500], Step [10/22], Loss: 0.3473\n",
      "Epoch [132/500], Step [20/22], Loss: 0.0570\n",
      "Epoch [133/500], Step [10/22], Loss: 0.4530\n",
      "Epoch [133/500], Step [20/22], Loss: 0.0796\n",
      "Epoch [134/500], Step [10/22], Loss: 0.5044\n",
      "Epoch [134/500], Step [20/22], Loss: 0.1133\n",
      "Epoch [135/500], Step [10/22], Loss: 0.3268\n",
      "Epoch [135/500], Step [20/22], Loss: 0.1341\n",
      "Epoch [136/500], Step [10/22], Loss: 0.4516\n",
      "Epoch [136/500], Step [20/22], Loss: 0.1332\n",
      "Epoch [137/500], Step [10/22], Loss: 0.4399\n",
      "Epoch [137/500], Step [20/22], Loss: 0.1830\n",
      "Epoch [138/500], Step [10/22], Loss: 0.4248\n",
      "Epoch [138/500], Step [20/22], Loss: 0.1569\n",
      "Epoch [139/500], Step [10/22], Loss: 0.4788\n",
      "Epoch [139/500], Step [20/22], Loss: 0.2044\n",
      "Epoch [140/500], Step [10/22], Loss: 0.5765\n",
      "Epoch [140/500], Step [20/22], Loss: 0.1366\n",
      "Epoch [141/500], Step [10/22], Loss: 0.3998\n",
      "Epoch [141/500], Step [20/22], Loss: 0.0902\n",
      "Epoch [142/500], Step [10/22], Loss: 0.3750\n",
      "Epoch [142/500], Step [20/22], Loss: 0.1070\n",
      "Epoch [143/500], Step [10/22], Loss: 0.5551\n",
      "Epoch [143/500], Step [20/22], Loss: 0.0915\n",
      "Epoch [144/500], Step [10/22], Loss: 0.3486\n",
      "Epoch [144/500], Step [20/22], Loss: 0.0807\n",
      "Epoch [145/500], Step [10/22], Loss: 0.3990\n",
      "Epoch [145/500], Step [20/22], Loss: 0.0934\n",
      "Epoch [146/500], Step [10/22], Loss: 0.3341\n",
      "Epoch [146/500], Step [20/22], Loss: 0.1729\n",
      "Epoch [147/500], Step [10/22], Loss: 0.4837\n",
      "Epoch [147/500], Step [20/22], Loss: 0.1441\n",
      "Epoch [148/500], Step [10/22], Loss: 0.3192\n",
      "Epoch [148/500], Step [20/22], Loss: 0.1366\n",
      "Epoch [149/500], Step [10/22], Loss: 0.3646\n",
      "Epoch [149/500], Step [20/22], Loss: 0.1243\n",
      "Epoch [150/500], Step [10/22], Loss: 0.3577\n",
      "Epoch [150/500], Step [20/22], Loss: 0.1507\n",
      "Epoch [151/500], Step [10/22], Loss: 0.4703\n",
      "Epoch [151/500], Step [20/22], Loss: 0.1116\n",
      "Epoch [152/500], Step [10/22], Loss: 0.3116\n",
      "Epoch [152/500], Step [20/22], Loss: 0.1394\n",
      "Epoch [153/500], Step [10/22], Loss: 0.3671\n",
      "Epoch [153/500], Step [20/22], Loss: 0.1619\n",
      "Epoch [154/500], Step [10/22], Loss: 0.3566\n",
      "Epoch [154/500], Step [20/22], Loss: 0.1412\n",
      "Epoch [155/500], Step [10/22], Loss: 0.5237\n",
      "Epoch [155/500], Step [20/22], Loss: 0.1074\n",
      "Epoch [156/500], Step [10/22], Loss: 0.3229\n",
      "Epoch [156/500], Step [20/22], Loss: 0.0868\n",
      "Epoch [157/500], Step [10/22], Loss: 0.3707\n",
      "Epoch [157/500], Step [20/22], Loss: 0.0866\n",
      "Epoch [158/500], Step [10/22], Loss: 0.4051\n",
      "Epoch [158/500], Step [20/22], Loss: 0.1082\n",
      "Epoch [159/500], Step [10/22], Loss: 0.3894\n",
      "Epoch [159/500], Step [20/22], Loss: 0.1189\n",
      "Epoch [160/500], Step [10/22], Loss: 0.3331\n",
      "Epoch [160/500], Step [20/22], Loss: 0.0676\n",
      "Epoch [161/500], Step [10/22], Loss: 0.3883\n",
      "Epoch [161/500], Step [20/22], Loss: 0.1061\n",
      "Epoch [162/500], Step [10/22], Loss: 0.3175\n",
      "Epoch [162/500], Step [20/22], Loss: 0.0771\n",
      "Epoch [163/500], Step [10/22], Loss: 0.3862\n",
      "Epoch [163/500], Step [20/22], Loss: 0.0739\n",
      "Epoch [164/500], Step [10/22], Loss: 0.3737\n",
      "Epoch [164/500], Step [20/22], Loss: 0.0376\n",
      "Epoch [165/500], Step [10/22], Loss: 0.4407\n",
      "Epoch [165/500], Step [20/22], Loss: 0.0972\n",
      "Epoch [166/500], Step [10/22], Loss: 0.2821\n",
      "Epoch [166/500], Step [20/22], Loss: 0.0454\n",
      "Epoch [167/500], Step [10/22], Loss: 0.3529\n",
      "Epoch [167/500], Step [20/22], Loss: 0.0681\n",
      "Epoch [168/500], Step [10/22], Loss: 0.3375\n",
      "Epoch [168/500], Step [20/22], Loss: 0.0872\n",
      "Epoch [169/500], Step [10/22], Loss: 0.5473\n",
      "Epoch [169/500], Step [20/22], Loss: 0.1253\n",
      "Epoch [170/500], Step [10/22], Loss: 0.4085\n",
      "Epoch [170/500], Step [20/22], Loss: 0.0998\n",
      "Epoch [171/500], Step [10/22], Loss: 0.3963\n",
      "Epoch [171/500], Step [20/22], Loss: 0.1558\n",
      "Epoch [172/500], Step [10/22], Loss: 0.4070\n",
      "Epoch [172/500], Step [20/22], Loss: 0.1236\n",
      "Epoch [173/500], Step [10/22], Loss: 0.3741\n",
      "Epoch [173/500], Step [20/22], Loss: 0.1064\n",
      "Epoch [174/500], Step [10/22], Loss: 0.4233\n",
      "Epoch [174/500], Step [20/22], Loss: 0.0801\n",
      "Epoch [175/500], Step [10/22], Loss: 0.3942\n",
      "Epoch [175/500], Step [20/22], Loss: 0.1425\n",
      "Epoch [176/500], Step [10/22], Loss: 0.3737\n",
      "Epoch [176/500], Step [20/22], Loss: 0.0578\n",
      "Epoch [177/500], Step [10/22], Loss: 0.3227\n",
      "Epoch [177/500], Step [20/22], Loss: 0.0284\n",
      "Epoch [178/500], Step [10/22], Loss: 0.3403\n",
      "Epoch [178/500], Step [20/22], Loss: 0.1637\n",
      "Epoch [179/500], Step [10/22], Loss: 0.3697\n",
      "Epoch [179/500], Step [20/22], Loss: 0.1194\n",
      "Epoch [180/500], Step [10/22], Loss: 0.3798\n",
      "Epoch [180/500], Step [20/22], Loss: 0.0767\n",
      "Epoch [181/500], Step [10/22], Loss: 0.3913\n",
      "Epoch [181/500], Step [20/22], Loss: 0.0445\n",
      "Epoch [182/500], Step [10/22], Loss: 0.3637\n",
      "Epoch [182/500], Step [20/22], Loss: 0.0469\n",
      "Epoch [183/500], Step [10/22], Loss: 0.3191\n",
      "Epoch [183/500], Step [20/22], Loss: 0.0301\n",
      "Epoch [184/500], Step [10/22], Loss: 0.3599\n",
      "Epoch [184/500], Step [20/22], Loss: 0.1041\n",
      "Epoch [185/500], Step [10/22], Loss: 0.4174\n",
      "Epoch [185/500], Step [20/22], Loss: 0.0762\n",
      "Epoch [186/500], Step [10/22], Loss: 0.4231\n",
      "Epoch [186/500], Step [20/22], Loss: 0.0528\n",
      "Epoch [187/500], Step [10/22], Loss: 0.3853\n",
      "Epoch [187/500], Step [20/22], Loss: 0.1016\n",
      "Epoch [188/500], Step [10/22], Loss: 0.3396\n",
      "Epoch [188/500], Step [20/22], Loss: 0.1222\n",
      "Epoch [189/500], Step [10/22], Loss: 0.3944\n",
      "Epoch [189/500], Step [20/22], Loss: 0.0644\n",
      "Epoch [190/500], Step [10/22], Loss: 0.3642\n",
      "Epoch [190/500], Step [20/22], Loss: 0.1285\n",
      "Epoch [191/500], Step [10/22], Loss: 0.4034\n",
      "Epoch [191/500], Step [20/22], Loss: 0.2498\n",
      "Epoch [192/500], Step [10/22], Loss: 0.4662\n",
      "Epoch [192/500], Step [20/22], Loss: 0.4208\n",
      "Epoch [193/500], Step [10/22], Loss: 0.4930\n",
      "Epoch [193/500], Step [20/22], Loss: 0.2034\n",
      "Epoch [194/500], Step [10/22], Loss: 0.4621\n",
      "Epoch [194/500], Step [20/22], Loss: 0.1848\n",
      "Epoch [195/500], Step [10/22], Loss: 0.4426\n",
      "Epoch [195/500], Step [20/22], Loss: 0.1613\n",
      "Epoch [196/500], Step [10/22], Loss: 0.4447\n",
      "Epoch [196/500], Step [20/22], Loss: 0.1522\n",
      "Epoch [197/500], Step [10/22], Loss: 0.4248\n",
      "Epoch [197/500], Step [20/22], Loss: 0.1410\n",
      "Epoch [198/500], Step [10/22], Loss: 0.4211\n",
      "Epoch [198/500], Step [20/22], Loss: 0.1391\n",
      "Epoch [199/500], Step [10/22], Loss: 0.4162\n",
      "Epoch [199/500], Step [20/22], Loss: 0.1344\n",
      "Epoch [200/500], Step [10/22], Loss: 0.3835\n",
      "Epoch [200/500], Step [20/22], Loss: 0.1296\n",
      "Epoch [201/500], Step [10/22], Loss: 0.3699\n",
      "Epoch [201/500], Step [20/22], Loss: 0.1274\n",
      "Epoch [202/500], Step [10/22], Loss: 0.3517\n",
      "Epoch [202/500], Step [20/22], Loss: 0.1218\n",
      "Epoch [203/500], Step [10/22], Loss: 0.2920\n",
      "Epoch [203/500], Step [20/22], Loss: 0.1017\n",
      "Epoch [204/500], Step [10/22], Loss: 0.4080\n",
      "Epoch [204/500], Step [20/22], Loss: 0.1184\n",
      "Epoch [205/500], Step [10/22], Loss: 0.4660\n",
      "Epoch [205/500], Step [20/22], Loss: 0.1134\n",
      "Epoch [206/500], Step [10/22], Loss: 0.3597\n",
      "Epoch [206/500], Step [20/22], Loss: 0.0774\n",
      "Epoch [207/500], Step [10/22], Loss: 0.3207\n",
      "Epoch [207/500], Step [20/22], Loss: 0.0826\n",
      "Epoch [208/500], Step [10/22], Loss: 0.3272\n",
      "Epoch [208/500], Step [20/22], Loss: 0.0984\n",
      "Epoch [209/500], Step [10/22], Loss: 0.4843\n",
      "Epoch [209/500], Step [20/22], Loss: 0.1350\n",
      "Epoch [210/500], Step [10/22], Loss: 0.4042\n",
      "Epoch [210/500], Step [20/22], Loss: 0.1518\n",
      "Epoch [211/500], Step [10/22], Loss: 0.3409\n",
      "Epoch [211/500], Step [20/22], Loss: 0.1538\n",
      "Epoch [212/500], Step [10/22], Loss: 0.3421\n",
      "Epoch [212/500], Step [20/22], Loss: 0.1094\n",
      "Epoch [213/500], Step [10/22], Loss: 0.3994\n",
      "Epoch [213/500], Step [20/22], Loss: 0.1176\n",
      "Epoch [214/500], Step [10/22], Loss: 0.3495\n",
      "Epoch [214/500], Step [20/22], Loss: 0.0740\n",
      "Epoch [215/500], Step [10/22], Loss: 0.3527\n",
      "Epoch [215/500], Step [20/22], Loss: 0.0459\n",
      "Epoch [216/500], Step [10/22], Loss: 0.3490\n",
      "Epoch [216/500], Step [20/22], Loss: 0.0532\n",
      "Epoch [217/500], Step [10/22], Loss: 0.3251\n",
      "Epoch [217/500], Step [20/22], Loss: 0.0575\n",
      "Epoch [218/500], Step [10/22], Loss: 0.3447\n",
      "Epoch [218/500], Step [20/22], Loss: 0.0552\n",
      "Epoch [219/500], Step [10/22], Loss: 0.3512\n",
      "Epoch [219/500], Step [20/22], Loss: 0.1097\n",
      "Epoch [220/500], Step [10/22], Loss: 0.2691\n",
      "Epoch [220/500], Step [20/22], Loss: 0.0855\n",
      "Epoch [221/500], Step [10/22], Loss: 0.3350\n",
      "Epoch [221/500], Step [20/22], Loss: 0.0515\n",
      "Epoch [222/500], Step [10/22], Loss: 0.2843\n",
      "Epoch [222/500], Step [20/22], Loss: 0.0886\n",
      "Epoch [223/500], Step [10/22], Loss: 0.3440\n",
      "Epoch [223/500], Step [20/22], Loss: 0.0977\n",
      "Epoch [224/500], Step [10/22], Loss: 0.2723\n",
      "Epoch [224/500], Step [20/22], Loss: 0.1852\n",
      "Epoch [225/500], Step [10/22], Loss: 0.3209\n",
      "Epoch [225/500], Step [20/22], Loss: 0.0790\n",
      "Epoch [226/500], Step [10/22], Loss: 0.3353\n",
      "Epoch [226/500], Step [20/22], Loss: 0.0568\n",
      "Epoch [227/500], Step [10/22], Loss: 0.2380\n",
      "Epoch [227/500], Step [20/22], Loss: 0.0587\n",
      "Epoch [228/500], Step [10/22], Loss: 0.2875\n",
      "Epoch [228/500], Step [20/22], Loss: 0.0270\n",
      "Epoch [229/500], Step [10/22], Loss: 0.2486\n",
      "Epoch [229/500], Step [20/22], Loss: 0.0348\n",
      "Epoch [230/500], Step [10/22], Loss: 0.3092\n",
      "Epoch [230/500], Step [20/22], Loss: 0.1124\n",
      "Epoch [231/500], Step [10/22], Loss: 0.3918\n",
      "Epoch [231/500], Step [20/22], Loss: 0.0582\n",
      "Epoch [232/500], Step [10/22], Loss: 0.3101\n",
      "Epoch [232/500], Step [20/22], Loss: 0.0798\n",
      "Epoch [233/500], Step [10/22], Loss: 0.3114\n",
      "Epoch [233/500], Step [20/22], Loss: 0.0425\n",
      "Epoch [234/500], Step [10/22], Loss: 0.2798\n",
      "Epoch [234/500], Step [20/22], Loss: 0.0193\n",
      "Epoch [235/500], Step [10/22], Loss: 0.2593\n",
      "Epoch [235/500], Step [20/22], Loss: 0.0221\n",
      "Epoch [236/500], Step [10/22], Loss: 0.3223\n",
      "Epoch [236/500], Step [20/22], Loss: 0.0446\n",
      "Epoch [237/500], Step [10/22], Loss: 0.2600\n",
      "Epoch [237/500], Step [20/22], Loss: 0.0694\n",
      "Epoch [238/500], Step [10/22], Loss: 0.2341\n",
      "Epoch [238/500], Step [20/22], Loss: 0.1337\n",
      "Epoch [239/500], Step [10/22], Loss: 0.2621\n",
      "Epoch [239/500], Step [20/22], Loss: 0.0726\n",
      "Epoch [240/500], Step [10/22], Loss: 0.2887\n",
      "Epoch [240/500], Step [20/22], Loss: 0.0836\n",
      "Epoch [241/500], Step [10/22], Loss: 0.4761\n",
      "Epoch [241/500], Step [20/22], Loss: 0.0777\n",
      "Epoch [242/500], Step [10/22], Loss: 0.4465\n",
      "Epoch [242/500], Step [20/22], Loss: 0.1269\n",
      "Epoch [243/500], Step [10/22], Loss: 0.3831\n",
      "Epoch [243/500], Step [20/22], Loss: 0.0906\n",
      "Epoch [244/500], Step [10/22], Loss: 0.3593\n",
      "Epoch [244/500], Step [20/22], Loss: 0.0779\n",
      "Epoch [245/500], Step [10/22], Loss: 0.3041\n",
      "Epoch [245/500], Step [20/22], Loss: 0.0289\n",
      "Epoch [246/500], Step [10/22], Loss: 0.3258\n",
      "Epoch [246/500], Step [20/22], Loss: 0.0234\n",
      "Epoch [247/500], Step [10/22], Loss: 0.3175\n",
      "Epoch [247/500], Step [20/22], Loss: 0.0259\n",
      "Epoch [248/500], Step [10/22], Loss: 0.2696\n",
      "Epoch [248/500], Step [20/22], Loss: 0.0226\n",
      "Epoch [249/500], Step [10/22], Loss: 0.3614\n",
      "Epoch [249/500], Step [20/22], Loss: 0.0690\n",
      "Epoch [250/500], Step [10/22], Loss: 0.2737\n",
      "Epoch [250/500], Step [20/22], Loss: 0.0764\n",
      "Epoch [251/500], Step [10/22], Loss: 0.3019\n",
      "Epoch [251/500], Step [20/22], Loss: 0.0264\n",
      "Epoch [252/500], Step [10/22], Loss: 0.2775\n",
      "Epoch [252/500], Step [20/22], Loss: 0.0102\n",
      "Epoch [253/500], Step [10/22], Loss: 0.2130\n",
      "Epoch [253/500], Step [20/22], Loss: 0.0229\n",
      "Epoch [254/500], Step [10/22], Loss: 0.2729\n",
      "Epoch [254/500], Step [20/22], Loss: 0.0175\n",
      "Epoch [255/500], Step [10/22], Loss: 0.2095\n",
      "Epoch [255/500], Step [20/22], Loss: 0.0123\n",
      "Epoch [256/500], Step [10/22], Loss: 0.2440\n",
      "Epoch [256/500], Step [20/22], Loss: 0.1383\n",
      "Epoch [257/500], Step [10/22], Loss: 0.2170\n",
      "Epoch [257/500], Step [20/22], Loss: 0.6478\n",
      "Epoch [258/500], Step [10/22], Loss: 0.3339\n",
      "Epoch [258/500], Step [20/22], Loss: 0.2024\n",
      "Epoch [259/500], Step [10/22], Loss: 0.3126\n",
      "Epoch [259/500], Step [20/22], Loss: 0.1088\n",
      "Epoch [260/500], Step [10/22], Loss: 0.3316\n",
      "Epoch [260/500], Step [20/22], Loss: 0.1639\n",
      "Epoch [261/500], Step [10/22], Loss: 0.4568\n",
      "Epoch [261/500], Step [20/22], Loss: 0.1717\n",
      "Epoch [262/500], Step [10/22], Loss: 0.3782\n",
      "Epoch [262/500], Step [20/22], Loss: 0.1376\n",
      "Epoch [263/500], Step [10/22], Loss: 0.3474\n",
      "Epoch [263/500], Step [20/22], Loss: 0.1178\n",
      "Epoch [264/500], Step [10/22], Loss: 0.3008\n",
      "Epoch [264/500], Step [20/22], Loss: 0.0956\n",
      "Epoch [265/500], Step [10/22], Loss: 0.2664\n",
      "Epoch [265/500], Step [20/22], Loss: 0.0751\n",
      "Epoch [266/500], Step [10/22], Loss: 0.2550\n",
      "Epoch [266/500], Step [20/22], Loss: 0.0624\n",
      "Epoch [267/500], Step [10/22], Loss: 0.2322\n",
      "Epoch [267/500], Step [20/22], Loss: 0.0710\n",
      "Epoch [268/500], Step [10/22], Loss: 0.2333\n",
      "Epoch [268/500], Step [20/22], Loss: 0.0603\n",
      "Epoch [269/500], Step [10/22], Loss: 0.2390\n",
      "Epoch [269/500], Step [20/22], Loss: 0.0399\n",
      "Epoch [270/500], Step [10/22], Loss: 0.2092\n",
      "Epoch [270/500], Step [20/22], Loss: 0.0216\n",
      "Epoch [271/500], Step [10/22], Loss: 0.2190\n",
      "Epoch [271/500], Step [20/22], Loss: 0.0212\n",
      "Epoch [272/500], Step [10/22], Loss: 0.2139\n",
      "Epoch [272/500], Step [20/22], Loss: 0.0219\n",
      "Epoch [273/500], Step [10/22], Loss: 0.1840\n",
      "Epoch [273/500], Step [20/22], Loss: 0.0189\n",
      "Epoch [274/500], Step [10/22], Loss: 0.2957\n",
      "Epoch [274/500], Step [20/22], Loss: 0.0180\n",
      "Epoch [275/500], Step [10/22], Loss: 0.2250\n",
      "Epoch [275/500], Step [20/22], Loss: 0.0596\n",
      "Epoch [276/500], Step [10/22], Loss: 0.3159\n",
      "Epoch [276/500], Step [20/22], Loss: 0.0421\n",
      "Epoch [277/500], Step [10/22], Loss: 0.3914\n",
      "Epoch [277/500], Step [20/22], Loss: 0.1359\n",
      "Epoch [278/500], Step [10/22], Loss: 0.4208\n",
      "Epoch [278/500], Step [20/22], Loss: 0.2876\n",
      "Epoch [279/500], Step [10/22], Loss: 0.3538\n",
      "Epoch [279/500], Step [20/22], Loss: 0.1538\n",
      "Epoch [280/500], Step [10/22], Loss: 0.3644\n",
      "Epoch [280/500], Step [20/22], Loss: 0.2119\n",
      "Epoch [281/500], Step [10/22], Loss: 0.2732\n",
      "Epoch [281/500], Step [20/22], Loss: 0.0580\n",
      "Epoch [282/500], Step [10/22], Loss: 0.3727\n",
      "Epoch [282/500], Step [20/22], Loss: 0.0895\n",
      "Epoch [283/500], Step [10/22], Loss: 0.2667\n",
      "Epoch [283/500], Step [20/22], Loss: 0.0561\n",
      "Epoch [284/500], Step [10/22], Loss: 0.3269\n",
      "Epoch [284/500], Step [20/22], Loss: 0.0437\n",
      "Epoch [285/500], Step [10/22], Loss: 0.2018\n",
      "Epoch [285/500], Step [20/22], Loss: 0.0377\n",
      "Epoch [286/500], Step [10/22], Loss: 0.2173\n",
      "Epoch [286/500], Step [20/22], Loss: 0.0595\n",
      "Epoch [287/500], Step [10/22], Loss: 0.2749\n",
      "Epoch [287/500], Step [20/22], Loss: 0.1137\n",
      "Epoch [288/500], Step [10/22], Loss: 0.3032\n",
      "Epoch [288/500], Step [20/22], Loss: 0.0539\n",
      "Epoch [289/500], Step [10/22], Loss: 0.3820\n",
      "Epoch [289/500], Step [20/22], Loss: 0.0497\n",
      "Epoch [290/500], Step [10/22], Loss: 0.2419\n",
      "Epoch [290/500], Step [20/22], Loss: 0.0303\n",
      "Epoch [291/500], Step [10/22], Loss: 0.2305\n",
      "Epoch [291/500], Step [20/22], Loss: 0.0452\n",
      "Epoch [292/500], Step [10/22], Loss: 0.1950\n",
      "Epoch [292/500], Step [20/22], Loss: 0.0507\n",
      "Epoch [293/500], Step [10/22], Loss: 0.2843\n",
      "Epoch [293/500], Step [20/22], Loss: 0.0231\n",
      "Epoch [294/500], Step [10/22], Loss: 0.3192\n",
      "Epoch [294/500], Step [20/22], Loss: 0.0792\n",
      "Epoch [295/500], Step [10/22], Loss: 0.3246\n",
      "Epoch [295/500], Step [20/22], Loss: 0.1060\n",
      "Epoch [296/500], Step [10/22], Loss: 0.2492\n",
      "Epoch [296/500], Step [20/22], Loss: 0.0690\n",
      "Epoch [297/500], Step [10/22], Loss: 0.2498\n",
      "Epoch [297/500], Step [20/22], Loss: 0.0302\n",
      "Epoch [298/500], Step [10/22], Loss: 0.2787\n",
      "Epoch [298/500], Step [20/22], Loss: 0.0310\n",
      "Epoch [299/500], Step [10/22], Loss: 0.2343\n",
      "Epoch [299/500], Step [20/22], Loss: 0.0184\n",
      "Epoch [300/500], Step [10/22], Loss: 0.1888\n",
      "Epoch [300/500], Step [20/22], Loss: 0.0330\n",
      "Epoch [301/500], Step [10/22], Loss: 0.2167\n",
      "Epoch [301/500], Step [20/22], Loss: 0.0593\n",
      "Epoch [302/500], Step [10/22], Loss: 0.2182\n",
      "Epoch [302/500], Step [20/22], Loss: 0.0461\n",
      "Epoch [303/500], Step [10/22], Loss: 0.2419\n",
      "Epoch [303/500], Step [20/22], Loss: 0.4061\n",
      "Epoch [304/500], Step [10/22], Loss: 0.5267\n",
      "Epoch [304/500], Step [20/22], Loss: 0.1198\n",
      "Epoch [305/500], Step [10/22], Loss: 0.4521\n",
      "Epoch [305/500], Step [20/22], Loss: 0.0965\n",
      "Epoch [306/500], Step [10/22], Loss: 0.3030\n",
      "Epoch [306/500], Step [20/22], Loss: 0.0832\n",
      "Epoch [307/500], Step [10/22], Loss: 0.3526\n",
      "Epoch [307/500], Step [20/22], Loss: 0.1077\n",
      "Epoch [308/500], Step [10/22], Loss: 0.2593\n",
      "Epoch [308/500], Step [20/22], Loss: 0.0540\n",
      "Epoch [309/500], Step [10/22], Loss: 0.2429\n",
      "Epoch [309/500], Step [20/22], Loss: 0.0439\n",
      "Epoch [310/500], Step [10/22], Loss: 0.2542\n",
      "Epoch [310/500], Step [20/22], Loss: 0.0308\n",
      "Epoch [311/500], Step [10/22], Loss: 0.1872\n",
      "Epoch [311/500], Step [20/22], Loss: 0.0621\n",
      "Epoch [312/500], Step [10/22], Loss: 0.2467\n",
      "Epoch [312/500], Step [20/22], Loss: 0.0465\n",
      "Epoch [313/500], Step [10/22], Loss: 0.2800\n",
      "Epoch [313/500], Step [20/22], Loss: 0.0621\n",
      "Epoch [314/500], Step [10/22], Loss: 0.4802\n",
      "Epoch [314/500], Step [20/22], Loss: 0.1163\n",
      "Epoch [315/500], Step [10/22], Loss: 0.3655\n",
      "Epoch [315/500], Step [20/22], Loss: 0.0899\n",
      "Epoch [316/500], Step [10/22], Loss: 0.3214\n",
      "Epoch [316/500], Step [20/22], Loss: 0.1346\n",
      "Epoch [317/500], Step [10/22], Loss: 0.2721\n",
      "Epoch [317/500], Step [20/22], Loss: 0.0613\n",
      "Epoch [318/500], Step [10/22], Loss: 0.2462\n",
      "Epoch [318/500], Step [20/22], Loss: 0.0628\n",
      "Epoch [319/500], Step [10/22], Loss: 0.2498\n",
      "Epoch [319/500], Step [20/22], Loss: 0.0361\n",
      "Epoch [320/500], Step [10/22], Loss: 0.2059\n",
      "Epoch [320/500], Step [20/22], Loss: 0.0257\n",
      "Epoch [321/500], Step [10/22], Loss: 0.1763\n",
      "Epoch [321/500], Step [20/22], Loss: 0.0196\n",
      "Epoch [322/500], Step [10/22], Loss: 0.1458\n",
      "Epoch [322/500], Step [20/22], Loss: 0.0196\n",
      "Epoch [323/500], Step [10/22], Loss: 0.1941\n",
      "Epoch [323/500], Step [20/22], Loss: 0.0213\n",
      "Epoch [324/500], Step [10/22], Loss: 0.2840\n",
      "Epoch [324/500], Step [20/22], Loss: 0.0203\n",
      "Epoch [325/500], Step [10/22], Loss: 0.1833\n",
      "Epoch [325/500], Step [20/22], Loss: 0.0094\n",
      "Epoch [326/500], Step [10/22], Loss: 0.2164\n",
      "Epoch [326/500], Step [20/22], Loss: 0.0246\n",
      "Epoch [327/500], Step [10/22], Loss: 0.1934\n",
      "Epoch [327/500], Step [20/22], Loss: 0.0717\n",
      "Epoch [328/500], Step [10/22], Loss: 0.2214\n",
      "Epoch [328/500], Step [20/22], Loss: 0.6009\n",
      "Epoch [329/500], Step [10/22], Loss: 0.5814\n",
      "Epoch [329/500], Step [20/22], Loss: 0.1114\n",
      "Epoch [330/500], Step [10/22], Loss: 0.4973\n",
      "Epoch [330/500], Step [20/22], Loss: 0.2509\n",
      "Epoch [331/500], Step [10/22], Loss: 0.3795\n",
      "Epoch [331/500], Step [20/22], Loss: 0.1360\n",
      "Epoch [332/500], Step [10/22], Loss: 0.4377\n",
      "Epoch [332/500], Step [20/22], Loss: 0.1434\n",
      "Epoch [333/500], Step [10/22], Loss: 0.3356\n",
      "Epoch [333/500], Step [20/22], Loss: 0.0758\n",
      "Epoch [334/500], Step [10/22], Loss: 0.4321\n",
      "Epoch [334/500], Step [20/22], Loss: 0.0836\n",
      "Epoch [335/500], Step [10/22], Loss: 0.3556\n",
      "Epoch [335/500], Step [20/22], Loss: 0.0938\n",
      "Epoch [336/500], Step [10/22], Loss: 0.2424\n",
      "Epoch [336/500], Step [20/22], Loss: 0.0513\n",
      "Epoch [337/500], Step [10/22], Loss: 0.3137\n",
      "Epoch [337/500], Step [20/22], Loss: 0.0709\n",
      "Epoch [338/500], Step [10/22], Loss: 0.2732\n",
      "Epoch [338/500], Step [20/22], Loss: 0.0376\n",
      "Epoch [339/500], Step [10/22], Loss: 0.2064\n",
      "Epoch [339/500], Step [20/22], Loss: 0.0336\n",
      "Epoch [340/500], Step [10/22], Loss: 0.1834\n",
      "Epoch [340/500], Step [20/22], Loss: 0.0297\n",
      "Epoch [341/500], Step [10/22], Loss: 0.1978\n",
      "Epoch [341/500], Step [20/22], Loss: 0.0287\n",
      "Epoch [342/500], Step [10/22], Loss: 0.1671\n",
      "Epoch [342/500], Step [20/22], Loss: 0.0612\n",
      "Epoch [343/500], Step [10/22], Loss: 0.2955\n",
      "Epoch [343/500], Step [20/22], Loss: 0.0678\n",
      "Epoch [344/500], Step [10/22], Loss: 0.1844\n",
      "Epoch [344/500], Step [20/22], Loss: 0.0701\n",
      "Epoch [345/500], Step [10/22], Loss: 0.3390\n",
      "Epoch [345/500], Step [20/22], Loss: 0.0690\n",
      "Epoch [346/500], Step [10/22], Loss: 0.2564\n",
      "Epoch [346/500], Step [20/22], Loss: 0.0390\n",
      "Epoch [347/500], Step [10/22], Loss: 0.2208\n",
      "Epoch [347/500], Step [20/22], Loss: 0.0363\n",
      "Epoch [348/500], Step [10/22], Loss: 0.1811\n",
      "Epoch [348/500], Step [20/22], Loss: 0.0187\n",
      "Epoch [349/500], Step [10/22], Loss: 0.1794\n",
      "Epoch [349/500], Step [20/22], Loss: 0.0117\n",
      "Epoch [350/500], Step [10/22], Loss: 0.1620\n",
      "Epoch [350/500], Step [20/22], Loss: 0.0104\n",
      "Epoch [351/500], Step [10/22], Loss: 0.1475\n",
      "Epoch [351/500], Step [20/22], Loss: 0.0647\n",
      "Epoch [352/500], Step [10/22], Loss: 0.1966\n",
      "Epoch [352/500], Step [20/22], Loss: 0.2301\n",
      "Epoch [353/500], Step [10/22], Loss: 0.2839\n",
      "Epoch [353/500], Step [20/22], Loss: 0.2039\n",
      "Epoch [354/500], Step [10/22], Loss: 0.3237\n",
      "Epoch [354/500], Step [20/22], Loss: 0.1225\n",
      "Epoch [355/500], Step [10/22], Loss: 0.3193\n",
      "Epoch [355/500], Step [20/22], Loss: 0.0594\n",
      "Epoch [356/500], Step [10/22], Loss: 0.2515\n",
      "Epoch [356/500], Step [20/22], Loss: 0.1442\n",
      "Epoch [357/500], Step [10/22], Loss: 0.2511\n",
      "Epoch [357/500], Step [20/22], Loss: 0.0636\n",
      "Epoch [358/500], Step [10/22], Loss: 0.3565\n",
      "Epoch [358/500], Step [20/22], Loss: 0.0557\n",
      "Epoch [359/500], Step [10/22], Loss: 0.1786\n",
      "Epoch [359/500], Step [20/22], Loss: 0.0432\n",
      "Epoch [360/500], Step [10/22], Loss: 0.1651\n",
      "Epoch [360/500], Step [20/22], Loss: 0.0387\n",
      "Epoch [361/500], Step [10/22], Loss: 0.1665\n",
      "Epoch [361/500], Step [20/22], Loss: 0.0636\n",
      "Epoch [362/500], Step [10/22], Loss: 0.2173\n",
      "Epoch [362/500], Step [20/22], Loss: 0.0423\n",
      "Epoch [363/500], Step [10/22], Loss: 0.1968\n",
      "Epoch [363/500], Step [20/22], Loss: 0.1016\n",
      "Epoch [364/500], Step [10/22], Loss: 0.2667\n",
      "Epoch [364/500], Step [20/22], Loss: 0.0280\n",
      "Epoch [365/500], Step [10/22], Loss: 0.2353\n",
      "Epoch [365/500], Step [20/22], Loss: 0.0373\n",
      "Epoch [366/500], Step [10/22], Loss: 0.2773\n",
      "Epoch [366/500], Step [20/22], Loss: 0.0252\n",
      "Epoch [367/500], Step [10/22], Loss: 0.1372\n",
      "Epoch [367/500], Step [20/22], Loss: 0.0310\n",
      "Epoch [368/500], Step [10/22], Loss: 0.2249\n",
      "Epoch [368/500], Step [20/22], Loss: 0.0195\n",
      "Epoch [369/500], Step [10/22], Loss: 0.1409\n",
      "Epoch [369/500], Step [20/22], Loss: 0.0296\n",
      "Epoch [370/500], Step [10/22], Loss: 0.2075\n",
      "Epoch [370/500], Step [20/22], Loss: 0.0194\n",
      "Epoch [371/500], Step [10/22], Loss: 0.1805\n",
      "Epoch [371/500], Step [20/22], Loss: 0.0447\n",
      "Epoch [372/500], Step [10/22], Loss: 0.2505\n",
      "Epoch [372/500], Step [20/22], Loss: 0.0147\n",
      "Epoch [373/500], Step [10/22], Loss: 0.3014\n",
      "Epoch [373/500], Step [20/22], Loss: 0.0185\n",
      "Epoch [374/500], Step [10/22], Loss: 0.1804\n",
      "Epoch [374/500], Step [20/22], Loss: 0.0147\n",
      "Epoch [375/500], Step [10/22], Loss: 0.1393\n",
      "Epoch [375/500], Step [20/22], Loss: 0.0157\n",
      "Epoch [376/500], Step [10/22], Loss: 0.1255\n",
      "Epoch [376/500], Step [20/22], Loss: 0.0191\n",
      "Epoch [377/500], Step [10/22], Loss: 0.1552\n",
      "Epoch [377/500], Step [20/22], Loss: 0.0419\n",
      "Epoch [378/500], Step [10/22], Loss: 0.1204\n",
      "Epoch [378/500], Step [20/22], Loss: 0.1251\n",
      "Epoch [379/500], Step [10/22], Loss: 0.2043\n",
      "Epoch [379/500], Step [20/22], Loss: 0.1228\n",
      "Epoch [380/500], Step [10/22], Loss: 0.2316\n",
      "Epoch [380/500], Step [20/22], Loss: 0.0290\n",
      "Epoch [381/500], Step [10/22], Loss: 0.1614\n",
      "Epoch [381/500], Step [20/22], Loss: 0.0331\n",
      "Epoch [382/500], Step [10/22], Loss: 0.2060\n",
      "Epoch [382/500], Step [20/22], Loss: 0.0210\n",
      "Epoch [383/500], Step [10/22], Loss: 0.1609\n",
      "Epoch [383/500], Step [20/22], Loss: 0.0613\n",
      "Epoch [384/500], Step [10/22], Loss: 0.3298\n",
      "Epoch [384/500], Step [20/22], Loss: 0.0583\n",
      "Epoch [385/500], Step [10/22], Loss: 0.2870\n",
      "Epoch [385/500], Step [20/22], Loss: 0.0326\n",
      "Epoch [386/500], Step [10/22], Loss: 0.1660\n",
      "Epoch [386/500], Step [20/22], Loss: 0.0203\n",
      "Epoch [387/500], Step [10/22], Loss: 0.1736\n",
      "Epoch [387/500], Step [20/22], Loss: 0.0212\n",
      "Epoch [388/500], Step [10/22], Loss: 0.1379\n",
      "Epoch [388/500], Step [20/22], Loss: 0.0145\n",
      "Epoch [389/500], Step [10/22], Loss: 0.1353\n",
      "Epoch [389/500], Step [20/22], Loss: 0.0536\n",
      "Epoch [390/500], Step [10/22], Loss: 0.1301\n",
      "Epoch [390/500], Step [20/22], Loss: 0.4972\n",
      "Epoch [391/500], Step [10/22], Loss: 0.3555\n",
      "Epoch [391/500], Step [20/22], Loss: 0.1993\n",
      "Epoch [392/500], Step [10/22], Loss: 0.4076\n",
      "Epoch [392/500], Step [20/22], Loss: 0.3358\n",
      "Epoch [393/500], Step [10/22], Loss: 0.3156\n",
      "Epoch [393/500], Step [20/22], Loss: 0.1304\n",
      "Epoch [394/500], Step [10/22], Loss: 0.3158\n",
      "Epoch [394/500], Step [20/22], Loss: 0.1429\n",
      "Epoch [395/500], Step [10/22], Loss: 0.2298\n",
      "Epoch [395/500], Step [20/22], Loss: 0.0812\n",
      "Epoch [396/500], Step [10/22], Loss: 0.2189\n",
      "Epoch [396/500], Step [20/22], Loss: 0.0686\n",
      "Epoch [397/500], Step [10/22], Loss: 0.1826\n",
      "Epoch [397/500], Step [20/22], Loss: 0.0445\n",
      "Epoch [398/500], Step [10/22], Loss: 0.1633\n",
      "Epoch [398/500], Step [20/22], Loss: 0.0405\n",
      "Epoch [399/500], Step [10/22], Loss: 0.1326\n",
      "Epoch [399/500], Step [20/22], Loss: 0.0422\n",
      "Epoch [400/500], Step [10/22], Loss: 0.1384\n",
      "Epoch [400/500], Step [20/22], Loss: 0.0226\n",
      "Epoch [401/500], Step [10/22], Loss: 0.1533\n",
      "Epoch [401/500], Step [20/22], Loss: 0.0307\n",
      "Epoch [402/500], Step [10/22], Loss: 0.1331\n",
      "Epoch [402/500], Step [20/22], Loss: 0.0135\n",
      "Epoch [403/500], Step [10/22], Loss: 0.1395\n",
      "Epoch [403/500], Step [20/22], Loss: 0.0304\n",
      "Epoch [404/500], Step [10/22], Loss: 0.1203\n",
      "Epoch [404/500], Step [20/22], Loss: 0.0133\n",
      "Epoch [405/500], Step [10/22], Loss: 0.1801\n",
      "Epoch [405/500], Step [20/22], Loss: 0.0175\n",
      "Epoch [406/500], Step [10/22], Loss: 0.1289\n",
      "Epoch [406/500], Step [20/22], Loss: 0.0611\n",
      "Epoch [407/500], Step [10/22], Loss: 0.1944\n",
      "Epoch [407/500], Step [20/22], Loss: 0.0281\n",
      "Epoch [408/500], Step [10/22], Loss: 0.1700\n",
      "Epoch [408/500], Step [20/22], Loss: 0.0533\n",
      "Epoch [409/500], Step [10/22], Loss: 0.2534\n",
      "Epoch [409/500], Step [20/22], Loss: 0.0689\n",
      "Epoch [410/500], Step [10/22], Loss: 0.3709\n",
      "Epoch [410/500], Step [20/22], Loss: 0.0540\n",
      "Epoch [411/500], Step [10/22], Loss: 0.2091\n",
      "Epoch [411/500], Step [20/22], Loss: 0.0178\n",
      "Epoch [412/500], Step [10/22], Loss: 0.2142\n",
      "Epoch [412/500], Step [20/22], Loss: 0.0760\n",
      "Epoch [413/500], Step [10/22], Loss: 0.1589\n",
      "Epoch [413/500], Step [20/22], Loss: 0.1453\n",
      "Epoch [414/500], Step [10/22], Loss: 0.2258\n",
      "Epoch [414/500], Step [20/22], Loss: 0.0371\n",
      "Epoch [415/500], Step [10/22], Loss: 0.2081\n",
      "Epoch [415/500], Step [20/22], Loss: 0.0111\n",
      "Epoch [416/500], Step [10/22], Loss: 0.1692\n",
      "Epoch [416/500], Step [20/22], Loss: 0.0136\n",
      "Epoch [417/500], Step [10/22], Loss: 0.1453\n",
      "Epoch [417/500], Step [20/22], Loss: 0.0104\n",
      "Epoch [418/500], Step [10/22], Loss: 0.1409\n",
      "Epoch [418/500], Step [20/22], Loss: 0.0095\n",
      "Epoch [419/500], Step [10/22], Loss: 0.1257\n",
      "Epoch [419/500], Step [20/22], Loss: 0.0081\n",
      "Epoch [420/500], Step [10/22], Loss: 0.1193\n",
      "Epoch [420/500], Step [20/22], Loss: 0.0070\n",
      "Epoch [421/500], Step [10/22], Loss: 0.1093\n",
      "Epoch [421/500], Step [20/22], Loss: 0.0085\n",
      "Epoch [422/500], Step [10/22], Loss: 0.1113\n",
      "Epoch [422/500], Step [20/22], Loss: 0.0051\n",
      "Epoch [423/500], Step [10/22], Loss: 0.1032\n",
      "Epoch [423/500], Step [20/22], Loss: 0.0135\n",
      "Epoch [424/500], Step [10/22], Loss: 0.1144\n",
      "Epoch [424/500], Step [20/22], Loss: 0.0045\n",
      "Epoch [425/500], Step [10/22], Loss: 0.1187\n",
      "Epoch [425/500], Step [20/22], Loss: 0.0060\n",
      "Epoch [426/500], Step [10/22], Loss: 0.1366\n",
      "Epoch [426/500], Step [20/22], Loss: 0.0073\n",
      "Epoch [427/500], Step [10/22], Loss: 0.2985\n",
      "Epoch [427/500], Step [20/22], Loss: 0.0100\n",
      "Epoch [428/500], Step [10/22], Loss: 0.2120\n",
      "Epoch [428/500], Step [20/22], Loss: 0.0423\n",
      "Epoch [429/500], Step [10/22], Loss: 0.2212\n",
      "Epoch [429/500], Step [20/22], Loss: 0.0108\n",
      "Epoch [430/500], Step [10/22], Loss: 0.3144\n",
      "Epoch [430/500], Step [20/22], Loss: 0.0171\n",
      "Epoch [431/500], Step [10/22], Loss: 0.2120\n",
      "Epoch [431/500], Step [20/22], Loss: 0.0152\n",
      "Epoch [432/500], Step [10/22], Loss: 0.1408\n",
      "Epoch [432/500], Step [20/22], Loss: 0.0111\n",
      "Epoch [433/500], Step [10/22], Loss: 0.1332\n",
      "Epoch [433/500], Step [20/22], Loss: 0.0066\n",
      "Epoch [434/500], Step [10/22], Loss: 0.1206\n",
      "Epoch [434/500], Step [20/22], Loss: 0.0049\n",
      "Epoch [435/500], Step [10/22], Loss: 0.1455\n",
      "Epoch [435/500], Step [20/22], Loss: 0.0198\n",
      "Epoch [436/500], Step [10/22], Loss: 0.1425\n",
      "Epoch [436/500], Step [20/22], Loss: 0.0703\n",
      "Epoch [437/500], Step [10/22], Loss: 0.1367\n",
      "Epoch [437/500], Step [20/22], Loss: 0.1291\n",
      "Epoch [438/500], Step [10/22], Loss: 0.3420\n",
      "Epoch [438/500], Step [20/22], Loss: 0.0351\n",
      "Epoch [439/500], Step [10/22], Loss: 0.1944\n",
      "Epoch [439/500], Step [20/22], Loss: 0.1450\n",
      "Epoch [440/500], Step [10/22], Loss: 0.1853\n",
      "Epoch [440/500], Step [20/22], Loss: 0.0552\n",
      "Epoch [441/500], Step [10/22], Loss: 0.1925\n",
      "Epoch [441/500], Step [20/22], Loss: 0.0414\n",
      "Epoch [442/500], Step [10/22], Loss: 0.1453\n",
      "Epoch [442/500], Step [20/22], Loss: 0.0146\n",
      "Epoch [443/500], Step [10/22], Loss: 0.1240\n",
      "Epoch [443/500], Step [20/22], Loss: 0.0092\n",
      "Epoch [444/500], Step [10/22], Loss: 0.1371\n",
      "Epoch [444/500], Step [20/22], Loss: 0.0159\n",
      "Epoch [445/500], Step [10/22], Loss: 0.1104\n",
      "Epoch [445/500], Step [20/22], Loss: 0.0097\n",
      "Epoch [446/500], Step [10/22], Loss: 0.1162\n",
      "Epoch [446/500], Step [20/22], Loss: 0.1650\n",
      "Epoch [447/500], Step [10/22], Loss: 0.2689\n",
      "Epoch [447/500], Step [20/22], Loss: 0.2035\n",
      "Epoch [448/500], Step [10/22], Loss: 0.2210\n",
      "Epoch [448/500], Step [20/22], Loss: 0.1039\n",
      "Epoch [449/500], Step [10/22], Loss: 0.3152\n",
      "Epoch [449/500], Step [20/22], Loss: 0.0249\n",
      "Epoch [450/500], Step [10/22], Loss: 0.2688\n",
      "Epoch [450/500], Step [20/22], Loss: 0.0600\n",
      "Epoch [451/500], Step [10/22], Loss: 0.1866\n",
      "Epoch [451/500], Step [20/22], Loss: 0.0308\n",
      "Epoch [452/500], Step [10/22], Loss: 0.1381\n",
      "Epoch [452/500], Step [20/22], Loss: 0.0168\n",
      "Epoch [453/500], Step [10/22], Loss: 0.1235\n",
      "Epoch [453/500], Step [20/22], Loss: 0.0148\n",
      "Epoch [454/500], Step [10/22], Loss: 0.1147\n",
      "Epoch [454/500], Step [20/22], Loss: 0.0096\n",
      "Epoch [455/500], Step [10/22], Loss: 0.1066\n",
      "Epoch [455/500], Step [20/22], Loss: 0.0109\n",
      "Epoch [456/500], Step [10/22], Loss: 0.1150\n",
      "Epoch [456/500], Step [20/22], Loss: 0.0062\n",
      "Epoch [457/500], Step [10/22], Loss: 0.1008\n",
      "Epoch [457/500], Step [20/22], Loss: 0.0077\n",
      "Epoch [458/500], Step [10/22], Loss: 0.0971\n",
      "Epoch [458/500], Step [20/22], Loss: 0.0054\n",
      "Epoch [459/500], Step [10/22], Loss: 0.0948\n",
      "Epoch [459/500], Step [20/22], Loss: 0.0051\n",
      "Epoch [460/500], Step [10/22], Loss: 0.0867\n",
      "Epoch [460/500], Step [20/22], Loss: 0.0041\n",
      "Epoch [461/500], Step [10/22], Loss: 0.1110\n",
      "Epoch [461/500], Step [20/22], Loss: 0.0047\n",
      "Epoch [462/500], Step [10/22], Loss: 0.0929\n",
      "Epoch [462/500], Step [20/22], Loss: 0.0056\n",
      "Epoch [463/500], Step [10/22], Loss: 0.0952\n",
      "Epoch [463/500], Step [20/22], Loss: 0.0070\n",
      "Epoch [464/500], Step [10/22], Loss: 0.1806\n",
      "Epoch [464/500], Step [20/22], Loss: 0.0047\n",
      "Epoch [465/500], Step [10/22], Loss: 0.1355\n",
      "Epoch [465/500], Step [20/22], Loss: 0.0057\n",
      "Epoch [466/500], Step [10/22], Loss: 0.1361\n",
      "Epoch [466/500], Step [20/22], Loss: 0.0161\n",
      "Epoch [467/500], Step [10/22], Loss: 0.2065\n",
      "Epoch [467/500], Step [20/22], Loss: 0.0123\n",
      "Epoch [468/500], Step [10/22], Loss: 0.3508\n",
      "Epoch [468/500], Step [20/22], Loss: 0.1733\n",
      "Epoch [469/500], Step [10/22], Loss: 0.2762\n",
      "Epoch [469/500], Step [20/22], Loss: 0.0629\n",
      "Epoch [470/500], Step [10/22], Loss: 0.2285\n",
      "Epoch [470/500], Step [20/22], Loss: 0.0390\n",
      "Epoch [471/500], Step [10/22], Loss: 0.2093\n",
      "Epoch [471/500], Step [20/22], Loss: 0.0276\n",
      "Epoch [472/500], Step [10/22], Loss: 0.1988\n",
      "Epoch [472/500], Step [20/22], Loss: 0.0724\n",
      "Epoch [473/500], Step [10/22], Loss: 0.1925\n",
      "Epoch [473/500], Step [20/22], Loss: 0.3892\n",
      "Epoch [474/500], Step [10/22], Loss: 0.4407\n",
      "Epoch [474/500], Step [20/22], Loss: 0.1422\n",
      "Epoch [475/500], Step [10/22], Loss: 0.2076\n",
      "Epoch [475/500], Step [20/22], Loss: 0.1078\n",
      "Epoch [476/500], Step [10/22], Loss: 0.1922\n",
      "Epoch [476/500], Step [20/22], Loss: 0.0777\n",
      "Epoch [477/500], Step [10/22], Loss: 0.1755\n",
      "Epoch [477/500], Step [20/22], Loss: 0.0533\n",
      "Epoch [478/500], Step [10/22], Loss: 0.1412\n",
      "Epoch [478/500], Step [20/22], Loss: 0.0428\n",
      "Epoch [479/500], Step [10/22], Loss: 0.1230\n",
      "Epoch [479/500], Step [20/22], Loss: 0.0287\n",
      "Epoch [480/500], Step [10/22], Loss: 0.1127\n",
      "Epoch [480/500], Step [20/22], Loss: 0.0185\n",
      "Epoch [481/500], Step [10/22], Loss: 0.1000\n",
      "Epoch [481/500], Step [20/22], Loss: 0.0160\n",
      "Epoch [482/500], Step [10/22], Loss: 0.0999\n",
      "Epoch [482/500], Step [20/22], Loss: 0.0108\n",
      "Epoch [483/500], Step [10/22], Loss: 0.0948\n",
      "Epoch [483/500], Step [20/22], Loss: 0.0097\n",
      "Epoch [484/500], Step [10/22], Loss: 0.0911\n",
      "Epoch [484/500], Step [20/22], Loss: 0.0072\n",
      "Epoch [485/500], Step [10/22], Loss: 0.0893\n",
      "Epoch [485/500], Step [20/22], Loss: 0.0058\n",
      "Epoch [486/500], Step [10/22], Loss: 0.0820\n",
      "Epoch [486/500], Step [20/22], Loss: 0.0048\n",
      "Epoch [487/500], Step [10/22], Loss: 0.0811\n",
      "Epoch [487/500], Step [20/22], Loss: 0.0038\n",
      "Epoch [488/500], Step [10/22], Loss: 0.0733\n",
      "Epoch [488/500], Step [20/22], Loss: 0.0034\n",
      "Epoch [489/500], Step [10/22], Loss: 0.0736\n",
      "Epoch [489/500], Step [20/22], Loss: 0.0036\n",
      "Epoch [490/500], Step [10/22], Loss: 0.0681\n",
      "Epoch [490/500], Step [20/22], Loss: 0.0023\n",
      "Epoch [491/500], Step [10/22], Loss: 0.0705\n",
      "Epoch [491/500], Step [20/22], Loss: 0.0043\n",
      "Epoch [492/500], Step [10/22], Loss: 0.1466\n",
      "Epoch [492/500], Step [20/22], Loss: 0.0035\n",
      "Epoch [493/500], Step [10/22], Loss: 0.2051\n",
      "Epoch [493/500], Step [20/22], Loss: 0.0324\n",
      "Epoch [494/500], Step [10/22], Loss: 0.1192\n",
      "Epoch [494/500], Step [20/22], Loss: 0.0663\n",
      "Epoch [495/500], Step [10/22], Loss: 0.2121\n",
      "Epoch [495/500], Step [20/22], Loss: 0.0221\n",
      "Epoch [496/500], Step [10/22], Loss: 0.1344\n",
      "Epoch [496/500], Step [20/22], Loss: 0.0121\n",
      "Epoch [497/500], Step [10/22], Loss: 0.0980\n",
      "Epoch [497/500], Step [20/22], Loss: 0.0044\n",
      "Epoch [498/500], Step [10/22], Loss: 0.0755\n",
      "Epoch [498/500], Step [20/22], Loss: 0.0056\n",
      "Epoch [499/500], Step [10/22], Loss: 0.1450\n",
      "Epoch [499/500], Step [20/22], Loss: 0.0036\n",
      "Epoch [500/500], Step [10/22], Loss: 0.0721\n",
      "Epoch [500/500], Step [20/22], Loss: 0.0317\n"
     ]
    }
   ],
   "source": [
    "# 定義交叉驗證參數\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "fold_scores = []\n",
    "best_score = 0\n",
    "# 執行交叉驗證\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(train_data)):\n",
    "    # 根據fold拆分成訓練和驗證數據\n",
    "    train_data_fold = torch.utils.data.Subset(train_data, train_index)\n",
    "    val_data_fold = torch.utils.data.Subset(train_data, val_index)\n",
    "\n",
    "    # 創建訓練和驗證的DataLoader\n",
    "    train_loader_fold = DataLoader(train_data_fold, batch_size=32, shuffle=False)\n",
    "    val_loader_fold = DataLoader(val_data_fold, batch_size=32, shuffle=False)\n",
    "\n",
    "    # 初始化模型，定義損失函數和優化器\n",
    "    model = LSTM(input_size=train_set_np.shape[1]-2, hidden_size=50, num_layers=2).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # 訓練模型\n",
    "    num_epochs = 500\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (inputs, labels) in enumerate(train_loader_fold):\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device).float().view(-1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print loss\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader_fold)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    #評估模型\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        for inputs, labels in val_loader_fold:\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).int()\n",
    "            true_labels.extend(labels.tolist())\n",
    "            pred_labels.extend(preds.tolist())\n",
    "    # 計算準確率\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    # 將當前fold的結果存放到列表中\n",
    "    fold_scores.append(accuracy)\n",
    "\n",
    "    # 判斷當前fold的準確率是否為最佳準確率\n",
    "    if accuracy > best_score:\n",
    "        best_model = model\n",
    "        best_score = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Validation Score: 0.888235294117647\n",
      "Fold 2 Validation Score: 0.8764705882352941\n",
      "Fold 3 Validation Score: 0.8647058823529412\n",
      "Fold 4 Validation Score: 0.9058823529411765\n",
      "Fold 5 Validation Score: 0.9\n"
     ]
    }
   ],
   "source": [
    "# print 每個fold的結果\n",
    "for fold, score in enumerate(fold_scores):\n",
    "    print(f'Fold {fold+1} Validation Score: {score}')\n",
    "\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device).float()\n",
    "        outputs = best_model(inputs)\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).int()\n",
    "        true_labels.extend(labels.tolist())\n",
    "        pred_labels.extend(preds.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[265  19]\n",
      " [ 43  29]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.93      0.90       284\n",
      "         1.0       0.60      0.40      0.48        72\n",
      "\n",
      "    accuracy                           0.83       356\n",
      "   macro avg       0.73      0.67      0.69       356\n",
      "weighted avg       0.81      0.83      0.81       356\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 計算混淆矩陣\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "print(cm)\n",
    "print(classification_report(true_labels, pred_labels))\n",
    "\n",
    "labels = np.array([[\"TN\", \"FP\"], [\"FN\", \"TP\"]])\n",
    "for m in range(cm.shape[0]):\n",
    "    for n in range(cm.shape[1]):\n",
    "        plt.text(x=m,y=n,s=f\"{labels[n][m]}\\n{cm[n][m]}\", va='center', ha='center', size='xx-large')\n",
    "\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model,'my_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(18, 50, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('my_model.pth')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
